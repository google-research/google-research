# coding=utf-8
# Copyright 2024 The Google Research Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Utility functions."""
import os
import random

import tensorflow as tf
from tensorflow.contrib.layers.python import layers as tf_layers
from tensorflow.python.platform import flags

FLAGS = flags.FLAGS


## Image helper
def get_images(paths, labels, nb_samples=None, shuffle=True, train=True):
  """Retrieves image filepaths."""
  # pylint: disable=g-complex-comprehension
  del train  # Unused
  if nb_samples is not None:
    sampler = lambda x: random.sample(x, nb_samples)
  else:
    sampler = lambda x: x
  images = [(i, os.path.join(path, image)) \
      for i, path in zip(labels, paths) \
      for image in sampler(sorted(os.listdir(path)))]
  if shuffle:
    random.shuffle(images)
  return images


## Network helpers
def conv_block(inp,
               cweight,
               bweight,
               reuse,
               scope,
               activation=tf.nn.relu,
               max_pool_pad='VALID',
               residual=False):
  """Perform, conv, batch norm, nonlinearity, and max pool."""
  del residual  # Not used.
  stride, no_stride = [1, 2, 2, 1], [1, 1, 1, 1]

  if FLAGS.max_pool:
    conv_output = tf.nn.conv2d(inp, cweight, no_stride, 'SAME') + bweight
  else:
    conv_output = tf.nn.conv2d(inp, cweight, stride, 'SAME') + bweight
  normed = normalize(conv_output, activation, reuse, scope)
  if FLAGS.max_pool:
    normed = tf.nn.max_pool(normed, stride, stride, max_pool_pad)
  return normed


def normalize(inp, activation, reuse, scope):
  """Normalizes input based on FLAGS.norm setting."""
  if FLAGS.norm == 'batch_norm':
    return tf_layers.batch_norm(
        inp, activation_fn=activation, reuse=reuse, scope=scope)
  elif FLAGS.norm == 'layer_norm':
    return tf_layers.layer_norm(
        inp, activation_fn=activation, reuse=reuse, scope=scope)
  elif FLAGS.norm == 'None':
    if activation is not None:
      return activation(inp)
    else:
      return inp


## Loss functions
def mse(pred, label):
  pred = tf.reshape(pred, [-1])
  label = tf.reshape(label, [-1])
  return tf.reduce_mean(tf.square(pred - label))


def xent(pred, label):
  # Note - with tf version <=0.12, this loss has incorrect 2nd derivatives
  return tf.nn.softmax_cross_entropy_with_logits(
      logits=pred, labels=label) / FLAGS.update_batch_size
