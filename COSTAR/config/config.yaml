hydra:
  sweep:
    dir: multirun
    subdir: ${exp.tags}/${dataset.name}/${model.name}/${exp.seed}
  


# Model specific parameters
model:
  use_best_model: False
  dim_treatments: ???               # Will be defined with +dataset=...
  dim_vitals: ???                   # Will be defined with +dataset=...
  dim_static_features: ???          # Will be defined with +dataset=...
  dim_outcomes: ???                 # Will be defined with +dataset=...
  generative_style_predict: False

# Dataset for training / evaluation -- specific values should be filled for each dataset
dataset:
  val_batch_size: ???               # Will be defined with +dataset=...
  treatment_mode: ???               # multiclass / multilabel; Will be defined with +dataset=...
  few_shot_sample_num: -1           # when > 0, resample generated training samples to make the dataset few-shot
  data_gen_n_jobs: 8
  use_few_shot: False
  max_number: -1

# target_dataset:
#   val_batch_size: ???               # Will be defined with +dataset=...
#   treatment_mode: ???               # multiclass / multilabel; Will be defined with +dataset=...
#   few_shot_sample_num: -1           # when > 0, resample generated training samples to make the dataset few-shot

# Experiment specific parameters
exp:
  seed: 100                         # Random seed for all the initialisations, dataset generation etc.
  gpus: [0]                         # Number of GPUs to use / indices of GPUs like [0,1]
  max_epochs: 100                   # Number of epochs
  logging: True                     # Logging to MlFlow
  watch_model: False
  mlflow_uri: http://127.0.0.1:5000 # MlFlow server is located on mtec-mis-gpu02.ethz.ch
  unscale_rmse: ???                 # RMSE with unnormalised targets; Will be defined with +dataset=...
  percentage_rmse: ???              # RMSE as percentage wrt norm_const; Will be defined with +dataset=...

  alpha: 1.0                        # Has no full effect, if update_alpha is True
  update_alpha: True                # Exponential growth of alpha from 0.0 to 1.0
  alpha_rate: exp                   # exp / lin
  balancing:                        # grad_reverse / domain_confusion

  alpha_prev_treat: 0.0
  update_alpha_prev_treat: False
  alpha_prev_treat_rate: exp

  alpha_age: 0.0
  update_alpha_age: False
  alpha_age_rate: exp

  train_domain_label_adv: False

  bce_weight: False                 # Weight in BCE loss, proportional to treatment frequency
  weights_ema:                      # Exponential moving average of weights
  beta: 0.99                        # EMA beta

  early_stopping: False
  early_stopping_patience: 20

  check_val_every_n_epoch: 1
  save_ckpt_int: 0    # disable by default

  gen_data_only: False
  data_save_path: "data"
  tags: ""

  eval_only: False
  eval_ckpt_dir: ""
  eval_ckpt_type: "last"

  finetune_ckpt_dir: ""
  finetune_ckpt_type: "last"
  finetune_tag: "" # if set, wait until the exp in finetune_tag finish to start
  pretrain_tag: ""
  pretrain_src_tag: ""
  skip_train_rep: False

  interpret_only: False

  save_representations: False

  save_prepared_data: False

# Hydra defaults
defaults:
  - _self_
  - override hydra/hydra_logging: colorlog
  - override hydra/job_logging: colorlog

