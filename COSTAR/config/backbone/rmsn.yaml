# @package _global_
model:
  name: RMSN

  propensity_treatment:                 # Missing hyperparameters are to be filled in command line / with tune_hparams = True / selected with +backbone/rmsn_hparams=...
    _target_: src.models.rmsn.RMSNPropensityNetworkTreatment
    seq_hidden_units:                   # LSTM hidden units (d_h)
    dropout_rate:                       # Dropout of LSTM hidden layers
    num_layer: 1
    batch_size:
    max_grad_norm:
    optimizer:
      optimizer_cls: adam
      learning_rate:
      weight_decay: 0.0
      lr_scheduler: False

    tune_hparams: False                 # Hparam tuning
    tune_range: 50
    resources_per_trial:
    hparams_grid:

  propensity_history:                   # Missing hyperparameters are to be filled in command line / with tune_hparams = True / selected with +backbone/rmsn_hparams=...
    _target_: src.models.rmsn.RMSNPropensityNetworkHistory
    seq_hidden_units:                   # LSTM hidden units (d_h)
    dropout_rate:
    num_layer: 1
    batch_size:
    max_grad_norm:
    optimizer:
      optimizer_cls: adam
      learning_rate:
      weight_decay: 0.0
      lr_scheduler: False

    tune_hparams: False                 # Hparam tuning
    tune_range: 50
    resources_per_trial:
    hparams_grid:

  encoder:                              # Missing hyperparameters are to be filled in command line / with tune_hparams = True / selected with +backbone/rmsn_hparams=...
    _target_: src.models.rmsn.RMSNEncoder
    seq_hidden_units:                   # LSTM hidden units (d_h)
    dropout_rate:                       # Dropout of LSTM hidden layers
    num_layer: 1
    batch_size:
    max_grad_norm:
    optimizer:
      optimizer_cls: adam
      learning_rate:
      weight_decay: 0.0
      lr_scheduler: False

    tune_hparams: False                   # Hparam tuning
    tune_range: 50
    resources_per_trial:
    hparams_grid:

  train_decoder: True
  decoder:                                # Missing hyperparameters are to be filled in command line / with tune_hparams = True / selected with +backbone/rmsn_hparams=...
    _target_: src.models.rmsn.RMSNDecoder
    seq_hidden_units:                     # LSTM hidden units (d_h)
    dropout_rate:                         # Dropout of LSTM hidden layers
    num_layer: 1
    batch_size:
    max_grad_norm:
    optimizer:
      optimizer_cls: adam
      learning_rate:
      weight_decay: 0.0
      lr_scheduler: False

    tune_hparams: False                   # Hparam tuning
    tune_range: 20
    resources_per_trial:

exp:
  update_alpha: False