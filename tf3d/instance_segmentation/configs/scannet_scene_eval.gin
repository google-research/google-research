num_classes = 41


# Preprocessor
prepare_scannet_scene_dataset.valid_object_classes = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 16, 24, 28, 33, 34, 36, 39]
object_detection_preprocess.input_field_mapping_fn = @prepare_scannet_scene_dataset
object_detection_preprocess.images_points_correspondence_fn = None
object_detection_preprocess.points_pad_or_clip_size = None
object_detection_preprocess.voxel_grid_cell_size = (0.02, 0.02, 0.02)
object_detection_preprocess.voxels_pad_or_clip_size = None
object_detection_preprocess.point_feature_keys = ('point_colors', 'point_normals')
object_detection_preprocess.x_min_degree_rotation = None
object_detection_preprocess.x_max_degree_rotation = None
object_detection_preprocess.y_min_degree_rotation = None
object_detection_preprocess.y_max_degree_rotation = None
object_detection_preprocess.z_min_degree_rotation = None
object_detection_preprocess.z_max_degree_rotation = None
object_detection_preprocess.min_scale_ratio = None
object_detection_preprocess.max_scale_ratio = None
object_detection_preprocess.translation_range = None
object_detection_preprocess.points_within_box_margin = 0.0
object_detection_preprocess.min_num_points_in_objects = 5
object_detection_preprocess.num_points_to_randomly_sample = 500000
object_detection_preprocess.crop_points_around_random_seed_point = False


# Dataset
get_tf_data_dataset.dataset_name = 'scannet_scene'
get_tf_data_dataset.split_name = 'val'
get_tf_data_dataset.preprocess_fn = @object_detection_preprocess
get_tf_data_dataset.feature_keys = ['point_positions',
                                    'point_normals',
                                    'point_colors',
                                    'num_valid_points',
                                    'voxel_positions',
                                    'voxel_features',
                                    'voxel_xyz_indices',
                                    'num_valid_voxels',
                                    'points_to_voxel_mapping',
                                    'camera_image_name']
get_tf_data_dataset.label_keys = ['object_class_points',
                                  'object_class_voxels',
                                  'object_instance_id_points',
                                  'object_instance_id_voxels',
                                  'objects_class']
get_tf_data_dataset.shuffle_buffer_size = 4
get_tf_data_dataset.filenames_shuffle_buffer_size = 4
get_tf_data_dataset.read_block_length = 1
get_tf_data_dataset.num_readers = 16


# 3D Network
SparseConvUNet.conv_filter_size = 3
SparseConvUNet.encoder_dimensions = ((64, 64), (64, 96), (96, 128), (128, 160), (160, 192), (192, 224), (224, 256))
SparseConvUNet.bottleneck_dimensions = (256, 256)
SparseConvUNet.decoder_dimensions = ((256, 256), (224, 224), (192, 192), (160, 160), (128, 128), (96, 96), (64, 64))
SparseConvUNet.use_batch_norm = True


# Model
InstanceSegmentationModel.num_classes = %num_classes
InstanceSegmentationModel.embedding_dims = 64
InstanceSegmentationModel.embedding_similarity_strategy = 'distance'
InstanceSegmentationModel.embedding_similarity_threshold = 0.5
InstanceSegmentationModel.apply_nms = True
InstanceSegmentationModel.nms_score_threshold = 0.1
InstanceSegmentationModel.nms_iou_threshold = 0.3
InstanceSegmentationModel.num_furthest_voxel_samples = 1000
InstanceSegmentationModel.sampler_score_vs_distance_coef = 0.5


# Evaluation
evaluation.input_fn = @get_tf_data_dataset
evaluation.model_class = @InstanceSegmentationModel
evaluation.num_quantitative_examples = 300
evaluation.num_qualitative_examples = 5

m1/InstanceSegmentationMetric.iou_threshold = 0.25
m1/InstanceSegmentationMetric.num_classes = %num_classes
m1/InstanceSegmentationMetric.label_map_path = %label_map_path

m2/InstanceSegmentationMetric.iou_threshold = 0.5
m2/InstanceSegmentationMetric.num_classes = %num_classes
m2/InstanceSegmentationMetric.label_map_path = %label_map_path

m3/InstanceSegmentationMetric.iou_threshold = 0.7
m3/InstanceSegmentationMetric.num_classes = %num_classes
m3/InstanceSegmentationMetric.label_map_path = %label_map_path

CustomTensorBoard.metric_classes = [
    @m1/InstanceSegmentationMetric,
    @m2/InstanceSegmentationMetric,
    @m3/InstanceSegmentationMetric
]
