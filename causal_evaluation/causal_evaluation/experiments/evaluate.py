# coding=utf-8
# Copyright 2025 The Google Research Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Runs evaluation for a given task.

This script is used to run several evaluation steps for a given task and write
the results to a file. This script is not intended to be generalizable and
is intended to be run on the outputs generated by colab/causal_adjustment.ipynb
or colab/acs_fit_models.ipynb.
It computes absolute and relative performance (a comparison of predictions to a
reference model).
It also computes weighted performance to match distributions across groups.
Results are written to a file.
"""

from collections.abc import Sequence
import itertools
import os

from absl import app
from absl import flags
from causal_evaluation import utils
import numpy as np
import pandas as pd


FLAGS = flags.FLAGS

flags.DEFINE_string(
    'data_directory',
    None,
    'Path to the data.',
    required=True,
)

flags.DEFINE_string(
    'pred_file_name',
    None,
    'Name of the prediction file.',
    required=True,
)

flags.DEFINE_string(
    'result_file_name',
    None,
    'Name of the result file.',
    required=True,
)

flags.DEFINE_integer('n_resamples', 1000, 'Number of resamples.')

flags.DEFINE_string('task', None, 'Name of the task.', required=True)

flags.DEFINE_string(
    'label_col_name',
    'labels',
    'Name of the label column.',
)

flags.DEFINE_string(
    'group_col_name',
    'group',
    'Name of the group column.',
)

TASK_COL_NAME = flags.DEFINE_string(
    'task_col_name',
    'task',
    'Name of the task column.',
)


def _map_weight_type(value: str) -> str:
  """Maps substrings in the weights column to a weight type."""
  weight_type_map = {
      'weights_population': 'population',
      'weights_stable': 'stable',
  }
  for substring, result in weight_type_map.items():
    if substring in value:
      return result
  return 'None'


def main(argv: Sequence[str]) -> None:
  if len(argv) > 1:
    raise app.UsageError('Too many command-line arguments.')
  print(f'task: {FLAGS.task}', flush=True)

  pred_df = pd.read_parquet(
      os.path.join(FLAGS.data_directory, FLAGS.pred_file_name)
  )
  num_groups = pred_df[FLAGS.group_col_name].nunique()

  result_dict = {}

  # Get relative performance
  result_dict[(FLAGS.task, 'relative_comparison', 'weights_none')] = (
      utils.run_evaluation(
          labels=pred_df[FLAGS.label_col_name],
          pred_probs=pred_df['pred_probs_y1_xa'],
          pred_probs_reference=pred_df['pred_probs_y1_x'],
          group=pred_df[FLAGS.group_col_name],
          n_resamples=FLAGS.n_resamples,
      )
  )

  # Get relative performance for the stratified model
  result_dict[
      (FLAGS.task, 'relative_comparison_stratified', 'weights_none')
  ] = utils.run_evaluation(
      labels=pred_df[FLAGS.label_col_name],
      pred_probs=pred_df['pred_probs_y1_xa_stratified'],
      pred_probs_reference=pred_df['pred_probs_y1_x'],
      group=pred_df[FLAGS.group_col_name],
      n_resamples=FLAGS.n_resamples,
  )

  # Get relative performance overall
  result_dict[(FLAGS.task, 'relative_comparison', 'weights_none_overall')] = (
      utils.run_evaluation(
          labels=pred_df[FLAGS.label_col_name],
          pred_probs=pred_df['pred_probs_y1_xa'],
          pred_probs_reference=pred_df['pred_probs_y1_x'],
          n_resamples=FLAGS.n_resamples,
      )
  )
  # Get relative performance overall for the stratified model
  result_dict[
      (FLAGS.task, 'relative_comparison_stratified', 'weights_none_overall')
  ] = utils.run_evaluation(
      labels=pred_df[FLAGS.label_col_name],
      pred_probs=pred_df['pred_probs_y1_xa_stratified'],
      pred_probs_reference=pred_df['pred_probs_y1_x'],
      n_resamples=FLAGS.n_resamples,
  )

  # Get absolute performance
  for feature_set in ['features_x', 'features_xa', 'features_xa_stratified']:
    feature_suffix = feature_set.split('features_')[-1]

    # Compute performance for each group
    result_dict[(FLAGS.task, feature_set, 'weights_none')] = (
        utils.run_evaluation(
            labels=pred_df[FLAGS.label_col_name],
            pred_probs=pred_df[f'pred_probs_y1_{feature_suffix}'],
            group=pred_df[FLAGS.group_col_name],
            n_resamples=FLAGS.n_resamples,
        )
    )

    # Compute performance overall
    result_dict[(FLAGS.task, feature_set, 'weights_none_overall')] = (
        utils.run_evaluation(
            labels=pred_df[FLAGS.label_col_name],
            pred_probs=pred_df[f'pred_probs_y1_{feature_suffix}'],
            n_resamples=FLAGS.n_resamples,
        )
    )

  # Set up for weighted evaluation
  weight_task_list = []
  for weight_task in itertools.product(
      ['x', 'xa', 'xa_stratified'],
      ['none', 'x', 'y', 'r'],
  ):
    feature_suffix = weight_task[0]
    weight_suffix = weight_task[1]
    if weight_suffix == 'r':
      weight_suffix = f'{weight_suffix}_{feature_suffix}'
    weight_task_list.append((feature_suffix, weight_suffix))

  # Run weighted evaluation

  for weight_task in weight_task_list:
    feature_suffix = weight_task[0]
    weight_suffix = weight_task[1]

    if weight_suffix == 'none':
      pred_probs_group = np.ones(
          (pred_df.shape[0], len(pred_df[FLAGS.group_col_name].unique()))
      )
    else:
      pred_probs_group = np.concatenate(
          pred_df[f'pred_probs_group_{weight_suffix}']
          .map(lambda x: x.reshape(1, -1))
          .values,
          axis=0,
      )

    ## Weights mapping total population to groups
    result_dict[(
        FLAGS.task,
        f'features_{feature_suffix}',
        f'weights_population_{weight_suffix}',
    )] = (
        pd.concat({
            i: utils.run_evaluation(
                labels=pred_df[FLAGS.label_col_name],
                pred_probs=pred_df[f'pred_probs_y1_{feature_suffix}'],
                sample_weight=pred_probs_group[:, i],
                n_resamples=FLAGS.n_resamples,
            )
            for i in range(num_groups)
        })
        .reset_index(level=-1, drop=True)
        .drop(columns=['group'])
        .rename_axis('group')
        .reset_index()
    )

    result_dict[(
        FLAGS.task,
        f'features_{feature_suffix}',
        f'population_on_group_comparison_weights_{weight_suffix}',
    )] = utils.run_evaluation(
        labels=pred_df[FLAGS.label_col_name],
        pred_probs=pred_df[f'pred_probs_y1_{feature_suffix}'],
        sample_weight=pred_probs_group,
        group=pred_df[FLAGS.group_col_name],
        n_resamples=FLAGS.n_resamples,
        weighted_population_on_group_comparison=True,
        normalize_population_on_group_comparison=False,
    )

    if weight_suffix != 'none':
      ## Stable weights -- mapping groups to overlap regime
      result_dict[(
          FLAGS.task,
          f'features_{feature_suffix}',
          f'weights_stable_{weight_suffix}',
      )] = utils.run_evaluation(
          labels=pred_df[FLAGS.label_col_name],
          pred_probs=pred_df[f'pred_probs_y1_{feature_suffix}'],
          group=pred_df[FLAGS.group_col_name],
          sample_weight=utils.compute_balancing_weights(
              pred_df[FLAGS.group_col_name],
              np.concatenate(
                  pred_df[f'pred_probs_group_{weight_suffix}']
                  .map(lambda x: x.reshape(1, -1))
                  .values,
                  axis=0,
              ),
              weight_type='stable',
          ),
          n_resamples=FLAGS.n_resamples,
      )

  result_df = (
      pd.concat(result_dict)
      .reset_index(level=-1, drop=True)
      .rename_axis([FLAGS.task_col_name, 'features', 'weights'])
      .reset_index()
  )

  result_df['weight_type'] = result_df['weights'].apply(_map_weight_type)
  result_df['group'] = result_df['group'].astype(str)

  # Write results to file
  pd.DataFrame(result_df).to_parquet(
      os.path.join(FLAGS.data_directory, FLAGS.result_file_name)
  )


if __name__ == '__main__':
  app.run(main)
