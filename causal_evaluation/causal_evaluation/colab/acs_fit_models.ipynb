{
  "cells": [
    {
      "metadata": {
        "id": "yV7l__wn9cDb"
      },
      "cell_type": "markdown",
      "source": [
        "## A colab for the American Community Survey Public Use Microdata Sample"
      ]
    },
    {
      "metadata": {
        "id": "uNPr0gW8Yv1F"
      },
      "cell_type": "markdown",
      "source": [
        "Copyright 2025 The Google Research Authors\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "```\n",
        " http://www.apache.org/licenses/LICENSE-2.0\n",
        "```\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
      ]
    },
    {
      "metadata": {
        "id": "T5-sk2CjY9BZ"
      },
      "cell_type": "markdown",
      "source": [
        "## Experiments with the American Community Survey (ACS) Public Use Microdata Sample (PUMS)"
      ]
    },
    {
      "metadata": {
        "id": "ojROW50-879u"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from folktables import ACSDataSource, BasicProblem\n",
        "from folktables.acs import adult_filter, public_coverage_filter\n",
        "from causal_evaluation import utils"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "SwGaY8kX9OO0"
      },
      "cell_type": "code",
      "source": [
        "data_root = './../../data/acs_pums/raw'  # @param\n",
        "preds_data_path = './../../data/acs_pums/' # @param\n",
        "year = 2018  # @param\n",
        "horizon = '5-Year'  # @param\n",
        "state = 'CA'  # @param"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "-sl0mpkUNcKA"
      },
      "cell_type": "code",
      "source": [
        "os.makedirs(data_root, exist_ok=True)\n",
        "os.makedirs(preds_data_path, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "VFxPG-iInQSP"
      },
      "cell_type": "code",
      "source": [
        "data_source = ACSDataSource(\n",
        "    survey_year=str(year), horizon=horizon, survey='person', root_dir=data_root\n",
        ")\n",
        "data_df = data_source.get_data(states=[state], download=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "5jP-SjnDX3OL"
      },
      "cell_type": "code",
      "source": [
        "definitions = data_source.get_definitions()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "fmyGV0LHKH_f"
      },
      "cell_type": "code",
      "source": [
        "# Get names for race field\n",
        "race_df = definitions.loc[definitions[1] == 'RAC1P'][[5, 6]]\n",
        "race_df.columns = ['RAC1P', 'RAC1P_NAME']\n",
        "race_df = race_df.query('~RAC1P.isna()')\n",
        "race_df['RAC1P'] = race_df['RAC1P'].astype(int)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "V3jJo--HXDzc"
      },
      "cell_type": "code",
      "source": [
        "def map_race_eth(df):\n",
        "  \"\"\"Maps race and ethnicity data to a single variable.\n",
        "\n",
        "  The logic is as follows:\n",
        "\n",
        "    * If ethnicity is Hispanic, then the combined variable is Hispanic.\n",
        "    * If ethnicity is not Hispanic, then the combined variable is the value of\n",
        "    the race variable.\n",
        "    * After mapping the ethnicity variable, rare categories are combined with\n",
        "    the \"Other\" category. The result is that American Indian, Alaskan Native,\n",
        "    and Pacific Islander groups are mapped to \"Other\".\n",
        "  \"\"\"\n",
        "  df['HISP_binary'] = (df['HISP'] != 1) * 1\n",
        "  df['HISP_binary'] = df['HISP_binary'].map({0: 'Non-Hispanic', 1: 'Hispanic'})\n",
        "  df = df.merge(race_df, how='left')\n",
        "  df['RACE_ETH_NAME'] = df['HISP_binary'].where(\n",
        "      df['HISP_binary'] == 'Hispanic', df['RAC1P_NAME']\n",
        "  )\n",
        "  race_eth_mapping = {\n",
        "      'White alone': 'White',\n",
        "      'Hispanic': 'Hispanic',\n",
        "      'Asian alone': 'Asian',\n",
        "      'Black or African American alone': 'Black',\n",
        "      'Two or More Races': 'Multiracial',\n",
        "      'Native Hawaiian and Other Pacific Islander alone': 'Other',\n",
        "      'Some Other Race alone': 'Other',\n",
        "      'American Indian and Alaska Native tribes specified; or American Indian or Alaska Native, not specified and no other races': (\n",
        "          'Other'\n",
        "      ),\n",
        "      'Alaska Native alone': 'Other',\n",
        "      'American Indian alone': 'Other',\n",
        "  }\n",
        "  df['RACE_ETH_NAME'] = df['RACE_ETH_NAME'].map(race_eth_mapping)\n",
        "  df['RACE_ETH'] = pd.Categorical(df['RACE_ETH_NAME']).codes\n",
        "  return df\n",
        "\n",
        "\n",
        "data_df = map_race_eth(data_df)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "FMmm1R7TYC6l"
      },
      "cell_type": "code",
      "source": [
        "display.display(data_df[['RAC1P', 'RAC1P_NAME']].value_counts())\n",
        "display.display(\n",
        "    data_df[['RAC1P', 'RAC1P_NAME', 'RACE_ETH', 'RACE_ETH_NAME']].value_counts()\n",
        ")\n",
        "display.display(data_df[['RACE_ETH', 'RACE_ETH_NAME']].value_counts())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "zf3XVyUcEzlI"
      },
      "cell_type": "code",
      "source": [
        "tasks = {\n",
        "    'ACSIncome': {\n",
        "        'task_spec': BasicProblem(\n",
        "            features=[\n",
        "                'AGEP',\n",
        "                'COW',\n",
        "                'SCHL',\n",
        "                'MAR',\n",
        "                'OCCP',\n",
        "                'POBP',\n",
        "                'RELP',\n",
        "                'WKHP',\n",
        "                'SEX',\n",
        "                'RACE_ETH',\n",
        "            ],\n",
        "            target='PINCP',\n",
        "            target_transform=lambda x: x \u003e 50000,\n",
        "            group='RACE_ETH',\n",
        "            preprocess=adult_filter,\n",
        "        ),\n",
        "        'categorical_cols': [\n",
        "            'COW',\n",
        "            'SCHL',\n",
        "            'MAR',\n",
        "            'OCCP',\n",
        "            'POBP',\n",
        "            'RELP',\n",
        "            'SEX',\n",
        "            'RACE_ETH',\n",
        "        ],\n",
        "        'numeric_cols': ['WKHP', 'AGEP'],\n",
        "    },\n",
        "    'ACSPublicCoverage': {\n",
        "        'task_spec': BasicProblem(\n",
        "            features=[\n",
        "                'AGEP',\n",
        "                'SCHL',\n",
        "                'MAR',\n",
        "                'SEX',\n",
        "                'DIS',\n",
        "                'ESP',\n",
        "                'CIT',\n",
        "                'MIG',\n",
        "                'MIL',\n",
        "                'ANC',\n",
        "                'NATIVITY',\n",
        "                'DEAR',\n",
        "                'DEYE',\n",
        "                'DREM',\n",
        "                'PINCP',\n",
        "                'ESR',\n",
        "                'ST',\n",
        "                'FER',\n",
        "                'RACE_ETH',\n",
        "            ],\n",
        "            target='PUBCOV',\n",
        "            target_transform=lambda x: x == 1,\n",
        "            group='RACE_ETH',\n",
        "            preprocess=public_coverage_filter,\n",
        "        ),\n",
        "        'categorical_cols': [\n",
        "            'SCHL',\n",
        "            'MAR',\n",
        "            'SEX',\n",
        "            'DIS',\n",
        "            'ESP',\n",
        "            'CIT',\n",
        "            'MIG',\n",
        "            'MIL',\n",
        "            'ANC',\n",
        "            'NATIVITY',\n",
        "            'DEAR',\n",
        "            'DEYE',\n",
        "            'DREM',\n",
        "            'ESR',\n",
        "            'ST',\n",
        "            'FER',\n",
        "            'RACE_ETH',\n",
        "        ],\n",
        "        'numeric_cols': ['AGEP', 'PINCP'],\n",
        "    },\n",
        "}"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "xdqwqLo8LrXt"
      },
      "cell_type": "code",
      "source": [
        "group_id_map_df = data_df[['RACE_ETH', 'RACE_ETH_NAME']].drop_duplicates()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "HW4wZ0SaJn6-"
      },
      "cell_type": "code",
      "source": [
        "data_dict = {}\n",
        "for task in tasks.keys():\n",
        "  data_dict[task] = {}\n",
        "  (\n",
        "      data_dict[task]['features'],\n",
        "      data_dict[task]['labels'],\n",
        "      data_dict[task]['group'],\n",
        "  ) = tasks[task]['task_spec'].df_to_pandas(data_df)\n",
        "  data_dict[task]['labels'] = data_dict[task]['labels'].values.squeeze()\n",
        "  data_dict[task]['group'] = data_dict[task]['group']\n",
        "  group_name_df = data_dict[task]['group'].merge(group_id_map_df, how='left')\n",
        "  data_dict[task]['group'] = data_dict[task]['group'].values.squeeze()\n",
        "  data_dict[task]['group_name'] = group_name_df['RACE_ETH_NAME'].values"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "CN_LNwGxdeHf"
      },
      "cell_type": "code",
      "source": [
        "# Check missing values\n",
        "for task in data_dict.keys():\n",
        "  print(task)\n",
        "  print(data_dict[task]['features'].isna().sum())  # Missing values OK\n",
        "  assert np.isnan(data_dict[task]['labels']).sum() == 0"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "GUxUtI5Rgg3M"
      },
      "cell_type": "code",
      "source": [
        "group_labels_df = pd.concat(\n",
        "    pd.DataFrame(\n",
        "        {'task': key, 'labels': value['labels'], 'group': value['group']}\n",
        "    )\n",
        "    for key, value in data_dict.items()\n",
        ").merge(group_id_map_df, left_on='group', right_on='RACE_ETH')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "oJDbT7SgkaNk"
      },
      "cell_type": "code",
      "source": [
        "# Get a table with the counts and prevalence of the outcome across groups for all tasks\n",
        "count_prevalence_df = (\n",
        "    group_labels_df.groupby(['task', 'RACE_ETH_NAME'])[['labels']]\n",
        "    .agg(count=('labels', lambda x: x.count()), prevalence=('labels', 'mean'))\n",
        "    .reset_index()\n",
        ")\n",
        "count_prevalence_df_long = count_prevalence_df.melt(\n",
        "    id_vars=['task', 'RACE_ETH_NAME']\n",
        ")\n",
        "count_prevalence_df_wide = count_prevalence_df_long.pivot(\n",
        "    index=['RACE_ETH_NAME'], columns=['task', 'variable']\n",
        ").sort_index(level=0, axis=1)\n",
        "for col in count_prevalence_df_wide.columns:\n",
        "  if col[-1] == 'prevalence':\n",
        "    count_prevalence_df_wide[col] = count_prevalence_df_wide[col].map(\n",
        "        lambda x: f'{x:.3f}'\n",
        "    )\n",
        "  if col[-1] == 'count':\n",
        "    count_prevalence_df_wide[col] = count_prevalence_df_wide[col].map(\n",
        "        lambda x: f'{x:,.0f}'\n",
        "    )\n",
        "display.display(count_prevalence_df_wide)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "knoDGe17qT-z"
      },
      "cell_type": "markdown",
      "source": [
        "## Prepare the data for model fitting"
      ]
    },
    {
      "metadata": {
        "id": "lBaG7HxRHJ6W"
      },
      "cell_type": "code",
      "source": [
        "def map_rare_categories_df(x, max_categories=255):\n",
        "  \"\"\"For a pd.DataFrame maps rare categories in columns with more than max_categories values to a new category.\n",
        "\n",
        "  For numeric data, this is column.max() + 1, for categorical data this is a\n",
        "  category called 'RARE_CATEGORY'.\n",
        "\n",
        "  This preprocessing is necessary for use of\n",
        "  sklearn.HistGradientBoostingClassifier.\n",
        "  \"\"\"\n",
        "  x = x.copy()\n",
        "  for column in x.columns:\n",
        "    counts = x[column].value_counts()\n",
        "    if len(counts) \u003e max_categories:\n",
        "      rare_categories = counts.iloc[(max_categories - 1) :]\n",
        "    else:\n",
        "      continue\n",
        "    if pd.api.types.is_numeric_dtype(x[column].dtype):\n",
        "      replace_value = x[column].max() + 1\n",
        "    else:\n",
        "      replace_value = 'RARE_CATEGORY'\n",
        "    x[column] = x[column].replace(\n",
        "        {key: replace_value for key in list(rare_categories.index)}\n",
        "    )\n",
        "  return x"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "I7PQRKaoNn3a"
      },
      "cell_type": "code",
      "source": [
        "# Define preprocessors to map rare categories\n",
        "for task in data_dict.keys():\n",
        "  data_dict[task][\"preprocessor\"] = ColumnTransformer(\n",
        "      transformers=[\n",
        "          (\n",
        "              \"numerical\",\n",
        "              FunctionTransformer(lambda x: x),\n",
        "              tasks[task][\"numeric_cols\"],\n",
        "          ),\n",
        "          (\n",
        "              \"categorical\",\n",
        "              FunctionTransformer(map_rare_categories_df),\n",
        "              tasks[task][\"categorical_cols\"],\n",
        "          ),\n",
        "      ]\n",
        "  )\n",
        "  data_dict[task][\"features_processed\"] = data_dict[task][\n",
        "      \"preprocessor\"\n",
        "  ].fit_transform(data_dict[task][\"features\"])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "bU5rPaHCONaP"
      },
      "cell_type": "code",
      "source": [
        "# Record the categorical indices\n",
        "for task in data_dict.keys():\n",
        "  data_dict[task]['categorical_indices'] = [\n",
        "      list(tasks[task]['numeric_cols'] + tasks[task]['categorical_cols']).index(\n",
        "          col\n",
        "      )\n",
        "      for col in tasks[task]['categorical_cols']\n",
        "  ]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "znHxrEnktV5X"
      },
      "cell_type": "code",
      "source": [
        "# Split the data\n",
        "for task in data_dict.keys():\n",
        "  stratify_col = (\n",
        "      data_dict[task]['features']['RACE_ETH'].astype(str)\n",
        "      + '-'\n",
        "      + pd.Series(data_dict[task]['labels']).astype(str)\n",
        "  )\n",
        "  (\n",
        "      data_dict[task]['features_train'],\n",
        "      data_dict[task]['features_test'],\n",
        "      data_dict[task]['labels_train'],\n",
        "      data_dict[task]['labels_test'],\n",
        "      data_dict[task]['group_train'],\n",
        "      data_dict[task]['group_test'],\n",
        "  ) = train_test_split(\n",
        "      data_dict[task]['features_processed'],\n",
        "      data_dict[task]['labels'],\n",
        "      data_dict[task]['group'],\n",
        "      test_size=0.2,\n",
        "      stratify=stratify_col,\n",
        "      random_state=14,\n",
        "  )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "12xvYtRi4tKb"
      },
      "cell_type": "code",
      "source": [
        "# Get a separate feature array after removing RACE_ETH from the feature set\n",
        "def delete_and_shift(x, delete_value):\n",
        "  result = []\n",
        "  for value in x:\n",
        "    if value \u003c delete_value:\n",
        "      result.append(value)\n",
        "    elif value \u003e delete_value:\n",
        "      result.append(value - 1)\n",
        "    else:\n",
        "      continue\n",
        "  return result\n",
        "\n",
        "\n",
        "for task in data_dict.keys():\n",
        "  print(task)\n",
        "  # Create new feature arrays\n",
        "  race_eth_feature_index = list(data_dict[task]['features'].columns).index(\n",
        "      'RACE_ETH'\n",
        "  )\n",
        "  data_dict[task]['features_population_train'] = np.delete(\n",
        "      data_dict[task]['features_train'], race_eth_feature_index, axis=1\n",
        "  )\n",
        "  data_dict[task]['features_population_test'] = np.delete(\n",
        "      data_dict[task]['features_test'], race_eth_feature_index, axis=1\n",
        "  )\n",
        "  data_dict[task]['categorical_indices_population'] = delete_and_shift(\n",
        "      data_dict[task]['categorical_indices'], race_eth_feature_index\n",
        "  )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "VXl_yU_3Xtl2"
      },
      "cell_type": "code",
      "source": [
        "# Create a LabelEncoder for group membership\n",
        "for task in data_dict.keys():\n",
        "  group_encoder = LabelEncoder()\n",
        "  data_dict[task]['group_encoder'] = group_encoder.fit(\n",
        "      data_dict[task]['group_train']\n",
        "  )\n",
        "  data_dict[task]['group_encoded_train'] = group_encoder.transform(\n",
        "      data_dict[task]['group_train']\n",
        "  )\n",
        "  data_dict[task]['group_encoded_test'] = group_encoder.transform(\n",
        "      data_dict[task]['group_test']\n",
        "  )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "RYrDfJZI4M7l"
      },
      "cell_type": "markdown",
      "source": [
        "## Fit models"
      ]
    },
    {
      "metadata": {
        "id": "M_QtdQYt4Oe_"
      },
      "cell_type": "code",
      "source": [
        "# Initialize a dictionary to hold predictions\n",
        "preds_dict = {}"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "bEgDLrrj-3tm"
      },
      "cell_type": "code",
      "source": [
        "model_type = 'gradient_boosting'"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "aSBM7l9VYrmr"
      },
      "cell_type": "code",
      "source": [
        "for task in data_dict.keys():\n",
        "  print(task)\n",
        "\n",
        "  preds_dict[task] = {}\n",
        "  # Population model\n",
        "\n",
        "  model_population = utils.fit_model(\n",
        "      data_dict[task]['features_population_train'],\n",
        "      data_dict[task]['labels_train'],\n",
        "      model_type=model_type,\n",
        "      model_kwarg_dict={\n",
        "          'categorical_features': data_dict[task][\n",
        "              'categorical_indices_population'\n",
        "          ]\n",
        "      },\n",
        "      model_cross_val=True,\n",
        "  )\n",
        "  preds_dict[task]['pred_probs_y_x'] = model_population.predict_proba(\n",
        "      data_dict[task]['features_population_test'],\n",
        "  )\n",
        "  preds_dict[task]['pred_probs_y1_x'] = preds_dict[task]['pred_probs_y_x'][\n",
        "      :, -1\n",
        "  ]\n",
        "\n",
        "  # XA model\n",
        "  model = utils.fit_model(\n",
        "      data_dict[task]['features_train'],\n",
        "      data_dict[task]['labels_train'],\n",
        "      model_type=model_type,\n",
        "      model_kwarg_dict={\n",
        "          'categorical_features': data_dict[task]['categorical_indices']\n",
        "      },\n",
        "      model_cross_val=True,\n",
        "  )\n",
        "  preds_dict[task]['pred_probs_y_xa'] = model.predict_proba(\n",
        "      data_dict[task]['features_test'],\n",
        "  )\n",
        "  preds_dict[task]['pred_probs_y1_xa'] = preds_dict[task]['pred_probs_y_xa'][\n",
        "      :, -1\n",
        "  ]\n",
        "\n",
        "  # Stratified\n",
        "  model_dict = utils.fit_model_stratified(\n",
        "      data_dict[task]['features_population_train'],\n",
        "      data_dict[task]['labels_train'],\n",
        "      group=data_dict[task]['group_train'],\n",
        "      model_type=model_type,\n",
        "      model_cross_val=True,\n",
        "      model_kwarg_dict={\n",
        "          'categorical_features': data_dict[task][\n",
        "              'categorical_indices_population'\n",
        "          ]\n",
        "      },\n",
        "  )\n",
        "\n",
        "  # Predict y in the eval data\n",
        "  preds_dict[task]['pred_probs_y_xa_stratified'] = utils.array_to_series(\n",
        "      utils.predict_proba_stratified(\n",
        "          data_dict[task]['features_population_test'],\n",
        "          model_dict,\n",
        "          group=data_dict[task]['group_test'],\n",
        "      )\n",
        "  )\n",
        "  preds_dict[task]['pred_probs_y1_xa_stratified'] = preds_dict[task][\n",
        "      'pred_probs_y_xa_stratified'\n",
        "  ].map(lambda x: x[-1])\n",
        "\n",
        "  # # Fit model to predict P(A | X)\n",
        "  model_group_x = utils.fit_model(\n",
        "      data_dict[task]['features_population_train'],\n",
        "      data_dict[task]['group_encoded_train'],\n",
        "      model_type=model_type,\n",
        "      model_kwarg_dict={\n",
        "          'categorical_features': data_dict[task][\n",
        "              'categorical_indices_population'\n",
        "          ]\n",
        "      },\n",
        "      model_cross_val=True,\n",
        "  )\n",
        "  # Apply P(A | X) model to the test data\n",
        "  preds_dict[task]['pred_probs_group_x'] = model_group_x.predict_proba(\n",
        "      data_dict[task]['features_population_test'],\n",
        "  )\n",
        "\n",
        "  # Fit model to predict P(A | Y)\n",
        "  model_group_y = utils.fit_model(\n",
        "      data_dict[task]['labels_train'].reshape(-1, 1),\n",
        "      data_dict[task]['group_encoded_train'],\n",
        "      model_type=model_type,\n",
        "      model_cross_val=True,\n",
        "  )\n",
        "  # Apply P(A | Y) model to the test data\n",
        "  preds_dict[task]['pred_probs_group_y'] = model_group_y.predict_proba(\n",
        "      data_dict[task]['labels_test'].reshape(-1, 1)\n",
        "  )\n",
        "  preds_dict[task]['pred_probs_group_r_x'] = utils.fit_cross_val_predict(\n",
        "      scipy.special.logit(preds_dict[task]['pred_probs_y_x'][:, -1]).reshape(\n",
        "          -1, 1\n",
        "      ),\n",
        "      data_dict[task]['group_encoded_test'],\n",
        "      model_type=model_type,\n",
        "      model_cross_val=True,\n",
        "  )\n",
        "  preds_dict[task]['pred_probs_group_r_xa'] = utils.fit_cross_val_predict(\n",
        "      scipy.special.logit(preds_dict[task]['pred_probs_y_xa'][:, -1]).reshape(\n",
        "          -1, 1\n",
        "      ),\n",
        "      data_dict[task]['group_encoded_test'],\n",
        "      model_type=model_type,\n",
        "      model_cross_val=True,\n",
        "  )\n",
        "  preds_dict[task]['pred_probs_group_r_xa_stratified'] = (\n",
        "      utils.fit_cross_val_predict(\n",
        "          scipy.special.logit(\n",
        "              preds_dict[task]['pred_probs_y_xa_stratified']\n",
        "              .map(lambda x: x[-1])\n",
        "              .values\n",
        "          ).reshape(-1, 1),\n",
        "          data_dict[task]['group_encoded_test'],\n",
        "          model_type=model_type,\n",
        "          model_cross_val=True,\n",
        "      )\n",
        "  )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "XbN4zy3eF67J"
      },
      "cell_type": "code",
      "source": [
        "group_id_map_df.columns = ['group', 'group_name']"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "4x0hZ4Y4xXjF"
      },
      "cell_type": "code",
      "source": [
        "for task in preds_dict.keys():\n",
        "  print(task)\n",
        "  preds_file_name = f'preds_{task}_{horizon}_{year}_{model_type}.parquet'\n",
        "  preds_df = pd.DataFrame({\n",
        "      key: utils.array_to_series(value)\n",
        "      for key, value in preds_dict[task].items()\n",
        "  })\n",
        "  preds_df['labels'] = data_dict[task]['labels_test'] * 1\n",
        "  preds_df['group'] = data_dict[task]['group_test']\n",
        "  preds_df = preds_df.merge(group_id_map_df, how='left')\n",
        "  preds_df.to_parquet(\n",
        "      os.path.join(preds_data_path, preds_file_name), index=False\n",
        "  )"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "colab": {
      "name": "acs_fit_models.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
