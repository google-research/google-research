{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkdnLiKk71g-"
      },
      "source": [
        "##### Copyright 2024 Google Inc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0asMuNro71hA"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOoO2k9AS_ix"
      },
      "source": [
        "# DrJAX - Differentiable MapReduce Primitives in JAX\n",
        "\n",
        "In this colab, we will learn what DrJAX is, how to use it, and why it was designed. We'll go over a couple of JAX-related topics on the way, but some minor familiarity with JAX may be useful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wArQh8SwVKu1"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "091t-CEOxmNU"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade drjax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRawRHQeJ1hW"
      },
      "outputs": [],
      "source": [
        "import drjax\n",
        "import jax\n",
        "import jax.numpy as jnp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xrwn2ZHyTTt6"
      },
      "source": [
        "# What is DrJAX?\n",
        "\n",
        "DrJAX has a few goals.\n",
        "\n",
        "1. Create a JAX authoring surface for MapReduce-style algorithms built on JAX primitives.\n",
        "2. Enable the expression of differentiable MapReduce computations (in the style of [Federated Automatic Differentiation](https://arxiv.org/abs/2301.07806). We refer to this MapReduce AD.\n",
        "3. Ensure that DrJAX algorithms are as scalable and efficient as possible, especially when sharding across accelerators.\n",
        "\n",
        "We will discuss only (1) and (2) in this colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow6te-MvUPTS"
      },
      "source": [
        "## What is MapReduce AD?\n",
        "\n",
        "Suppose I have a computation that computes a loss value using MapReduce-style operations. In many settings, we want to compute the **derivative** of this function - `dz/dx`. This allows us to do things like gradient descent.\n",
        "\n",
        "For non-MapReduce computations, in frameworks like TF, PyTorch, JAX, we can just call `grad(foo)` For example, in JAX we can do the following:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqI_D5bvUoUh"
      },
      "outputs": [],
      "source": [
        "def square_and_dot(x, y):\n",
        "  return jnp.dot(jnp.square(x), y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 56,
          "status": "ok",
          "timestamp": 1706149717284,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 480
        },
        "id": "MKWJKlTdVzsg",
        "outputId": "965e9495-709f-4b6c-84cd-0c5882949b25"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Array(20., dtype=float32)"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = jnp.array([1.0, -3.0])\n",
        "y = jnp.array([2.0, 2.0])\n",
        "square_and_dot(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaLKVVxtVBJb"
      },
      "source": [
        "To get the derivative of the output with respect to `x`, we can just use `jax.grad`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 55,
          "status": "ok",
          "timestamp": 1706149717591,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 480
        },
        "id": "scFCqL9xU7G5",
        "outputId": "025e3ef9-2de2-40fb-f86d-624170d08726"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Array([  4., -12.], dtype=float32)"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "jax.grad(square_and_dot)(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEfs4KOFWAUP"
      },
      "source": [
        "With a single call to `jax.grad`, we can compute the derivative. This is using what's known as **automatic differentiation** (AD).\n",
        "\n",
        "We would like to be able to use this for **MapReduce-style computations**. While the [federated AD paper](https://arxiv.org/abs/2301.07806) gives a theoretical framework for doing this, it does not have any direct implementation.\n",
        "\n",
        "This is where DrJAX comes in. DrJAX defines MapReduce primitives (eg. `broadcast`, `map_fn`, `reduce_sum`.) as JAX primitives. This allows JAX to differentiate through them automatically, enabling MapReduce AD!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDv6CtR2WnLN"
      },
      "source": [
        "# DrJAX and MapReduce Primitives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcXrZIDLWsa2"
      },
      "source": [
        "Let's take a look at how we can define MapReduce computations in DrJAX. Conceptually, DrJAX operates on two different kinds of values: non-partitioned values (these are just standard values) and **partitioned** values. The latter are values that are partitioned across workers in a MapReduce computation. We will also sometimes refer to these as unplaced and placed values. Intuitively, the \"placement\" is an extra data dimension over which we MapReduce."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1706149717845,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 480
        },
        "id": "hfVwpWCDXNwa",
        "outputId": "da65afe9-c24a-4edd-d492-6936a4154662"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2,)"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nonpartitioned_value = jnp.array([-1.0, 1.0])\n",
        "nonpartitioned_value.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06S7lGrWXODX"
      },
      "source": [
        "For non-partitioned values, we add an extra axis to our tensors. This axis represents the partition of data on which MapReduce will operate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1706149718145,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 480
        },
        "id": "FbF8WdB6W98X",
        "outputId": "4a324640-fb86-47ec-e666-bd318194c988"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3, 2)"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "partitioned_value = jnp.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
        "partitioned_value.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I41HBhtzXDj3"
      },
      "source": [
        "In the above code, we have an example with 3 groups of data, each of which has a 1-d tensor of shape `(2,)`. There is also has a non-partitioned 1-d tensor of shape `(2,)`. Let's define a map to transform such vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1706149718439,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 480
        },
        "id": "0sf7VOI7XX_H",
        "outputId": "29bfd432-f1cf-430b-eda8-7caad70b8617"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0. 3.]\n",
            "[[2. 4.]\n",
            " [4. 6.]\n",
            " [6. 8.]]\n"
          ]
        }
      ],
      "source": [
        "def add_constant(x):\n",
        "  return x + jnp.array([1.0, 2.0])\n",
        "\n",
        "print(add_constant(nonpartitioned_value))\n",
        "print(add_constant(partitioned_value))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4JUrbiAXkrp"
      },
      "source": [
        "We see something interesting here - JAX can recognize additional axes, as in the case of the partitioned value, and automatically tries to vectorize."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wh8zvpK-X1mx"
      },
      "source": [
        "Now let's map a function over the partitioned values via `drjax.map_fn`. We have to tell DrJAX the name of our MapReduce axis (in this case we call it `data_groups`), and how many groups of data we are mapping over (in this case, we'll use 3)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 53,
          "status": "ok",
          "timestamp": 1706149718785,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 480
        },
        "id": "gVA-6c7dXZ3L",
        "outputId": "2f57707c-3ce0-4c90-f2a1-5bbe23def38e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Array([[2., 4.],\n",
              "       [4., 6.],\n",
              "       [6., 8.]], dtype=float32)"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "@drjax.program(placements={'data_groups': 3})\n",
        "def partitioned_add_one(x):\n",
        "  return drjax.map_fn(add_constant, x)\n",
        "\n",
        "partitioned_add_one(partitioned_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30tVSSZSZXba"
      },
      "source": [
        "We see the same thing! Let's try another primitive, `drjax.broadcast`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 54,
          "status": "ok",
          "timestamp": 1706149719104,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 480
        },
        "id": "8KgTNTXPX9wr",
        "outputId": "3486691b-2458-483e-b033-fa722a6cce30"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Array([[-1.,  1.],\n",
              "       [-1.,  1.],\n",
              "       [-1.,  1.]], dtype=float32)"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "@drjax.program(placements={'data_groups': 3})\n",
        "def broadcast(x):\n",
        "  return drjax.broadcast(x)\n",
        "\n",
        "broadcast(nonpartitioned_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVZZdc-xZgcm"
      },
      "source": [
        "This does what we expect - we send the same vector to all data groups. So far, no real surprises. The cool thing though, is that we can apply differentiation, just as we did above.\n",
        "\n",
        "The only real restriction is that we want to differentiate nonpartitioned values, with respect to nonpartitioned values. So let's do that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 76,
          "status": "ok",
          "timestamp": 1706149719434,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 480
        },
        "id": "vEkNGpTYZuJK",
        "outputId": "88bf9156-982b-4e4f-e7f6-55c232f0be35"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Array(6., dtype=float32)"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "@drjax.program(placements={'data_groups': 3})\n",
        "def broadcast_and_sum(x):\n",
        "  broadcast_x = drjax.broadcast(x)\n",
        "  return drjax.reduce_sum(broadcast_x)\n",
        "\n",
        "broadcast_and_sum(2.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyPnPtw5Z8pN"
      },
      "source": [
        "Now we differentiate using *reverse-mode AD* (more on that at the end). We have to tell JAX which arg to differentiate with respect to, for posterity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 103,
          "status": "ok",
          "timestamp": 1706149719795,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 480
        },
        "id": "uBNPvVj8Z5u5",
        "outputId": "c4359a9f-8e9d-4e9c-ba42-35598955b5e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Array(3., dtype=float32)"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "jax.grad(broadcast_and_sum, argnums=0)(2.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JrQoRBbZkIH"
      },
      "source": [
        "Why is this the derivative? Well, let's think about what this function is. We get something like:\n",
        "\n",
        "```\n",
        "x -\u003e [x, x, x] -\u003e sum([x, x, x]) = 3x\n",
        "```\n",
        "Taking a derivative with respect to `x`, we should get 3!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgUfHsFDWfUf"
      },
      "source": [
        "# Linear Regression and MapReduce AD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InndTb0EWiUV"
      },
      "source": [
        "Let's see how this all works in a more interesting example. We're going to do something akin to linear regression.\n",
        "\n",
        "Let's assume all data groups have their own 2d vector `y`. Given a 2d linear regression model `x`, we'll set up our objective function as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlHgd2_fL2P3"
      },
      "outputs": [],
      "source": [
        "def compute_loss(x, y):\n",
        "  return 0.5*jnp.square(jnp.dot(x, y) - 1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKwDxb6fbCaH"
      },
      "source": [
        "Essentially, this is doing linear regression where (1) each data group has a single example and (2) all groups have label 1.0 for that example. This is not an important observation, the point is that `x, y` go in, and we get out some scalar loss.\n",
        "\n",
        "Let's try it out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 73,
          "status": "ok",
          "timestamp": 1706149720434,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 480
        },
        "id": "XNjSjJ-ePwk8",
        "outputId": "72eda4dd-beef-4a94-bc48-308c88436aec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Array(364.5, dtype=float32)"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "compute_loss(jnp.array([2.0, 3.0]), jnp.array([5.0, 6.0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY6cJzBVbScR"
      },
      "source": [
        "Great! Now, we can evaluate across data groups with this loss function. To do that we will:\n",
        "\n",
        "1. Broadcast `x` across the MapReduce axis (ie. the `data_groups`).\n",
        "2. Do `compute_loss(x, y)` for each group.\n",
        "3. Average the results.\n",
        "\n",
        "We can do that in DrJAX as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 185,
          "status": "ok",
          "timestamp": 1706149720871,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 480
        },
        "id": "b8_s9bh8PydP",
        "outputId": "bda2597c-e5f6-4132-b995-98af82a5161e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Array(87.16667, dtype=float32)"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "@drjax.program(placements={'data_groups': 3})\n",
        "def partitioned_eval(model, partitioned_data):\n",
        "  broadcast_vector = drjax.broadcast(model)\n",
        "  partitioned_losses = drjax.map_fn(compute_loss, (broadcast_vector, partitioned_data))\n",
        "  return drjax.reduce_mean(partitioned_losses)\n",
        "\n",
        "model = jnp.array([2.0, -1.0])\n",
        "partitioned_data = jnp.array([[1.0, 2.0], [3.0, -4.0], [-7.0, 6.0]])\n",
        "partitioned_eval(model, partitioned_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c6I7j2QbkXS"
      },
      "source": [
        "Just as above, we can use `jax.grad` to differentiate through this function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 292,
          "status": "ok",
          "timestamp": 1706149721470,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 480
        },
        "id": "0SRZMRPuR6mu",
        "outputId": "03f59eb7-96f1-478d-e65b-76039eabb271"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Array([ 57.666668, -54.666668], dtype=float32)"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grad_fn = jax.grad(partitioned_eval, argnums=0)\n",
        "grad_fn(model, partitioned_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9HcGfN8bofE"
      },
      "source": [
        "Let's pause here to think about what this means.\n",
        "\n",
        "We have some loss function $\\ell(x, y)$. Our partitioned evaluation is computing, for some partitioned set of data $C$:\n",
        "\n",
        "$$\n",
        "\\dfrac{1}{|C|}\\sum_{i \\in C} \\ell(x, y_i).\n",
        "$$\n",
        "\n",
        "By taking a derivative we get:\n",
        "\n",
        "$$\n",
        "\\dfrac{d}{dx}\\left(\\dfrac{1}{|C|}\\sum_{i \\in C} \\ell(x, y_i) \\right) = \\dfrac{1}{|C|} \\sum_{i \\in C} \\dfrac{d\\ell}{dx}(x, y_i).\n",
        "$$\n",
        "\n",
        "In other words we are just computing **the average gradient across the partitioned data**. This is exactly what distributed SGD does! The only missing ingredient is to use that derivative to update our model. We'll do that via gradient descent with learning rate 0.01."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 206,
          "status": "ok",
          "timestamp": 1706149721928,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 480
        },
        "id": "znvCCrWYSeu5",
        "outputId": "eeb1cefc-2c0e-4f91-b8b9-d0e351024e71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "87.16667\n",
            "35.551258\n"
          ]
        }
      ],
      "source": [
        "@drjax.program(placements={'data_groups': 3})\n",
        "def distributed_sgd_step(model, partitioned_data):\n",
        "  grad = grad_fn(model, partitioned_data)\n",
        "  return model - 0.01 * grad\n",
        "\n",
        "print(partitioned_eval(model, partitioned_data))\n",
        "updated_model = distributed_sgd_step(model, partitioned_data)\n",
        "print(partitioned_eval(updated_model, partitioned_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSX-tgF8dFX4"
      },
      "source": [
        "As expected, after applying a step of distributed SGD, we get a vector with lower loss!\n",
        "\n",
        "Moreover, going from `partitioned_Eval -\u003e distributed SGD` was essentially trivial - we just applied MapReduce AD!"
      ]
    },
    {
      "metadata": {
        "id": "7Sle5BNYSRTi"
      },
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "Above, we showed how to use DrJAX to define MapReduce-style computations, and how to apply MapReduce AD to differentiate through them. We encourage you to try out your own MapReduce-style computations, especially at scale."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
