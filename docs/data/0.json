{
    "0": {
        "file_id": 0,
        "content": "/video_structure/README.md",
        "type": "filepath"
    },
    "1": {
        "file_id": 0,
        "content": "This code repository implements unsupervised learning of object structure and dynamics from videos, written in Python and requiring specific packages. It has been tested on Linux and can run on GPU or CPU depending on the environment variable.",
        "type": "summary"
    },
    "2": {
        "file_id": 0,
        "content": "# Unsupervised Learning of Object Structure and Dynamics from Videos\nThis reprository contains code for the model described in\nhttps://arxiv.org/abs/1906.07889.\nAlso see the [project website](https://mjlm.github.io/video_structure/).\n## Usage\n### Installation\nThis code has been tested on Linux.\n1. Clone the repository.\n  ```\n  git clone -b master --single-branch https://github.com/google-research/google-research.git\n  ```\n2. Make sure `google-research` is the current directory:\n  ```\n  cd google-research\n  ```\n3. Create and activate a new virtualenv:\n  ```\n  virtualenv -p python3 video_structure\n  source video_structure/bin/activate\n  ```\n4. Install required packages:\n  ```\n  pip install -r video_structure/requirements.txt\n  ```\n### Run minimal example\nRun on GPU device 0:\n```\nCUDA_VISIBLE_DEVICES=0 python -m video_structure.train\n```\nRun on CPU:\n```\nCUDA_VISIBLE_DEVICES= python -m video_structure.train\n```\n## Citation\n```\n@inproceedings{minderer2019unsupervised,\n\ttitle = {Unsupervised Learning of Object Structure and Dynamics from Videos},",
        "type": "code",
        "location": "/video_structure/README.md:1-57"
    },
    "3": {
        "file_id": 0,
        "content": "This code repository contains the implementation of a model for unsupervised learning of object structure and dynamics from videos. The code is written in Python and requires specific packages, which can be installed via pip. It has been tested on Linux, and can run either on GPU or CPU depending on the CUDA_VISIBLE_DEVICES environment variable. The authors provide a minimal example for running the code and ask to cite their paper when using this implementation.",
        "type": "comment"
    },
    "4": {
        "file_id": 0,
        "content": "\tauthor = {Minderer, Matthias and Sun, Chen and Villegas, Ruben and Cole, Forrester and Murphy, Kevin and Lee, Honglak},\n\tbooktitle = {arXiv: 1906.07889},\n\tyear = {2019},\n}\n```",
        "type": "code",
        "location": "/video_structure/README.md:58-62"
    },
    "5": {
        "file_id": 0,
        "content": "The code represents a citation for a research paper published on the arXiv preprint server in 2019. The paper was authored by Minderer, Sun, Villegas, Cole, Murphy, and Lee.",
        "type": "comment"
    },
    "6": {
        "file_id": 1,
        "content": "/video_structure/__init__.py",
        "type": "filepath"
    },
    "7": {
        "file_id": 1,
        "content": "This code block is a license notice for the \"google-research/video_structure\" package. It states copyright information, licensing terms (Apache License, Version 2.0), and the absence of warranties or conditions, encouraging compliance with the given license.",
        "type": "summary"
    },
    "8": {
        "file_id": 1,
        "content": "# coding=utf-8\n# Copyright 2023 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.",
        "type": "code",
        "location": "/video_structure/__init__.py:1-14"
    },
    "9": {
        "file_id": 1,
        "content": "This code block is a license notice for the \"google-research/video_structure\" package. It states copyright information, licensing terms (Apache License, Version 2.0), and the absence of warranties or conditions, encouraging compliance with the given license.",
        "type": "comment"
    },
    "10": {
        "file_id": 2,
        "content": "/video_structure/datasets.py",
        "type": "filepath"
    },
    "11": {
        "file_id": 2,
        "content": "This code defines TensorFlow pipeline functions to load sequence datasets, generates a dataset object with various arguments, and includes utility functions for data manipulation. The main function splits sequence dictionaries into chunks while handling missing keys and returns shuffled sequence chunks as a TensorFlow dataset.",
        "type": "summary"
    },
    "12": {
        "file_id": 2,
        "content": "# coding=utf-8\n# Copyright 2023 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Load sequence datasets into tf.data.Dataset pipeline.\"\"\"\nimport functools\nimport os\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n# Data fields used by the model:\nREQUIRED_DATA_FIELDS = ['image', 'true_object_pos']\ndef get_sequence_dataset(data_dir,\n                         batch_size,\n                         num_timesteps,\n                         file_glob='*.npz',\n                         random_offset=True,",
        "type": "code",
        "location": "/video_structure/datasets.py:1-31"
    },
    "13": {
        "file_id": 2,
        "content": "This code imports various libraries and defines functions to load sequence datasets into a TensorFlow pipeline. It takes in directory, batch size, number of timesteps, file glob pattern for the data files, and an optional parameter for randomizing the offsets of the timesteps. The dataset is returned as a TensorFlow Dataset object.",
        "type": "comment"
    },
    "14": {
        "file_id": 2,
        "content": "                         repeat_dataset=True,\n                         seed=0):\n  \"\"\"Returns a tf.data.Dataset object for a Numpy image sequence dataset.\n  Args:\n    data_dir: Directory containing Numpy files where each file contains an image\n      sequence and ground-truth object coordinates.\n    batch_size: Desired number of sequences per batch in the output dataset.\n    num_timesteps: Desired sequence length in the output dataset.\n    file_glob: Glob pattern to sub-select files in data_dir.\n    random_offset: If True, a random number of frames will be dropped from the\n      start of the input sequence before dividing it into chunks of length\n      num_timesteps.\n    repeat_dataset: If True, output dataset will repeat forever.\n    seed: Random seed for shuffling.\n  Returns:\n    A tf.data.Dataset, or a one-shot iterator of the dataset if return_iterator\n    is True.\n  Raises:\n    RuntimeError: If no data files are found in data_dir.\n  \"\"\"\n  # Find files for dataset. Each file contains a sequence of arbitrary length:",
        "type": "code",
        "location": "/video_structure/datasets.py:32-56"
    },
    "15": {
        "file_id": 2,
        "content": "This code generates a TensorFlow dataset from numpy image sequences in the specified directory. It takes arguments such as data_dir, batch_size, num_timesteps, file_glob, random_offset, repeat_dataset, and seed to create a tf.data.Dataset object. If no data files are found in data_dir, it raises a RuntimeError.",
        "type": "comment"
    },
    "16": {
        "file_id": 2,
        "content": "  file_glob = file_glob if '.npz' in file_glob else file_glob + '.npz'\n  filenames = sorted(tf.gfile.Glob(os.path.join(data_dir, file_glob)))\n  if not filenames:\n    raise RuntimeError('No data files match {}.'.format(\n        os.path.join(data_dir, file_glob)))\n  # Deterministic in-place shuffle:\n  np.random.RandomState(seed).shuffle(filenames)\n  # Create dataset:\n  dtypes, pre_chunk_shapes = _read_data_types_and_shapes(filenames)\n  dataset = tf.data.Dataset.from_generator(\n      lambda: _read_numpy_sequences(filenames), dtypes, pre_chunk_shapes)\n  if repeat_dataset:\n    dataset = dataset.repeat()\n  # Divide sequences into num_timesteps chunks:\n  chunk_fn = functools.partial(\n      _chunk_sequence, chunk_length=num_timesteps, random_offset=random_offset)\n  dataset = dataset.interleave(chunk_fn, cycle_length=batch_size)\n  # Format dataset:\n  dataset = dataset.shuffle(\n      100 * batch_size, seed=seed, reshuffle_each_iteration=True)\n  dataset = dataset.batch(batch_size, drop_remainder=True)\n  dataset = dataset.prefetch(buffer_size=None)  # None = Auto-tune",
        "type": "code",
        "location": "/video_structure/datasets.py:57-83"
    },
    "17": {
        "file_id": 2,
        "content": "This code reads data files from a specified directory, shuffles them in place, creates a TensorFlow dataset, divides sequences into chunks, interleaves the chunks to form batches, shuffles and batches the data again, then prepares it for efficient loading.",
        "type": "comment"
    },
    "18": {
        "file_id": 2,
        "content": "  # Get shapes after data formatting:\n  format_shape = lambda shape: (None,) + tuple(shape.as_list()[1:])\n  shapes = {\n      key: format_shape(shape) for key, shape in dataset.output_shapes.items()}\n  return dataset, shapes\ndef _read_numpy_sequences(filenames):\n  \"\"\"Generator that reads Numpy files with sequence data from disk into a dict.\n  Unreadable files (i.e. files that cause an IOError) will be skipped.\n  For traceability, fields containing the filename and frame number will be\n  added to the sequence dict.\n  Args:\n    filenames: List of paths to Numpy files.\n  Yields:\n    Dict containing Numpy arrays corresponding to the data for one sequence.\n  \"\"\"\n  for filename in filenames:\n    try:\n      with tf.gfile.Open(filename, 'rb') as f:\n        sequence_dict = {k: v for k, v in np.load(f).items()}\n    except IOError as e:\n      print('Caught IOError: \"{}\". Skipping file {}.'.format(e, filename))\n    # Format data:\n    sequence_dict = _choose_data_fields(sequence_dict)\n    sequence_dict = {\n        k: _adjust_precision_for_tf(v) for k, v in sequence_dict.items()}",
        "type": "code",
        "location": "/video_structure/datasets.py:85-118"
    },
    "19": {
        "file_id": 2,
        "content": "This code defines a function that reads Numpy files containing sequence data from disk and returns the data in a dictionary format. It also handles unreadable files by skipping them for traceability. The data is formatted to suit TensorFlow requirements before being returned.",
        "type": "comment"
    },
    "20": {
        "file_id": 2,
        "content": "    sequence_dict['image'] = _format_image_data(sequence_dict['image'])\n    # Add filename and frame arrays for traceability:\n    num_frames = list(sequence_dict.values())[0].shape[0]\n    sequence_dict['frame_ind'] = np.arange(num_frames, dtype=np.int32)\n    sequence_dict['filename'] = np.full(num_frames, os.path.basename(filename))\n    yield sequence_dict\ndef _choose_data_fields(data_dict):\n  \"\"\"Returns a new dict containing only fields required by the model.\"\"\"\n  output_dict = {}\n  for k in REQUIRED_DATA_FIELDS:\n    if k in data_dict:\n      output_dict[k] = data_dict[k]\n    elif k == 'true_object_pos':\n      # Create dummy ground truth if it's not in the dict:\n      tf.logging.log_first_n(tf.logging.WARN,\n                             'Found no true_object_pos in data, adding dummy.',\n                             1)\n      num_timesteps = data_dict['image'].shape[0]\n      output_dict['true_object_pos'] = np.zeros([num_timesteps, 0, 2])\n    else:\n      raise ValueError(\n          'Required key \"{}\" is not in the  dict with keys {}.'.format(",
        "type": "code",
        "location": "/video_structure/datasets.py:119-144"
    },
    "21": {
        "file_id": 2,
        "content": "The code reads in a sequence of data, formats the image data, adds filename and frame index arrays for traceability. It then yields the modified sequence dictionary. The function _choose_data_fields takes a dictionary and returns a new one containing only fields required by the model. If a required key is not present, it adds dummy ground truth if the missing key is 'true_object_pos', otherwise it raises a ValueError.",
        "type": "comment"
    },
    "22": {
        "file_id": 2,
        "content": "              k, list(data_dict.keys())))\n  return output_dict\ndef _adjust_precision_for_tf(array):\n  \"\"\"Adjusts precision for TensorFlow.\"\"\"\n  if array.dtype == np.float64:\n    return array.astype(np.float32)\n  if array.dtype == np.int64:\n    return array.astype(np.int32)\n  return array\ndef _format_image_data(image):\n  \"\"\"Formats the uint8 input image to float32 in the range [-0.5, 0.5].\"\"\"\n  if not np.issubdtype(image.dtype, np.uint8):\n    raise ValueError('Expected image to be of type {}, but got type {}.'.format(\n        np.uint8, image.dtype))\n  return image.astype(np.float32) / 255.0 - 0.5\ndef _read_data_types_and_shapes(filenames):\n  \"\"\"Gets dtypes and shapes for all keys in the dataset.\"\"\"\n  sequences = _read_numpy_sequences(filenames)\n  sequence = next(sequences)\n  sequences.close()\n  dtypes = {k: tf.as_dtype(v.dtype) for k, v in sequence.items()}\n  shapes = {k: (None,) + v.shape[1:] for k, v in sequence.items()}\n  return dtypes, shapes\ndef _chunk_sequence(sequence_dict, chunk_length, random_offset=False):",
        "type": "code",
        "location": "/video_structure/datasets.py:145-176"
    },
    "23": {
        "file_id": 2,
        "content": "This code contains several utility functions for handling and adjusting data types, shapes, and formats. The main function is `_chunk_sequence`, which takes a dictionary of sequence data and chunks it into pieces of specified length. Other functions include `_adjust_precision_for_tf` to change data precision for TensorFlow compatibility, `_format_image_data` to format image data in the range [-0.5, 0.5], `_read_data_types_and_shapes` to read and return data types and shapes of all keys in a dataset, and `_chunk_sequence` to split sequence dictionary into chunks.",
        "type": "comment"
    },
    "24": {
        "file_id": 2,
        "content": "  \"\"\"Splits a dict of sequence tensors into a batch of chunks.\n  This function does not expect a batch of sequences, but a single sequence.\n  Args:\n    sequence_dict: dict of tensors with time along the first dimension.\n    chunk_length: Size of chunks the sequence will be split into.\n    random_offset: Start chunking from a random offset in the sequence,\n      enforcing that at least one chunk is generated.\n  Returns:\n    tf.data.Dataset of sequence chunks.\n  \"\"\"\n  length = tf.shape(list(sequence_dict.values())[0])[0]\n  if random_offset:\n    num_chunks = tf.maximum(1, length // chunk_length - 1)\n    output_length = num_chunks * chunk_length\n    max_offset = length - output_length\n    offset = tf.random_uniform((), 0, max_offset + 1, dtype=tf.int32)\n  else:\n    num_chunks = length // chunk_length\n    output_length = num_chunks * chunk_length\n    offset = 0\n  chunked = {}\n  for key, tensor in sequence_dict.items():\n    tensor = tensor[offset:offset + output_length]\n    chunked_shape = [num_chunks, chunk_length] + tensor.shape[1:].as_list()",
        "type": "code",
        "location": "/video_structure/datasets.py:177-205"
    },
    "25": {
        "file_id": 2,
        "content": "This function takes a dictionary of sequence tensors and splits them into chunks. It receives the chunk length, which determines the size of each chunk, and a boolean for random offset, which if set, randomly selects an offset to ensure at least one chunk is created. It returns a tf.data.Dataset containing the sequence chunks.",
        "type": "comment"
    },
    "26": {
        "file_id": 2,
        "content": "    chunked[key] = tf.reshape(tensor, chunked_shape)\n  filename = sequence_dict['filename'][0]\n  seed = tf.strings.to_hash_bucket_fast(filename, num_buckets=2**62)\n  return tf.data.Dataset.from_tensor_slices(chunked).shuffle(\n      tf.cast(length, tf.int64), seed=seed)",
        "type": "code",
        "location": "/video_structure/datasets.py:206-211"
    },
    "27": {
        "file_id": 2,
        "content": "This code chunks the tensor based on a given shape and shuffles it using a seed generated from the filename. The chunked data is then returned as a TensorFlow dataset after being shuffled.",
        "type": "comment"
    },
    "28": {
        "file_id": 3,
        "content": "/video_structure/datasets_test.py",
        "type": "filepath"
    },
    "29": {
        "file_id": 3,
        "content": "The code tests the \"video_structure.datasets\" module, creates a Keras model for image classification with Conv2D and L2 loss, trains and tests it on two datasets, and checks data order determinism. The test is flaky but has low failure probability by comparing 'frame_ind' arrays in repeats[0] and repeats[1].",
        "type": "summary"
    },
    "30": {
        "file_id": 3,
        "content": "# coding=utf-8\n# Copyright 2023 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Tests for video_structure.datasets.\"\"\"\nimport os\nfrom absl import flags\nfrom absl.testing import absltest\nimport numpy as np\nimport tensorflow.compat.v1 as tf\nfrom video_structure import datasets\nFLAGS = flags.FLAGS\nTESTDATA_DIR = 'video_structure/testdata'\nclass GetSequenceDatasetTest(tf.test.TestCase):\n  def setUp(self):\n    super(GetSequenceDatasetTest, self).setUp()\n    self.data_dir = os.path.join(FLAGS.test_srcdir, TESTDATA_DIR)",
        "type": "code",
        "location": "/video_structure/datasets_test.py:1-34"
    },
    "31": {
        "file_id": 3,
        "content": "This code is for testing the \"video_structure.datasets\" module in the google-research/video_structure repository. It imports necessary packages, defines a class GetSequenceDatasetTest with setUp method, and sets up variables to be used in the tests. The TESTDATA_DIR provides the path to test data directories.",
        "type": "comment"
    },
    "32": {
        "file_id": 3,
        "content": "    self.file_glob = 'acrobot*.npz'\n    self.batch_size = 4\n    self.num_timesteps = 2\n    self.num_channels = 3\n  def get_dataset(self, batch_size=None, random_offset=True, seed=0):\n    return datasets.get_sequence_dataset(\n        data_dir=self.data_dir,\n        file_glob=self.file_glob,\n        batch_size=batch_size or self.batch_size,\n        num_timesteps=self.num_timesteps,\n        random_offset=random_offset,\n        seed=seed)\n  def testOutputShapes(self):\n    dataset, _ = self.get_dataset()\n    expected_keys = {'image', 'true_object_pos'}\n    self.assertEqual(\n        expected_keys,\n        set(dataset.output_shapes.keys()).intersection(expected_keys))\n    self.assertEqual(\n        dataset.output_shapes['image'],\n        [self.batch_size, self.num_timesteps, 64, 64, self.num_channels])\n    self.assertEqual(dataset.output_shapes['true_object_pos'],\n                     [self.batch_size, self.num_timesteps, 0, 2])\n  def testImageRange(self):\n    dataset, _ = self.get_dataset()\n    dataset_iterator = dataset.make_one_shot_iterator()",
        "type": "code",
        "location": "/video_structure/datasets_test.py:35-66"
    },
    "33": {
        "file_id": 3,
        "content": "This code defines a class with methods for getting a dataset, testing output shapes, and testing image range. The class uses the \"get_sequence_dataset\" function to retrieve data from specified files (glob pattern 'acrobot*.npz') in the specified directory. It also checks that expected keys ('image' and 'true_object_pos') are present in the dataset and validates the shape of the 'image' key. Finally, it tests if the image values fall within a certain range.",
        "type": "comment"
    },
    "34": {
        "file_id": 3,
        "content": "    with self.session() as sess:\n      batch = sess.run(dataset_iterator.get_next())\n    max_val = np.max(batch['image'])\n    min_val = np.min(batch['image'])\n    self.assertLessEqual(max_val, 0.5)\n    self.assertGreaterEqual(min_val, -0.5)\n    self.assertGreater(max_val - min_val, 0.25,\n                       'Image range is suspiciously small.')\n  def testFeedingToModel(self):\n    \"\"\"Build a simple Keras model and test that it trains with the datasets.\"\"\"\n    dataset, _ = self.get_dataset()\n    inputs = tf.keras.Input(shape=(self.num_timesteps, 64, 64, 3), name='image')\n    conv_layer = tf.keras.layers.Conv2D(\n        3, 2, padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))\n    outputs = tf.keras.layers.TimeDistributed(conv_layer)(inputs)\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    model.add_loss(tf.nn.l2_loss(inputs - outputs))\n    model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-2, clipnorm=1.))\n    model.fit(dataset, steps_per_epoch=10, epochs=1)\n  def testOrderIsDeterministic(self):",
        "type": "code",
        "location": "/video_structure/datasets_test.py:67-92"
    },
    "35": {
        "file_id": 3,
        "content": "The code defines a test for checking if the image range is not suspiciously small, and then builds a simple Keras model that trains with the datasets. The model has a Conv2D layer and adds an L2 loss between inputs and outputs to the model's losses. It compiles the model using Adam optimizer with a learning rate of 1e-2 and clips norm at 1. Finally, it fits the model with the dataset for 1 epoch and 10 steps per epoch.",
        "type": "comment"
    },
    "36": {
        "file_id": 3,
        "content": "    \"\"\"Tests that data order is deterministic if random_offset is False.\"\"\"\n    def get_new_dataset():\n      dataset = self.get_dataset(batch_size=32, random_offset=False)[0]\n      return dataset.make_one_shot_iterator()\n    with self.session() as sess:\n      repeats = [sess.run(get_new_dataset().get_next()) for _ in range(2)]\n    # Check that order is reproducible:\n    np.testing.assert_array_equal(repeats[0]['filename'],\n                                  repeats[1]['filename'])\n    np.testing.assert_array_equal(repeats[0]['frame_ind'],\n                                  repeats[1]['frame_ind'])\n  def testRandomOffset(self):\n    \"\"\"Tests that data order is random if random_offset is True.\"\"\"\n    def get_new_dataset(seed):\n      dataset = self.get_dataset(\n          batch_size=32, random_offset=True, seed=seed)[0]\n      return dataset.make_one_shot_iterator()\n    with self.session() as sess:\n      repeats = [sess.run(get_new_dataset(seed).get_next()) for seed in [0, 1]]\n    # Check that two calls to a fresh dataset pipeline return different orders",
        "type": "code",
        "location": "/video_structure/datasets_test.py:93-119"
    },
    "37": {
        "file_id": 3,
        "content": "The code tests the determinism of data order when random_offset is False, and checks if data orders are different when random_offset is True. The function get_new_dataset() returns an iterable dataset, and repeats variable stores two runs of the dataset pipeline to compare their results. The np.testing.assert_array_equal() function is used for comparison.",
        "type": "comment"
    },
    "38": {
        "file_id": 3,
        "content": "    # (this test is technically flaky, but at a very low probability):\n    with self.assertRaises(AssertionError):\n      np.testing.assert_array_equal(repeats[0]['frame_ind'],\n                                    repeats[1]['frame_ind'])\nif __name__ == '__main__':\n  absltest.main()",
        "type": "code",
        "location": "/video_structure/datasets_test.py:120-127"
    },
    "39": {
        "file_id": 3,
        "content": "The code is testing for assertion errors between 'frame_ind' arrays in repeats[0] and repeats[1]. The test is considered flaky but with a low probability of failure.",
        "type": "comment"
    },
    "40": {
        "file_id": 4,
        "content": "/video_structure/dynamics.py",
        "type": "filepath"
    },
    "41": {
        "file_id": 4,
        "content": "This code constructs a TensorFlow VRNN model for video keypoint prediction, using RNN cells and decoders, with KLDivergence class, ScheduledSampling layer, and latent belief distribution sampling.",
        "type": "summary"
    },
    "42": {
        "file_id": 4,
        "content": "# coding=utf-8\n# Copyright 2023 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Dynamics-related components of the structured video representation model.\nThese components model the dynamics of the keypoints extracted by the vision\ncomponents.\n\"\"\"\nimport tensorflow.compat.v1 as tf\nlayers = tf.keras.layers\ndef build_vrnn(cfg):\n  \"\"\"Builds the VRNN dynamics model.\n  The model takes observed keypoints from the keypoint detector as input and\n  returns keypoints decoded from the model's latent belief. The model only uses",
        "type": "code",
        "location": "/video_structure/dynamics.py:1-32"
    },
    "43": {
        "file_id": 4,
        "content": "This code snippet is for building the VRNN (Video Recurrent Neural Network) dynamics model. It takes observed keypoints from a keypoint detector as input and returns the keypoints decoded from the model's latent belief. The model only uses input, hidden states, and recurrent layers. It is written in TensorFlow and contains a Keras layers module for building the network structure.",
        "type": "comment"
    },
    "44": {
        "file_id": 4,
        "content": "  the observed keypoints for the first cfg.observed_steps steps. For remaining\n  cfg.predicted_steps steps, it predicts keypoints using only the dynamics\n  model.\n  Args:\n    cfg: Hyperparameter ConfigDict.\n  Returns:\n    A tf.keras.Model and the KL loss tensor.\n  \"\"\"\n  # Build model components. All of the weights are shared across observed and\n  # predicted timesteps:\n  rnn_cell = layers.GRUCell(cfg.num_rnn_units)\n  prior_net = build_prior_net(cfg)\n  posterior_net = build_posterior_net(cfg)\n  decoder = build_decoder(cfg)\n  scheduled_sampler_obs = ScheduledSampling(\n      p_true_start=cfg.scheduled_sampling_p_true_start_obs,\n      p_true_end=cfg.scheduled_sampling_p_true_end_obs,\n      ramp_steps=cfg.scheduled_sampling_ramp_steps)\n  scheduled_sampler_pred = ScheduledSampling(\n      p_true_start=cfg.scheduled_sampling_p_true_start_pred,\n      p_true_end=cfg.scheduled_sampling_p_true_end_pred,\n      ramp_steps=cfg.scheduled_sampling_ramp_steps)\n  # Format inputs:\n  num_timesteps = cfg.observed_steps + cfg.predicted_steps",
        "type": "code",
        "location": "/video_structure/dynamics.py:33-60"
    },
    "45": {
        "file_id": 4,
        "content": "This code builds a model for predicting keypoints using a dynamics model. It includes an RNN cell, prior and posterior networks, and decoders. The observed steps are taken from the input, while predicted steps are generated by the model. Scheduled sampling is used to gradually transition between true and sampled values in both observed and predicted steps. The code also defines the number of timesteps based on observed and predicted steps.",
        "type": "comment"
    },
    "46": {
        "file_id": 4,
        "content": "  input_keypoints_stack = tf.keras.Input(\n      (num_timesteps, cfg.num_keypoints, 3), name='vrnn_input')\n  input_keypoints_list = layers.Lambda(lambda x: tf.unstack(x, axis=1))(\n      input_keypoints_stack)\n  # Initialize loop variables:\n  rnn_state = rnn_cell.get_initial_state(\n      batch_size=tf.shape(input_keypoints_stack)[0], dtype=tf.float32)\n  output_keypoints_list = [None] * num_timesteps\n  kl_div_list = [None] * cfg.observed_steps\n  # Process observed steps:\n  for t in range(cfg.observed_steps):\n    output_keypoints_list[t], rnn_state, kl_div_list[t] = _vrnn_iteration(\n        cfg, input_keypoints_list[t], rnn_state, rnn_cell, prior_net, decoder,\n        scheduled_sampler_obs, posterior_net)\n  # Process predicted steps:\n  for t in range(cfg.observed_steps, num_timesteps):\n    output_keypoints_list[t], rnn_state, _ = _vrnn_iteration(\n        cfg, input_keypoints_list[t], rnn_state, rnn_cell, prior_net, decoder,\n        scheduled_sampler_pred)\n  output_keypoints_stack = layers.Lambda(lambda x: tf.stack(x, axis=1))(",
        "type": "code",
        "location": "/video_structure/dynamics.py:61-85"
    },
    "47": {
        "file_id": 4,
        "content": "This code implements a Variational Recurrent Neural Network (VRNN) for predicting keypoints of a video over time. It processes observed steps and then predicted steps to generate output keypoints and a reconstruction of input keypoints. The RNN cell is initialized, and the code iterates through each step to generate the keypoints using _vrnn_iteration function.",
        "type": "comment"
    },
    "48": {
        "file_id": 4,
        "content": "      output_keypoints_list)\n  kl_div_stack = layers.Lambda(lambda x: tf.stack(x, axis=1))(kl_div_list)\n  return tf.keras.Model(\n      inputs=input_keypoints_stack,\n      outputs=[output_keypoints_stack, kl_div_stack],\n      name='vrnn')\ndef _vrnn_iteration(cfg,\n                    input_keypoints,\n                    rnn_state,\n                    rnn_cell,\n                    prior_net,\n                    decoder,\n                    scheduled_sampler,\n                    posterior_net=None):\n  \"\"\"Performs one timestep of the VRNN.\n  Args:\n    cfg: ConfigDict with model hyperparameters.\n    input_keypoints: [batch_size, num_keypoints, 3] tensor (one timestep of\n      the sequence returned by the keypoint detector).\n    rnn_state: Previous recurrent state.\n    rnn_cell: A Keras RNN cell object (e.g. tf.layers.GRUCell) that holds the\n      dynamics model.\n    prior_net: A tf.keras.Model that computes the prior latent belief from the\n      previous RNN state.\n    decoder: A tf.keras.Model that decodes the latent belief into keypoints.",
        "type": "code",
        "location": "/video_structure/dynamics.py:86-114"
    },
    "49": {
        "file_id": 4,
        "content": "This function performs one timestep of the Variational Recurrent Neural Network (VRNN), taking input keypoints, previous recurrent state, RNN cell, prior net, decoder, and a scheduled sampler as arguments. It returns updated output keypoints stack and Kullback-Leibler divergence stack.",
        "type": "comment"
    },
    "50": {
        "file_id": 4,
        "content": "    scheduled_sampler: Keras layer instance that performs scheduled sampling.\n    posterior_net: (Optional) A tf.keras.Model that computes the posterior\n      latent belief, given observed keypoints and the previous RNN state. If no\n      posterior_net is supplied, prior latent belief is used for predictions.\n  Returns:\n    Three tensors: The output keypoints, the new RNN state, and the KL\n    divergence between the prior and posterior (None if no posterior_net is\n    provided).\n  \"\"\"\n  shape = input_keypoints.shape.as_list()[1:]\n  observed_keypoints_flat = layers.Reshape([shape[0] * shape[1]])(\n      input_keypoints)\n  # Obtain parameters mean, std for the latent belief distibution:\n  mean_prior, std_prior = prior_net(rnn_state)\n  if posterior_net:\n    mean, std = posterior_net([rnn_state, observed_keypoints_flat])\n    kl_divergence = KLDivergence(cfg.kl_annealing_steps)(\n        [mean_prior, std_prior, mean, std])\n  else:\n    # Having no posterior_net means that this cell is used to make predictions\n    # based on the prior only, without having access to observations. In this",
        "type": "code",
        "location": "/video_structure/dynamics.py:115-138"
    },
    "51": {
        "file_id": 4,
        "content": "This code defines a function that takes input keypoints and RNN state, and based on the presence of a posterior_net, calculates the mean and standard deviation for latent belief distribution either from prior_net alone or combining with observed keypoints. It also computes the KL divergence between the prior and posterior (if provided). This code is used in making predictions based on prior only when no posterior_net is available, allowing prediction without observation access.",
        "type": "comment"
    },
    "52": {
        "file_id": 4,
        "content": "    # case, no posterior is available to compute the KL term of the\n    # variational objective. We therefore cannot train the prior net during\n    # predicted steps. Since a reconstruction error is still generated, we\n    # need to stop the gradients explicitly to ensure the prior net is not\n    # updated based on these errors:\n    mean = layers.Lambda(tf.stop_gradient)(mean_prior)\n    std = layers.Lambda(tf.stop_gradient)(std_prior)\n    kl_divergence = None\n  # Sample a belief from the distribution and decode it into keypoints:\n  sampler = SampleBestBelief(\n      cfg.num_samples_for_bom,\n      decoder,\n      use_mean_instead_of_sample=cfg.use_deterministic_belief)\n  latent_belief, output_keypoints_flat = sampler(\n      [mean, std, rnn_state, observed_keypoints_flat])\n  output_keypoints = layers.Reshape(shape)(output_keypoints_flat)\n  # TODO(mjlm): Think through where we need stop_gradients.\n  # Step the RNN forward:\n  keypoints_for_rnn = scheduled_sampler([\n      observed_keypoints_flat, output_keypoints_flat])",
        "type": "code",
        "location": "/video_structure/dynamics.py:139-161"
    },
    "53": {
        "file_id": 4,
        "content": "This code is from the \"video_structure/dynamics.py\" file and it seems to be related to a video processing model. It uses layers and samplers to compute mean and standard deviation, sample beliefs, decode them into keypoints, and step the RNN forward. The tf.stop_gradient function is used to stop gradients in certain parts of the code likely for performance or other optimization reasons. It also mentions a future TODO where the need for stop_gradients should be reviewed.",
        "type": "comment"
    },
    "54": {
        "file_id": 4,
        "content": "  rnn_input = layers.Concatenate(axis=-1)([keypoints_for_rnn, latent_belief])\n  _, rnn_state = rnn_cell(rnn_input, [rnn_state])\n  rnn_state = rnn_state[0]  # rnn_cell needs state to be wrapped in list.\n  return output_keypoints, rnn_state, kl_divergence\ndef build_prior_net(cfg):\n  \"\"\"Computes the prior belief over current keypoints, given past information.\n  rnn_state[t-1] --> prior_mean[t], prior_std[t]\n  Args:\n    cfg: Hyperparameter ConfigDict.\n  Returns:\n    Keras Model object.\n  \"\"\"\n  rnn_state = tf.keras.Input(shape=[cfg.num_rnn_units], name='rnn_state')\n  hidden = layers.Dense(cfg.prior_net_dim, **cfg.dense_layer_kwargs)(rnn_state)\n  means = layers.Dense(cfg.latent_code_size, name='means')(hidden)\n  stds_raw = layers.Dense(cfg.latent_code_size)(hidden)\n  stds = layers.Lambda(\n      lambda x: tf.nn.softplus(x) + 1e-4, name='stds')(stds_raw)\n  return tf.keras.Model(inputs=rnn_state, outputs=[means, stds], name='prior')\ndef build_decoder(cfg):\n  \"\"\"Decodes keypoints from the latent belief.\n  rnn_state[t-1], latent_code[t] --> keypoints[t]",
        "type": "code",
        "location": "/video_structure/dynamics.py:163-193"
    },
    "55": {
        "file_id": 4,
        "content": "The code defines a function to build a prior network and decoder. The prior network takes the RNN state as input, outputs means and standard deviations for latent codes. The decoder takes the RNN state and latent code as inputs and outputs keypoints. Both functions return Keras Model objects.",
        "type": "comment"
    },
    "56": {
        "file_id": 4,
        "content": "  Args:\n    cfg: Hyperparameter ConfigDict.\n  Returns:\n    Keras Model object.\n  \"\"\"\n  rnn_state = tf.keras.Input(shape=[cfg.num_rnn_units], name='rnn_state')\n  latent_code = tf.keras.Input(shape=[cfg.latent_code_size], name='latent_code')\n  hidden = layers.Concatenate()([rnn_state, latent_code])\n  hidden = layers.Dense(128, **cfg.dense_layer_kwargs)(hidden)\n  keypoints = layers.Dense(cfg.num_keypoints * 3, activation=tf.nn.tanh)(\n      hidden)\n  return tf.keras.Model(\n      inputs=[rnn_state, latent_code], outputs=keypoints, name='decoder')\ndef build_posterior_net(cfg):\n  \"\"\"Incorporates observed information into the latent belief.\n  rnn_state[t-1], observed_keypoints[t] --> posterior_mean[t], posterior_std[t]\n  Args:\n    cfg: Hyperparameter ConfigDict.\n  Returns:\n    Keras Model object.\n  \"\"\"\n  rnn_state = tf.keras.Input(shape=[cfg.num_rnn_units], name='rnn_state')\n  keypoints = tf.keras.Input(shape=[cfg.num_keypoints * 3], name='keypoints')\n  hidden = layers.Concatenate()([rnn_state, keypoints])\n  hidden = layers.Dense(cfg.posterior_net_dim, **cfg.dense_layer_kwargs)(hidden)",
        "type": "code",
        "location": "/video_structure/dynamics.py:195-225"
    },
    "57": {
        "file_id": 4,
        "content": "This code is defining two Keras models: a decoder and a posterior network. The decoder takes rnn_state and latent_code as inputs, concatenates them, passes through dense layers, and outputs keypoints. The posterior network takes rnn_state and keypoints as inputs, concatenates them, passes through dense layers, and outputs posterior mean and standard deviation. Both models are built using the Keras API with specified input shapes and activation functions.",
        "type": "comment"
    },
    "58": {
        "file_id": 4,
        "content": "  means = layers.Dense(cfg.latent_code_size, name='means')(hidden)\n  stds_raw = layers.Dense(cfg.latent_code_size)(hidden)\n  stds = layers.Lambda(\n      lambda x: tf.nn.softplus(x) + 1e-4, name='stds')(stds_raw)\n  return tf.keras.Model(\n      inputs=[rnn_state, keypoints], outputs=[means, stds], name='posterior')\nclass TrainingStepCounter(layers.Layer):\n  \"\"\"Provides a class attribute that contains the training step count.\"\"\"\n  def __init__(self, **kwargs):\n    self.uses_learning_phase = True\n    super().__init__(**kwargs)\n  def build(self, input_shape):\n    self.train_step = self.add_weight(\n        name='train_step', shape=[], initializer='zeros', trainable=False)\n    increment = tf.cast(tf.keras.backend.learning_phase(), tf.float32)\n    increment_op = tf.assign_add(self.train_step, increment)\n    self.add_update(increment_op)\n    super().build(input_shape)\n  def reset_states(self):\n    self.train_step.set_value(0)\nclass KLDivergence(TrainingStepCounter):\n  \"\"\"Returns the KL divergence between the prior and posterior distributions.",
        "type": "code",
        "location": "/video_structure/dynamics.py:226-254"
    },
    "59": {
        "file_id": 4,
        "content": "This code defines a class that estimates the KL divergence between two distributions. It consists of a model with hidden layers, and a training step counter to track the number of steps during training. The KLDivergence class inherits from TrainingStepCounter and provides a method for calculating the KL divergence.",
        "type": "comment"
    },
    "60": {
        "file_id": 4,
        "content": "  Attributes:\n    kl_annealing_steps: The returned KL divergence value will be linearly\n      annealed from 0 to the final value over this many training steps.\n  \"\"\"\n  def __init__(self, kl_annealing_steps=0, **kwargs):\n    self.kl_annealing_steps = kl_annealing_steps\n    super().__init__(**kwargs)\n  def call(self, inputs):\n    mean_prior, std_prior, mean, std = inputs\n    prior = tf.distributions.Normal(mean_prior, std_prior)\n    posterior = tf.distributions.Normal(mean, std)\n    kl_div = tf.distributions.kl_divergence(posterior, prior)\n    kl_div = tf.reduce_sum(kl_div, axis=-1)  # Sum over distribution dimensions.\n    if self.kl_annealing_steps:\n      kl_div *= tf.minimum(self.train_step / self.kl_annealing_steps, 1.0)\n    return kl_div\nclass ScheduledSampling(TrainingStepCounter):\n  \"\"\"Keras layer that implements scheduled sampling for teacher forcing.\n  See https://arxiv.org/abs/1506.03099.\n  For training an RNN, teacher forcing (i.e. providing the ground-truth inputs,\n  rather than the previous RNN outputs) can stabilize training. However, this",
        "type": "code",
        "location": "/video_structure/dynamics.py:256-282"
    },
    "61": {
        "file_id": 4,
        "content": "This code defines a ScheduledSampling layer that implements scheduled sampling for teacher forcing in RNN training. It takes the kl_annealing_steps parameter to anneal the KL divergence from 0 to final value over this many steps and returns the Kl_div value after summing it over distribution dimensions.",
        "type": "comment"
    },
    "62": {
        "file_id": 4,
        "content": "  means that the RNN input distribution will be different during training and\n  inference. Scheduled sampling randomly mixes ground truth and RNN predictions\n  during training, slowly ramping down the ground-truth probablility as training\n  progresses. Thereby, training is stabilized initially and then becomes\n  gradually more realistic.\n  This layer implements a linear schedule.\n  To disable scheduled sampling, set p_true_start and p_true_end to 0.\n  Scheduled sampling is only applied during the learning phase (i.e. when\n  tf.keras.backend.learning_phase() is True). During testing, the layer always\n  returns the \"pred\" (i.e. second) input tensor.\n  Attributes:\n    p_true_start: Initial probability of sampling the \"true\" input.\n    p_true_end: Final probability of sampling the \"true\" input.\n    ramp_steps: Number of training steps over which the output will ramp from\n        p_true_start to p_true_end.\n  Returns:\n    Tensor containing either the true or the predicted input.\n  \"\"\"\n  def __init__(\n      self, p_true_start=1.0, p_true_end=0.2, ramp_steps=10000, **kwargs):",
        "type": "code",
        "location": "/video_structure/dynamics.py:283-308"
    },
    "63": {
        "file_id": 4,
        "content": "This code defines a layer implementing linear scheduled sampling for RNN input distributions during training. It randomly mixes ground truth and RNN predictions, gradually reducing the ground-truth probability as training progresses. Disable by setting p_true_start and p_true_end to 0. Applied only during learning phase and returns predicted input during testing.",
        "type": "comment"
    },
    "64": {
        "file_id": 4,
        "content": "    self.ramp_steps = ramp_steps\n    self.p_true_start = p_true_start\n    self.p_true_end = p_true_end\n    super().__init__(**kwargs)\n  def call(self, inputs):\n    \"\"\"Inputs should be [true, pred], each with size [batch, ...].\"\"\"\n    true, pred = inputs\n    # Compute current probability of choosing the ground truth:\n    ramp = self.train_step / self.ramp_steps\n    ramp = tf.minimum(ramp, 1.0)\n    p_true = self.p_true_start - (self.p_true_start - self.p_true_end) * ramp\n    # Flip a coin based on p_true:\n    return_true = tf.less(tf.random.uniform([]), p_true)\n    # During testing, use `pred` tensor (i.e. no teacher forcing):\n    return_true = tf.keras.backend.in_train_phase(return_true, False)\n    return tf.keras.backend.switch(return_true, true, pred)\nclass SampleBestBelief(layers.Layer):\n  \"\"\"Chooses the best keypoints from a number of latent belief samples.\n  This layer implements the \"best of many\" sample objective proposed in\n  https://arxiv.org/abs/1806.07772.\n  \"Best\" is defined to mean closest in Euclidean distance to the keypoints",
        "type": "code",
        "location": "/video_structure/dynamics.py:309-339"
    },
    "65": {
        "file_id": 4,
        "content": "This function implements a layer that chooses the best keypoints from multiple latent belief samples. It computes the probability of selecting ground truth based on training step, and then randomly selects either true or pred values during training and testing phases.",
        "type": "comment"
    },
    "66": {
        "file_id": 4,
        "content": "  observed by the vision model.\n  Attributes:\n    num_samples: Number of samples to choose the best from.\n    coordinate_decoder: tf.keras.Model object that decodes the latent belief\n      into keypoints.\n    use_mean_instead_of_sample: If true, do not sample, but just use the mean of\n      the latent belief distribution.\n  \"\"\"\n  def __init__(self,\n               num_samples,\n               coordinate_decoder,\n               use_mean_instead_of_sample=False,\n               **kwargs):\n    self.num_samples = num_samples\n    self.coordinate_decoder = coordinate_decoder\n    self.use_mean_instead_of_sample = use_mean_instead_of_sample\n    self.uses_learning_phase = True\n    super().__init__(**kwargs)\n  def call(self, inputs):\n    latent_mean, latent_std, rnn_state, observed_keypoints_flat = inputs\n    # Draw latent samples:\n    if self.use_mean_instead_of_sample:\n      sampled_latent = tf.stack([latent_mean] * self.num_samples)\n    else:\n      distribution = tf.distributions.Normal(loc=latent_mean, scale=latent_std)",
        "type": "code",
        "location": "/video_structure/dynamics.py:340-368"
    },
    "67": {
        "file_id": 4,
        "content": "This code defines a class that takes input latent belief distribution and draws samples from it. The number of samples to draw is specified by the `num_samples` parameter, and the decoding of latent belief into keypoints is done through the `coordinate_decoder`. If `use_mean_instead_of_sample` is True, it uses the mean of the latent belief distribution instead of drawing samples. The class also inherits from another class and uses a learning phase.",
        "type": "comment"
    },
    "68": {
        "file_id": 4,
        "content": "      sampled_latent = distribution.sample(sample_shape=(self.num_samples,))\n    sampled_latent_list = tf.unstack(sampled_latent)\n    # Decode samples into coordinates:\n    # sampled_keypoints has shape [num_samples, batch_size, 3 * num_keypoints].\n    sampled_keypoints = tf.stack([\n        self.coordinate_decoder([rnn_state, latent])\n        for latent in sampled_latent_list\n    ])\n    # If we have only 1 sample, we can just return that:\n    if self.num_samples == 1:\n      return [sampled_latent_list[0], sampled_keypoints[0]]\n    # Compute L2 prediction loss for all samples (note that this includes both\n    # the x,y-coordinates and the keypoint scale):\n    sample_losses = tf.reduce_mean(\n        (sampled_keypoints - observed_keypoints_flat[tf.newaxis, Ellipsis])**2.0,\n        axis=-1)  # Mean across keypoints.\n    # Choose the sample based on the loss:\n    return _choose_sample(sampled_latent, sampled_keypoints, sample_losses)\n  def compute_output_shape(self, input_shape):\n    return [input_shape[-1], input_shape[0]]",
        "type": "code",
        "location": "/video_structure/dynamics.py:369-393"
    },
    "69": {
        "file_id": 4,
        "content": "This code samples a distribution, decodes the samples into keypoints coordinates using a coordinate decoder, computes the L2 prediction loss for each sample and selects the best one based on the loss. The output shape is [input_shape[-1], input_shape[0]].",
        "type": "comment"
    },
    "70": {
        "file_id": 4,
        "content": "def _choose_sample(sampled_latent, sampled_keypoints, sample_losses):\n  \"\"\"Returns the first or lowest-loss sample, depending on learning phase.\n  During training, the sample with the lowest loss is returned.\n  During inference, the first sample is returned without regard to the loss.\n  Args:\n    sampled_latent: [num_samples, batch_size, latent_code_size] tensor.\n    sampled_keypoints: [num_samples, batch_size, 3 * num_keypoints] tensor.\n    sample_losses: [num_samples, batch_size] tensor.\n  Returns:\n    Two tensors: latent and keypoint representation of the best sample.\n  \"\"\"\n  # Find the indices of the samples with the lowest loss:\n  best_sample_ind = tf.argmin(sample_losses, axis=0)  # Shape is [batch_size].\n  best_sample_ind = tf.cast(best_sample_ind, tf.int32)\n  batch_ind = tf.range(tf.shape(sampled_latent)[1], dtype=tf.int32)\n  indices = tf.stack([best_sample_ind, batch_ind], axis=-1)\n  # Only keep the best keypoints and latent sample:\n  best_latent = tf.gather_nd(sampled_latent, indices)\n  best_keypoints = tf.gather_nd(sampled_keypoints, indices)",
        "type": "code",
        "location": "/video_structure/dynamics.py:396-419"
    },
    "71": {
        "file_id": 4,
        "content": "This function selects the best sample based on its loss. During training, it chooses the sample with the lowest loss, while during inference, it selects the first sample regardless of the loss. It takes latent and keypoint samples along with their corresponding losses as input. The function then finds the indices of the samples with the lowest loss, converts them to int32, and retrieves the best latent and keypoint samples using these indices.",
        "type": "comment"
    },
    "72": {
        "file_id": 4,
        "content": "  # During training, return the best sample. During inference, return the\n  # first sample:\n  return [\n      tf.keras.backend.in_train_phase(best_latent, sampled_latent[0]),\n      tf.keras.backend.in_train_phase(best_keypoints, sampled_keypoints[0]),\n  ]",
        "type": "code",
        "location": "/video_structure/dynamics.py:421-426"
    },
    "73": {
        "file_id": 4,
        "content": "This code snippet checks if the program is in training mode or not, then selects the best sample during training and the first sample during inference. It uses the tf.keras.backend.in_train_phase function to make this decision.",
        "type": "comment"
    },
    "74": {
        "file_id": 5,
        "content": "/video_structure/dynamics_test.py",
        "type": "filepath"
    },
    "75": {
        "file_id": 5,
        "content": "The code tests VRNN training loop, KLDivergence layer annealing, and verifies dynamics model's sampling schedule consistency. It checks the correctness of samples chosen during training and inference using TensorFlow operations, ensuring shape and expected best samples.",
        "type": "summary"
    },
    "76": {
        "file_id": 5,
        "content": "# coding=utf-8\n# Copyright 2023 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Tests for video_structure.vision.\"\"\"\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow.compat.v1 as tf\nfrom video_structure import dynamics\nfrom video_structure import hyperparameters\nclass DynamicsTest(tf.test.TestCase):\n  def setUp(self):\n    # Hyperparameter config for test models:\n    self.cfg = hyperparameters.get_config()\n    self.cfg.batch_size = 4",
        "type": "code",
        "location": "/video_structure/dynamics_test.py:1-32"
    },
    "77": {
        "file_id": 5,
        "content": "The code is a Python test file for testing video structure dynamics. It includes necessary imports, sets up hyperparameters, and defines a DynamicsTest class with a setUp method for configuring the tests.",
        "type": "comment"
    },
    "78": {
        "file_id": 5,
        "content": "    self.cfg.observed_steps = 2\n    self.cfg.predicted_steps = 2\n    self.cfg.num_keypoints = 3\n    self.cfg.num_rnn_units = 4\n    super().setUp()\n  def testTrainingLossIsNotNan(self):\n    \"\"\"Tests a minimal Keras training loop for the dynamics model.\"\"\"\n    observed_keypoints = np.random.RandomState(0).normal(size=(\n        self.cfg.batch_size, self.cfg.observed_steps + self.cfg.predicted_steps,\n        self.cfg.num_keypoints, 3))\n    model = dynamics.build_vrnn(self.cfg)\n    model.add_loss(tf.nn.l2_loss(model.inputs[0] - model.outputs[0]))  # KP loss\n    model.add_loss(tf.reduce_mean(model.outputs[1]))  # KL loss\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5))\n    history = model.fit(x=observed_keypoints, steps_per_epoch=1, epochs=1)\n    self.assertFalse(\n        np.any(np.isnan(history.history['loss'])),\n        'Loss contains nans: {}'.format(history.history['loss']))\n  def testDecoderShapes(self):\n    rnn_state = tf.zeros((self.cfg.batch_size, self.cfg.num_rnn_units))\n    latent_code = tf.zeros((self.cfg.batch_size, self.cfg.latent_code_size))",
        "type": "code",
        "location": "/video_structure/dynamics_test.py:33-56"
    },
    "79": {
        "file_id": 5,
        "content": "The code is testing a minimal Keras training loop for the dynamics model. It sets up configuration parameters, builds a Variational Recurrent Neural Network (VRNN) model, adds loss functions, compiles the model, trains it with provided observed keypoints data, and verifies that the loss does not contain any NaN values. Additionally, it tests the decoder shapes using RNN state and latent code size.",
        "type": "comment"
    },
    "80": {
        "file_id": 5,
        "content": "    keypoints = dynamics.build_decoder(self.cfg)([rnn_state, latent_code])\n    self.assertEqual(\n        keypoints.shape.as_list(),\n        [self.cfg.batch_size, self.cfg.num_keypoints * 3])\n  def testPosteriorNetShapes(self):\n    rnn_state = tf.zeros((self.cfg.batch_size, self.cfg.num_rnn_units))\n    keypoints = tf.zeros((self.cfg.batch_size, self.cfg.num_keypoints * 3))\n    means, stds = dynamics.build_posterior_net(self.cfg)([rnn_state, keypoints])\n    self.assertEqual(\n        means.shape.as_list(), [self.cfg.batch_size, self.cfg.latent_code_size])\n    self.assertEqual(\n        stds.shape.as_list(), [self.cfg.batch_size, self.cfg.latent_code_size])\nclass KLDivergenceTest(tf.test.TestCase):\n  def testKLDivergenceIsZero(self):\n    \"\"\"Tests that KL divergence of identical distributions is zero.\"\"\"\n    with self.session() as sess:\n      mean = tf.random.normal((3, 3, 3))\n      std = tf.random.normal((3, 3, 3))\n      kl_divergence = dynamics.KLDivergence()([mean, std, mean, std])\n      result = sess.run([kl_divergence])[0]",
        "type": "code",
        "location": "/video_structure/dynamics_test.py:57-80"
    },
    "81": {
        "file_id": 5,
        "content": "This code is testing the shapes of tensors outputted by the build_decoder and build_posterior_net functions, as well as checking that the KL divergence between identical distributions is zero. The build_decoder function takes in an RNN state and a latent code to produce keypoints. The build_posterior_net function also takes in these inputs and produces means and standard deviations for a latent code. These shapes are then checked against expected values defined by the cfg file. Finally, the code tests that the KL divergence between identical distributions is zero to ensure consistency.",
        "type": "comment"
    },
    "82": {
        "file_id": 5,
        "content": "    np.testing.assert_array_equal(result, result * 0.0)\n  def testNonzeroKLDivergence(self):\n    \"\"\"Test that KL divergence layer provides correct result.\"\"\"\n    mu = 2.0\n    sigma = 2.0\n    n = 3\n    # https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Multivariate_normal_distributions\n    result_np = 0.5 * (sigma ** 2 + mu ** 2 - np.log(sigma ** 2) - 1) * n\n    with self.session() as sess:\n      kl_divergence = dynamics.KLDivergence()(\n          [tf.zeros(n), tf.ones(n), tf.zeros(n) + mu, tf.ones(n) * sigma])\n      result_tf = sess.run(kl_divergence)\n    np.testing.assert_almost_equal(result_tf, result_np, decimal=4)\n  def testKLDivergenceAnnealing(self):\n    inputs = tf.keras.Input(1)\n    outputs = dynamics.KLDivergence(kl_annealing_steps=4)([\n        inputs, inputs,\n        tf.keras.layers.Lambda(lambda x: x + 1.0)(inputs), inputs\n    ])\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    model.compile('sgd', 'mse')\n    obtained_kl = [model.predict(x=[1], steps=1)]  # Should be zero before fit.",
        "type": "code",
        "location": "/video_structure/dynamics_test.py:81-107"
    },
    "83": {
        "file_id": 5,
        "content": "Test if KL divergence layer provides correct results. Compare numpy and TensorFlow results with a tolerance of 0.01.\nTest KLDivergence layer annealing functionality. Compile the model, predicting from input layers for different steps.",
        "type": "comment"
    },
    "84": {
        "file_id": 5,
        "content": "    for _ in range(5):\n      model.fit(x=[1], y=[1], epochs=1, steps_per_epoch=1)\n      obtained_kl.append(model.predict(x=[1], steps=1))\n    obtained_kl = np.array(obtained_kl).ravel()\n    np.testing.assert_array_almost_equal(\n        obtained_kl, [0, 0.125, 0.25, 0.375, 0.5, 0.5])\nclass TrainingStepCounterTest(tf.test.TestCase):\n  def setUp(self):\n    # Set up simple model in which the ground-truth data is a tensor of ones and\n    # the predicted data is a tensor of zeros.\n    super().setUp()  # Sets up the TensorFlow environment, so call it early.\n    self.sess = tf.keras.backend.get_session()\n    inputs = tf.keras.Input(1)\n    outputs = dynamics.TrainingStepCounter()(inputs)\n    self.model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    self.model.compile('sgd', 'mse')\n  def testTrainingPhaseStepsAreCounted(self):\n    num_epochs = 2\n    steps_per_epoch = 5\n    self.model.fit(\n        x=np.zeros(1),\n        y=np.zeros(1),\n        epochs=num_epochs,\n        steps_per_epoch=steps_per_epoch)\n    step_count = self.sess.run(self.model.layers[-1].weights[0])",
        "type": "code",
        "location": "/video_structure/dynamics_test.py:108-136"
    },
    "85": {
        "file_id": 5,
        "content": "The code sets up a simple model where the ground-truth data is a tensor of ones and the predicted data is a tensor of zeros. It then tests if the TrainingStepCounter correctly counts the training phase steps. The model is fitted with the given data and the step count is obtained through a session run. Finally, it asserts that the obtained step count is equal to [0, 0.125, 0.25, 0.375, 0.5, 0.5].",
        "type": "comment"
    },
    "86": {
        "file_id": 5,
        "content": "    self.assertEqual(step_count, num_epochs * steps_per_epoch)\n  def testTestingPhaseStepsAreNotCounted(self):\n    self.model.predict(x=np.zeros(1), steps=10)\n    step_count = self.sess.run(self.model.layers[-1].weights[0])\n    self.assertEqual(step_count, 0.0)\n  def testMultipleCallsAreCountedOnce(self):\n    \"\"\"Calling the same layer twice should not increase the counter twice.\"\"\"\n    # Create a model that calls the same TrainingStepCounter layer twice:\n    counter = dynamics.TrainingStepCounter()\n    inputs = tf.keras.Input(1)\n    output1 = counter(inputs)\n    output2 = counter(inputs)\n    model = tf.keras.Model(inputs=inputs, outputs=[output1, output2])\n    model.compile('sgd', 'mse')\n    # Train:\n    num_epochs = 2\n    steps_per_epoch = 5\n    model.fit(\n        x=np.zeros(1),\n        y=[np.zeros(1), np.zeros(1)],\n        epochs=num_epochs,\n        steps_per_epoch=steps_per_epoch)\n    step_count = self.sess.run(model.layers[-1].weights[0])\n    self.assertEqual(step_count, num_epochs * steps_per_epoch)\nclass ScheduledSamplingTest(tf.test.TestCase):",
        "type": "code",
        "location": "/video_structure/dynamics_test.py:137-168"
    },
    "87": {
        "file_id": 5,
        "content": "This code snippet is testing the functionality of a TrainingStepCounter class. The first test checks if the step count is equal to zero when no prediction is made, the second test checks that repeated calls don't increase the counter twice and the third test verifies that the step count is equal to the number of epochs multiplied by steps per epoch during training.",
        "type": "comment"
    },
    "88": {
        "file_id": 5,
        "content": "  def setUp(self):\n    # Set up simple model in which the ground-truth data is a tensor of ones and\n    # the predicted data is a tensor of zeros.\n    self.ramp_steps = 5000\n    self.p_true_start = 1.0\n    self.p_true_end = 0.0\n    super().setUp()  # Sets up the TensorFlow environment, so call it early.\n    self.sess = tf.keras.backend.get_session()\n    inputs = tf.keras.Input(1)\n    true = tf.keras.layers.Lambda(lambda x: x + 1.0)(inputs)\n    pred = tf.keras.layers.Lambda(lambda x: x + 0.0)(inputs)\n    outputs = dynamics.ScheduledSampling(\n        p_true_start=self.p_true_start,\n        p_true_end=self.p_true_end,\n        ramp_steps=self.ramp_steps,\n    )([true, pred])\n    self.model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    self.model.compile('sgd', 'mse')\n  def testSamplingSchedule(self):\n    num_epochs = 5\n    steps_per_epoch = self.ramp_steps / num_epochs\n    assert steps_per_epoch >= 1000, ('steps_per_epoch should be large enough to'\n                                     'average out the sampling randomness.')",
        "type": "code",
        "location": "/video_structure/dynamics_test.py:170-193"
    },
    "89": {
        "file_id": 5,
        "content": "This code sets up a test environment for evaluating the sampling schedule of a dynamics model. It creates a simple model where ground-truth data is a tensor of ones and predicted data is zeros. The test runs the model for a specified number of epochs, ensuring that steps_per_epoch is large enough to average out sampling randomness.",
        "type": "comment"
    },
    "90": {
        "file_id": 5,
        "content": "    history = self.model.fit(\n        x=np.zeros(1),\n        y=np.zeros(1),\n        epochs=num_epochs,\n        steps_per_epoch=steps_per_epoch)\n    expected_schedule = np.linspace(\n        self.p_true_start, self.p_true_end, 2 * num_epochs + 1)[1::2]\n    # Note that the model is set up such that the \"loss\" variable contains the\n    # average fraction of \"true\" samples obtained in each epoch:\n    np.testing.assert_array_almost_equal(\n        history.history['loss'],\n        expected_schedule,\n        decimal=1,\n        err_msg='Observed schedule deviates from expected linear schedule.')\nclass SampleBestBeliefTest(tf.test.TestCase, parameterized.TestCase):\n  def setUp(self):\n    super().setUp()\n    self.num_samples = 5\n    self.batch_size = 4\n    self.latent_code_size = 3\n    self.num_keypoints = 2\n    # Create sampled_latent of shape [num_samples, batch_size, latent_code_size]\n    # such that samples are numbered from 1 to num_samples:\n    sampled_latent = np.arange(self.num_samples)[:, np.newaxis, np.newaxis]",
        "type": "code",
        "location": "/video_structure/dynamics_test.py:194-222"
    },
    "91": {
        "file_id": 5,
        "content": "The code snippet is initializing a model and testing its behavior by comparing the observed schedule to an expected linear schedule. The model's 'loss' variable contains the average fraction of \"true\" samples obtained in each epoch. The 'SampleBestBeliefTest' class sets up parameters for sampling, such as number of samples, batch size, latent code size, and number of keypoints.",
        "type": "comment"
    },
    "92": {
        "file_id": 5,
        "content": "    sampled_latent = np.tile(sampled_latent,\n                             [1, self.batch_size, self.latent_code_size])\n    self.sampled_latent = tf.convert_to_tensor(sampled_latent, dtype=tf.float32)\n    # Create sampled_keypoints of shape [num_samples, batch_size,\n    # 3 * num_keypoints] such that samples are numbered from 1 to num_samples:\n    sampled_keypoints = np.arange(self.num_samples)\n    sampled_keypoints = sampled_keypoints[:, np.newaxis, np.newaxis]\n    sampled_keypoints = np.tile(sampled_keypoints,\n                                [1, self.batch_size, 3 * self.num_keypoints])\n    self.sampled_keypoints = tf.convert_to_tensor(\n        sampled_keypoints, dtype=tf.float32)\n    # Create sample_losses of shape [num_samples, batch_size] such that the best\n    # sample varies for the different elements in the batch. For batch example\n    # 0, sample 4 is best; for batch example 1, sample 2 is best, and so on...\n    batch_example = [0, 1, 2, 3]\n    self.best_samples = [4, 2, 1, 3]\n    sample_losses = np.ones((self.num_samples, self.batch_size))",
        "type": "code",
        "location": "/video_structure/dynamics_test.py:223-241"
    },
    "93": {
        "file_id": 5,
        "content": "Creating tensor for sampled latent and keypoints with batch size and specific shapes. Assigning sample_losses values based on best samples for each batch example. Converting numpy arrays to TensorFlow tensors for further computations.",
        "type": "comment"
    },
    "94": {
        "file_id": 5,
        "content": "    sample_losses[self.best_samples, batch_example] = 0.0\n    self.sample_losses = tf.convert_to_tensor(sample_losses, dtype=tf.float32)\n  def testBestSampleIsReturnedDuringTraining(self):\n    with self.session() as sess:\n      tf.keras.backend.set_learning_phase(1)\n      chosen_latent, chosen_keypoints = dynamics._choose_sample(\n          self.sampled_latent, self.sampled_keypoints, self.sample_losses)\n      chosen_latent, chosen_keypoints = sess.run(\n          [chosen_latent, chosen_keypoints])\n    # Check output shapes:\n    self.assertEqual(chosen_latent.shape,\n                     (self.batch_size, self.latent_code_size))\n    self.assertEqual(chosen_keypoints.shape,\n                     (self.batch_size, self.latent_code_size * 2))\n    # Check that the correct sample is chosen for each example in the batch:\n    self.assertEqual(list(chosen_latent[:, 0]), self.best_samples)\n    self.assertEqual(list(chosen_keypoints[:, 0]), self.best_samples)\n  def testFirstSampleIsReturnedDuringInference(self):\n    with self.session() as sess:",
        "type": "code",
        "location": "/video_structure/dynamics_test.py:242-264"
    },
    "95": {
        "file_id": 5,
        "content": "In this code, the `testBestSampleIsReturnedDuringTraining` function tests if the correct sample is chosen during training. It uses a session to run TensorFlow operations and checks the output shape of `chosen_latent` and `chosen_keypoints`. The code also verifies that for each example in the batch, the first element of `chosen_latent` and `chosen_keypoints` matches the expected best samples. Similarly, `testFirstSampleIsReturnedDuringInference` tests if the first sample is chosen during inference.",
        "type": "comment"
    },
    "96": {
        "file_id": 5,
        "content": "      tf.keras.backend.set_learning_phase(0)\n      chosen_latent, chosen_keypoints = dynamics._choose_sample(\n          self.sampled_latent, self.sampled_keypoints, self.sample_losses)\n      chosen_latent, chosen_keypoints = sess.run(\n          [chosen_latent, chosen_keypoints])\n    # Check that the 0th sample is chosen for each example in the batch:\n    np.testing.assert_array_equal(chosen_latent, 0.0 * chosen_latent)\n    np.testing.assert_array_equal(chosen_keypoints, 0.0 * chosen_keypoints)\n  @parameterized.named_parameters(('_with_sampling', False),\n                                  ('_use_mean_instead_of_sample', True))\n  def testWholeLayerRuns(self, use_mean_instead_of_sample):\n    def dummy_decoder(inputs):\n      del inputs\n      return tf.zeros((self.batch_size, 3 * self.num_keypoints))\n    sampler = dynamics.SampleBestBelief(self.num_samples, dummy_decoder,\n                                        use_mean_instead_of_sample)\n    latent_mean = tf.zeros((self.batch_size, self.latent_code_size))\n    latent_std = tf.zeros((self.batch_size, self.latent_code_size))",
        "type": "code",
        "location": "/video_structure/dynamics_test.py:265-286"
    },
    "97": {
        "file_id": 5,
        "content": "This code is testing the whole layer runs of dynamics classifier. It first sets the learning phase to 0, chooses a sample from latent and keypoints, then checks if the 0th sample is chosen for each example in batch using numpy's assert_array_equal function. Two test cases are defined: one without sampling and the other using mean instead of sample. It creates a dummy decoder, initializes sampler with it, and computes latent mean and std.",
        "type": "comment"
    },
    "98": {
        "file_id": 5,
        "content": "    rnn_state = tf.zeros((self.batch_size, 1))\n    observed_keypoints_flat = tf.zeros(\n        (self.batch_size, 3 * self.num_keypoints))\n    chosen_latent, chosen_keypoints = sampler(\n        [latent_mean, latent_std, rnn_state, observed_keypoints_flat])\n    with self.session() as sess:\n      chosen_latent, chosen_keypoints = sess.run(\n          [chosen_latent, chosen_keypoints])\n    self.assertEqual(chosen_latent.shape,\n                     (self.batch_size, self.latent_code_size))\n    self.assertEqual(chosen_keypoints.shape,\n                     (self.batch_size, self.latent_code_size * 2))\nif __name__ == '__main__':\n  absltest.main()",
        "type": "code",
        "location": "/video_structure/dynamics_test.py:287-302"
    },
    "99": {
        "file_id": 5,
        "content": "Initializes RNN state and observed keypoints as zeros. Samples chosen_latent and chosen_keypoints using sampler function. Runs the session to get actual values of chosen_latent and chosen_keypoints. Checks if shapes match expected shapes.",
        "type": "comment"
    }
}