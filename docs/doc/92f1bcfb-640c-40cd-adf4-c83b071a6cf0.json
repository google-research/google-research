{
    "summary": "The code tests VRNN training loop, KLDivergence layer annealing, and verifies dynamics model's sampling schedule consistency. It checks the correctness of samples chosen during training and inference using TensorFlow operations, ensuring shape and expected best samples.",
    "details": [
        {
            "comment": "The code is a Python test file for testing video structure dynamics. It includes necessary imports, sets up hyperparameters, and defines a DynamicsTest class with a setUp method for configuring the tests.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/dynamics_test.py\":0-31",
            "content": "# coding=utf-8\n# Copyright 2023 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Tests for video_structure.vision.\"\"\"\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow.compat.v1 as tf\nfrom video_structure import dynamics\nfrom video_structure import hyperparameters\nclass DynamicsTest(tf.test.TestCase):\n  def setUp(self):\n    # Hyperparameter config for test models:\n    self.cfg = hyperparameters.get_config()\n    self.cfg.batch_size = 4"
        },
        {
            "comment": "The code is testing a minimal Keras training loop for the dynamics model. It sets up configuration parameters, builds a Variational Recurrent Neural Network (VRNN) model, adds loss functions, compiles the model, trains it with provided observed keypoints data, and verifies that the loss does not contain any NaN values. Additionally, it tests the decoder shapes using RNN state and latent code size.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/dynamics_test.py\":32-55",
            "content": "    self.cfg.observed_steps = 2\n    self.cfg.predicted_steps = 2\n    self.cfg.num_keypoints = 3\n    self.cfg.num_rnn_units = 4\n    super().setUp()\n  def testTrainingLossIsNotNan(self):\n    \"\"\"Tests a minimal Keras training loop for the dynamics model.\"\"\"\n    observed_keypoints = np.random.RandomState(0).normal(size=(\n        self.cfg.batch_size, self.cfg.observed_steps + self.cfg.predicted_steps,\n        self.cfg.num_keypoints, 3))\n    model = dynamics.build_vrnn(self.cfg)\n    model.add_loss(tf.nn.l2_loss(model.inputs[0] - model.outputs[0]))  # KP loss\n    model.add_loss(tf.reduce_mean(model.outputs[1]))  # KL loss\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5))\n    history = model.fit(x=observed_keypoints, steps_per_epoch=1, epochs=1)\n    self.assertFalse(\n        np.any(np.isnan(history.history['loss'])),\n        'Loss contains nans: {}'.format(history.history['loss']))\n  def testDecoderShapes(self):\n    rnn_state = tf.zeros((self.cfg.batch_size, self.cfg.num_rnn_units))\n    latent_code = tf.zeros((self.cfg.batch_size, self.cfg.latent_code_size))"
        },
        {
            "comment": "This code is testing the shapes of tensors outputted by the build_decoder and build_posterior_net functions, as well as checking that the KL divergence between identical distributions is zero. The build_decoder function takes in an RNN state and a latent code to produce keypoints. The build_posterior_net function also takes in these inputs and produces means and standard deviations for a latent code. These shapes are then checked against expected values defined by the cfg file. Finally, the code tests that the KL divergence between identical distributions is zero to ensure consistency.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/dynamics_test.py\":56-79",
            "content": "    keypoints = dynamics.build_decoder(self.cfg)([rnn_state, latent_code])\n    self.assertEqual(\n        keypoints.shape.as_list(),\n        [self.cfg.batch_size, self.cfg.num_keypoints * 3])\n  def testPosteriorNetShapes(self):\n    rnn_state = tf.zeros((self.cfg.batch_size, self.cfg.num_rnn_units))\n    keypoints = tf.zeros((self.cfg.batch_size, self.cfg.num_keypoints * 3))\n    means, stds = dynamics.build_posterior_net(self.cfg)([rnn_state, keypoints])\n    self.assertEqual(\n        means.shape.as_list(), [self.cfg.batch_size, self.cfg.latent_code_size])\n    self.assertEqual(\n        stds.shape.as_list(), [self.cfg.batch_size, self.cfg.latent_code_size])\nclass KLDivergenceTest(tf.test.TestCase):\n  def testKLDivergenceIsZero(self):\n    \"\"\"Tests that KL divergence of identical distributions is zero.\"\"\"\n    with self.session() as sess:\n      mean = tf.random.normal((3, 3, 3))\n      std = tf.random.normal((3, 3, 3))\n      kl_divergence = dynamics.KLDivergence()([mean, std, mean, std])\n      result = sess.run([kl_divergence])[0]"
        },
        {
            "comment": "Test if KL divergence layer provides correct results. Compare numpy and TensorFlow results with a tolerance of 0.01.\nTest KLDivergence layer annealing functionality. Compile the model, predicting from input layers for different steps.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/dynamics_test.py\":80-106",
            "content": "    np.testing.assert_array_equal(result, result * 0.0)\n  def testNonzeroKLDivergence(self):\n    \"\"\"Test that KL divergence layer provides correct result.\"\"\"\n    mu = 2.0\n    sigma = 2.0\n    n = 3\n    # https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Multivariate_normal_distributions\n    result_np = 0.5 * (sigma ** 2 + mu ** 2 - np.log(sigma ** 2) - 1) * n\n    with self.session() as sess:\n      kl_divergence = dynamics.KLDivergence()(\n          [tf.zeros(n), tf.ones(n), tf.zeros(n) + mu, tf.ones(n) * sigma])\n      result_tf = sess.run(kl_divergence)\n    np.testing.assert_almost_equal(result_tf, result_np, decimal=4)\n  def testKLDivergenceAnnealing(self):\n    inputs = tf.keras.Input(1)\n    outputs = dynamics.KLDivergence(kl_annealing_steps=4)([\n        inputs, inputs,\n        tf.keras.layers.Lambda(lambda x: x + 1.0)(inputs), inputs\n    ])\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    model.compile('sgd', 'mse')\n    obtained_kl = [model.predict(x=[1], steps=1)]  # Should be zero before fit."
        },
        {
            "comment": "The code sets up a simple model where the ground-truth data is a tensor of ones and the predicted data is a tensor of zeros. It then tests if the TrainingStepCounter correctly counts the training phase steps. The model is fitted with the given data and the step count is obtained through a session run. Finally, it asserts that the obtained step count is equal to [0, 0.125, 0.25, 0.375, 0.5, 0.5].",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/dynamics_test.py\":107-135",
            "content": "    for _ in range(5):\n      model.fit(x=[1], y=[1], epochs=1, steps_per_epoch=1)\n      obtained_kl.append(model.predict(x=[1], steps=1))\n    obtained_kl = np.array(obtained_kl).ravel()\n    np.testing.assert_array_almost_equal(\n        obtained_kl, [0, 0.125, 0.25, 0.375, 0.5, 0.5])\nclass TrainingStepCounterTest(tf.test.TestCase):\n  def setUp(self):\n    # Set up simple model in which the ground-truth data is a tensor of ones and\n    # the predicted data is a tensor of zeros.\n    super().setUp()  # Sets up the TensorFlow environment, so call it early.\n    self.sess = tf.keras.backend.get_session()\n    inputs = tf.keras.Input(1)\n    outputs = dynamics.TrainingStepCounter()(inputs)\n    self.model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    self.model.compile('sgd', 'mse')\n  def testTrainingPhaseStepsAreCounted(self):\n    num_epochs = 2\n    steps_per_epoch = 5\n    self.model.fit(\n        x=np.zeros(1),\n        y=np.zeros(1),\n        epochs=num_epochs,\n        steps_per_epoch=steps_per_epoch)\n    step_count = self.sess.run(self.model.layers[-1].weights[0])"
        },
        {
            "comment": "This code snippet is testing the functionality of a TrainingStepCounter class. The first test checks if the step count is equal to zero when no prediction is made, the second test checks that repeated calls don't increase the counter twice and the third test verifies that the step count is equal to the number of epochs multiplied by steps per epoch during training.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/dynamics_test.py\":136-167",
            "content": "    self.assertEqual(step_count, num_epochs * steps_per_epoch)\n  def testTestingPhaseStepsAreNotCounted(self):\n    self.model.predict(x=np.zeros(1), steps=10)\n    step_count = self.sess.run(self.model.layers[-1].weights[0])\n    self.assertEqual(step_count, 0.0)\n  def testMultipleCallsAreCountedOnce(self):\n    \"\"\"Calling the same layer twice should not increase the counter twice.\"\"\"\n    # Create a model that calls the same TrainingStepCounter layer twice:\n    counter = dynamics.TrainingStepCounter()\n    inputs = tf.keras.Input(1)\n    output1 = counter(inputs)\n    output2 = counter(inputs)\n    model = tf.keras.Model(inputs=inputs, outputs=[output1, output2])\n    model.compile('sgd', 'mse')\n    # Train:\n    num_epochs = 2\n    steps_per_epoch = 5\n    model.fit(\n        x=np.zeros(1),\n        y=[np.zeros(1), np.zeros(1)],\n        epochs=num_epochs,\n        steps_per_epoch=steps_per_epoch)\n    step_count = self.sess.run(model.layers[-1].weights[0])\n    self.assertEqual(step_count, num_epochs * steps_per_epoch)\nclass ScheduledSamplingTest(tf.test.TestCase):"
        },
        {
            "comment": "This code sets up a test environment for evaluating the sampling schedule of a dynamics model. It creates a simple model where ground-truth data is a tensor of ones and predicted data is zeros. The test runs the model for a specified number of epochs, ensuring that steps_per_epoch is large enough to average out sampling randomness.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/dynamics_test.py\":169-192",
            "content": "  def setUp(self):\n    # Set up simple model in which the ground-truth data is a tensor of ones and\n    # the predicted data is a tensor of zeros.\n    self.ramp_steps = 5000\n    self.p_true_start = 1.0\n    self.p_true_end = 0.0\n    super().setUp()  # Sets up the TensorFlow environment, so call it early.\n    self.sess = tf.keras.backend.get_session()\n    inputs = tf.keras.Input(1)\n    true = tf.keras.layers.Lambda(lambda x: x + 1.0)(inputs)\n    pred = tf.keras.layers.Lambda(lambda x: x + 0.0)(inputs)\n    outputs = dynamics.ScheduledSampling(\n        p_true_start=self.p_true_start,\n        p_true_end=self.p_true_end,\n        ramp_steps=self.ramp_steps,\n    )([true, pred])\n    self.model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    self.model.compile('sgd', 'mse')\n  def testSamplingSchedule(self):\n    num_epochs = 5\n    steps_per_epoch = self.ramp_steps / num_epochs\n    assert steps_per_epoch >= 1000, ('steps_per_epoch should be large enough to'\n                                     'average out the sampling randomness.')"
        },
        {
            "comment": "The code snippet is initializing a model and testing its behavior by comparing the observed schedule to an expected linear schedule. The model's 'loss' variable contains the average fraction of \"true\" samples obtained in each epoch. The 'SampleBestBeliefTest' class sets up parameters for sampling, such as number of samples, batch size, latent code size, and number of keypoints.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/dynamics_test.py\":193-221",
            "content": "    history = self.model.fit(\n        x=np.zeros(1),\n        y=np.zeros(1),\n        epochs=num_epochs,\n        steps_per_epoch=steps_per_epoch)\n    expected_schedule = np.linspace(\n        self.p_true_start, self.p_true_end, 2 * num_epochs + 1)[1::2]\n    # Note that the model is set up such that the \"loss\" variable contains the\n    # average fraction of \"true\" samples obtained in each epoch:\n    np.testing.assert_array_almost_equal(\n        history.history['loss'],\n        expected_schedule,\n        decimal=1,\n        err_msg='Observed schedule deviates from expected linear schedule.')\nclass SampleBestBeliefTest(tf.test.TestCase, parameterized.TestCase):\n  def setUp(self):\n    super().setUp()\n    self.num_samples = 5\n    self.batch_size = 4\n    self.latent_code_size = 3\n    self.num_keypoints = 2\n    # Create sampled_latent of shape [num_samples, batch_size, latent_code_size]\n    # such that samples are numbered from 1 to num_samples:\n    sampled_latent = np.arange(self.num_samples)[:, np.newaxis, np.newaxis]"
        },
        {
            "comment": "Creating tensor for sampled latent and keypoints with batch size and specific shapes. Assigning sample_losses values based on best samples for each batch example. Converting numpy arrays to TensorFlow tensors for further computations.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/dynamics_test.py\":222-240",
            "content": "    sampled_latent = np.tile(sampled_latent,\n                             [1, self.batch_size, self.latent_code_size])\n    self.sampled_latent = tf.convert_to_tensor(sampled_latent, dtype=tf.float32)\n    # Create sampled_keypoints of shape [num_samples, batch_size,\n    # 3 * num_keypoints] such that samples are numbered from 1 to num_samples:\n    sampled_keypoints = np.arange(self.num_samples)\n    sampled_keypoints = sampled_keypoints[:, np.newaxis, np.newaxis]\n    sampled_keypoints = np.tile(sampled_keypoints,\n                                [1, self.batch_size, 3 * self.num_keypoints])\n    self.sampled_keypoints = tf.convert_to_tensor(\n        sampled_keypoints, dtype=tf.float32)\n    # Create sample_losses of shape [num_samples, batch_size] such that the best\n    # sample varies for the different elements in the batch. For batch example\n    # 0, sample 4 is best; for batch example 1, sample 2 is best, and so on...\n    batch_example = [0, 1, 2, 3]\n    self.best_samples = [4, 2, 1, 3]\n    sample_losses = np.ones((self.num_samples, self.batch_size))"
        },
        {
            "comment": "In this code, the `testBestSampleIsReturnedDuringTraining` function tests if the correct sample is chosen during training. It uses a session to run TensorFlow operations and checks the output shape of `chosen_latent` and `chosen_keypoints`. The code also verifies that for each example in the batch, the first element of `chosen_latent` and `chosen_keypoints` matches the expected best samples. Similarly, `testFirstSampleIsReturnedDuringInference` tests if the first sample is chosen during inference.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/dynamics_test.py\":241-263",
            "content": "    sample_losses[self.best_samples, batch_example] = 0.0\n    self.sample_losses = tf.convert_to_tensor(sample_losses, dtype=tf.float32)\n  def testBestSampleIsReturnedDuringTraining(self):\n    with self.session() as sess:\n      tf.keras.backend.set_learning_phase(1)\n      chosen_latent, chosen_keypoints = dynamics._choose_sample(\n          self.sampled_latent, self.sampled_keypoints, self.sample_losses)\n      chosen_latent, chosen_keypoints = sess.run(\n          [chosen_latent, chosen_keypoints])\n    # Check output shapes:\n    self.assertEqual(chosen_latent.shape,\n                     (self.batch_size, self.latent_code_size))\n    self.assertEqual(chosen_keypoints.shape,\n                     (self.batch_size, self.latent_code_size * 2))\n    # Check that the correct sample is chosen for each example in the batch:\n    self.assertEqual(list(chosen_latent[:, 0]), self.best_samples)\n    self.assertEqual(list(chosen_keypoints[:, 0]), self.best_samples)\n  def testFirstSampleIsReturnedDuringInference(self):\n    with self.session() as sess:"
        },
        {
            "comment": "This code is testing the whole layer runs of dynamics classifier. It first sets the learning phase to 0, chooses a sample from latent and keypoints, then checks if the 0th sample is chosen for each example in batch using numpy's assert_array_equal function. Two test cases are defined: one without sampling and the other using mean instead of sample. It creates a dummy decoder, initializes sampler with it, and computes latent mean and std.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/dynamics_test.py\":264-285",
            "content": "      tf.keras.backend.set_learning_phase(0)\n      chosen_latent, chosen_keypoints = dynamics._choose_sample(\n          self.sampled_latent, self.sampled_keypoints, self.sample_losses)\n      chosen_latent, chosen_keypoints = sess.run(\n          [chosen_latent, chosen_keypoints])\n    # Check that the 0th sample is chosen for each example in the batch:\n    np.testing.assert_array_equal(chosen_latent, 0.0 * chosen_latent)\n    np.testing.assert_array_equal(chosen_keypoints, 0.0 * chosen_keypoints)\n  @parameterized.named_parameters(('_with_sampling', False),\n                                  ('_use_mean_instead_of_sample', True))\n  def testWholeLayerRuns(self, use_mean_instead_of_sample):\n    def dummy_decoder(inputs):\n      del inputs\n      return tf.zeros((self.batch_size, 3 * self.num_keypoints))\n    sampler = dynamics.SampleBestBelief(self.num_samples, dummy_decoder,\n                                        use_mean_instead_of_sample)\n    latent_mean = tf.zeros((self.batch_size, self.latent_code_size))\n    latent_std = tf.zeros((self.batch_size, self.latent_code_size))"
        },
        {
            "comment": "Initializes RNN state and observed keypoints as zeros. Samples chosen_latent and chosen_keypoints using sampler function. Runs the session to get actual values of chosen_latent and chosen_keypoints. Checks if shapes match expected shapes.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/dynamics_test.py\":286-301",
            "content": "    rnn_state = tf.zeros((self.batch_size, 1))\n    observed_keypoints_flat = tf.zeros(\n        (self.batch_size, 3 * self.num_keypoints))\n    chosen_latent, chosen_keypoints = sampler(\n        [latent_mean, latent_std, rnn_state, observed_keypoints_flat])\n    with self.session() as sess:\n      chosen_latent, chosen_keypoints = sess.run(\n          [chosen_latent, chosen_keypoints])\n    self.assertEqual(chosen_latent.shape,\n                     (self.batch_size, self.latent_code_size))\n    self.assertEqual(chosen_keypoints.shape,\n                     (self.batch_size, self.latent_code_size * 2))\nif __name__ == '__main__':\n  absltest.main()"
        }
    ]
}