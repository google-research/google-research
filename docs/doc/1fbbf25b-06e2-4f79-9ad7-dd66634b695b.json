{
    "summary": "The \"ConfigDict\" class extends Python dictionaries, providing convenient attribute access. The code defines default hyperparameters for a video structure model and neural network layers, including directories, architecture, optimization settings, and loss functions.",
    "details": [
        {
            "comment": "The code defines a class \"ConfigDict\" that extends the built-in Python dictionary. This class allows its keys to be accessed as attributes, which provides a more convenient way of accessing and modifying values in the dictionary. The class also overrides the __getattr__ and __setattr__ methods to achieve this functionality.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/hyperparameters.py\":0-33",
            "content": "# coding=utf-8\n# Copyright 2023 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Hyperparameters of the structured video prediction models.\"\"\"\nimport tensorflow.compat.v1 as tf\nclass ConfigDict(dict):\n  \"\"\"A dictionary whose keys can be accessed as attributes.\"\"\"\n  def __getattr__(self, name):\n    try:\n      return self[name]\n    except KeyError:\n      raise AttributeError(name)\n  def __setattr__(self, name, value):\n    self[name] = value\n  def get(self, key, default=None):\n    \"\"\"Allows to specify defaults when accessing the config.\"\"\""
        },
        {
            "comment": "The code defines default values for hyperparameters in a video structure model. It includes settings for directories, architecture, optimization, image sequence parameters, keypoint encoding, dynamics, and loss functions.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/hyperparameters.py\":34-81",
            "content": "    if key not in self:\n      return default\n    return self[key]\ndef get_config():\n  \"\"\"Default values for all hyperparameters.\"\"\"\n  cfg = ConfigDict()\n  # Directories:\n  cfg.dataset = 'debug'\n  cfg.data_dir = 'video_structure/testdata'\n  cfg.train_dir = ''\n  cfg.test_dir = ''\n  # Architecture:\n  cfg.layers_per_scale = 2\n  cfg.conv_layer_kwargs = _conv_layer_kwargs()\n  cfg.dense_layer_kwargs = _dense_layer_kwargs()\n  # Optimization:\n  cfg.batch_size = 32\n  cfg.steps_per_epoch = 100\n  cfg.num_epochs = 100\n  cfg.learning_rate = 0.001\n  cfg.clipnorm = 10\n  # Image sequence parameters:\n  cfg.observed_steps = 8\n  cfg.predicted_steps = 8\n  # Keypoint encoding settings:\n  cfg.num_keypoints = 64\n  cfg.heatmap_width = 16\n  cfg.heatmap_regularization = 5.0\n  cfg.keypoint_width = 1.5\n  cfg.num_encoder_filters = 32\n  cfg.separation_loss_scale = 10.0\n  cfg.separation_loss_sigma = 0.1\n  # Dynamics:\n  cfg.num_rnn_units = 512\n  cfg.prior_net_dim = 128\n  cfg.posterior_net_dim = 128\n  cfg.latent_code_size = 16\n  cfg.kl_loss_scale = 0.0\n  cfg.kl_annealing_steps = 1000"
        },
        {
            "comment": "Code snippet contains functions to define default hyperparameters for convolutional and dense layers in a neural network. It also sets deterministic belief, scheduled sampling parameters, and number of samples for Bayesian optimization Markov chain.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/hyperparameters.py\":82-117",
            "content": "  cfg.use_deterministic_belief = False\n  cfg.scheduled_sampling_ramp_steps = (\n      cfg.steps_per_epoch * int(cfg.num_epochs * 0.8))\n  cfg.scheduled_sampling_p_true_start_obs = 1.0\n  cfg.scheduled_sampling_p_true_end_obs = 0.1\n  cfg.scheduled_sampling_p_true_start_pred = 1.0\n  cfg.scheduled_sampling_p_true_end_pred = 0.5\n  cfg.num_samples_for_bom = 10\n  return cfg\ndef _conv_layer_kwargs():\n  \"\"\"Returns a configDict with default conv layer hyperparameters.\"\"\"\n  cfg = ConfigDict()\n  cfg.kernel_size = 3\n  cfg.padding = 'same'\n  cfg.activation = tf.nn.leaky_relu\n  cfg.kernel_regularizer = tf.keras.regularizers.l2(1e-4)\n  # He-uniform initialization is suggested by this paper:\n  # https://arxiv.org/abs/1803.01719\n  # The paper only considers ReLU units and it might be different for leaky\n  # ReLU, but it is a better guess than Glorot.\n  cfg.kernel_initializer = 'he_uniform'\n  return cfg\ndef _dense_layer_kwargs():\n  \"\"\"Returns a configDict with default dense layer hyperparameters.\"\"\"\n  cfg = ConfigDict()\n  cfg.activation = tf.nn.relu"
        },
        {
            "comment": "These lines set the kernel initializer to 'he_uniform' in the configuration object and return it. The 'he_uniform' is an initialization method, which stands for He-normal distribution. It helps initialize the weights of the neural network layers with a uniform distribution to promote learning efficiency and stability during training.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/hyperparameters.py\":118-120",
            "content": "  cfg.kernel_initializer = 'he_uniform'\n  return cfg"
        }
    ]
}