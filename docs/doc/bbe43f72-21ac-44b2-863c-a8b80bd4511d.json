{
    "summary": "The code integrates vision components in a video model using TensorFlow ops as Keras layers, defines an encoding function for image sequences, and constructs an encoder-decoder model for video analysis and image sequence generation, offering adjustable parameters and heatmap penalty calculation.",
    "details": [
        {
            "comment": "This code is for vision-related components in a structured video representation model. It performs a pixels to keypoints transformation and wraps TensorFlow ops as Keras layers, including the stack_time function.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/vision.py\":0-30",
            "content": "# coding=utf-8\n# Copyright 2023 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Vision-related components of the structured video representation model.\nThese components perform the pixels <--> keypoints transformation.\n\"\"\"\nimport functools\nimport numpy as np\nimport tensorflow.compat.v1 as tf\nfrom video_structure import ops\nlayers = tf.keras.layers\n# Wrap commonly used Tensorflow ops in Keras layers:\ndef stack_time(inputs):\n  return layers.Lambda(lambda x: tf.stack(x, axis=1, name='stack_time'))(inputs)"
        },
        {
            "comment": "This code defines a function build_images_to_keypoints_net that constructs a model for encoding image sequences into keypoint sequences. It applies a convolutional feature extractor to all images in the sequence, reduces the feature maps to num_keypoints heatmaps and converts them to (x, y, scale)-keypoints using lambda layers. The function takes a ConfigDict with model hyperparameters and an image shape tuple as inputs, and returns a tf.keras.Model object.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/vision.py\":33-66",
            "content": "def unstack_time(inputs):\n  return layers.Lambda(lambda x: tf.unstack(x, axis=1, name='unstack_time'))(\n      inputs)\ndef add_coord_channels(inputs):\n  return layers.Lambda(ops.add_coord_channels)(inputs)\ndef maps_to_keypoints(inputs):\n  return layers.Lambda(ops.maps_to_keypoints)(inputs)\ndef build_images_to_keypoints_net(cfg, image_shape):\n  \"\"\"Builds a model that encodes an image sequence into a keypoint sequence.\n  The model applies the same convolutional feature extractor to all images in\n  the sequence. The feature maps are then reduced to num_keypoints heatmaps, and\n  the heatmaps to (x, y, scale)-keypoints.\n  Args:\n    cfg: ConfigDict with model hyperparamters.\n    image_shape: Image shape tuple: (num_timesteps, H, W, C).\n  Returns:\n    A tf.keras.Model object.\n  \"\"\"\n  image_sequence = tf.keras.Input(shape=image_shape, name='encoder_input')\n  # Adjust channel number to account for add_coord_channels:\n  encoder_input_shape = image_shape[1:-1] + (image_shape[-1] + 2,)\n  # Build feature extractor:\n  image_encoder = build_image_encoder("
        },
        {
            "comment": "This code builds an encoder network for video structure analysis using a specified input shape, number of filters, heatmap width, and layers per scale. It then adds a final layer that maps to the desired number of heatmaps, ensuring non-negative values. The timesteps are separated into a list and for each image in the list, it applies encoding, produces heatmaps, and converts them to keypoints. Finally, it combines all the heatmaps across the time dimension.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/vision.py\":67-97",
            "content": "      input_shape=encoder_input_shape,\n      initial_num_filters=cfg.num_encoder_filters,\n      output_map_width=cfg.heatmap_width,\n      layers_per_scale=cfg.layers_per_scale,\n      **cfg.conv_layer_kwargs)\n  # Build final layer that maps to the desired number of heatmaps:\n  features_to_keypoint_heatmaps = layers.Conv2D(\n      filters=cfg.num_keypoints,\n      kernel_size=1,\n      padding='same',\n      activation=tf.nn.softplus,  # Heatmaps must be non-negative.\n      activity_regularizer=functools.partial(\n          _get_heatmap_penalty, factor=cfg.heatmap_regularization))\n  # Separate timesteps into list:\n  image_list = unstack_time(image_sequence)\n  heatmaps_list = []\n  keypoints_list = []\n  # Image to keypoints:\n  for image in image_list:\n    image = add_coord_channels(image)\n    encoded = image_encoder(image)\n    heatmaps = features_to_keypoint_heatmaps(encoded)\n    keypoints = maps_to_keypoints(heatmaps)\n    heatmaps_list.append(heatmaps)\n    keypoints_list.append(keypoints)\n  # Combine timesteps:\n  heatmaps = stack_time(heatmaps_list)"
        },
        {
            "comment": "The code builds a model that reconstructs an image sequence from a keypoint sequence. The model architecture takes keypoint_sequence, image[0], and keypoints[0] as inputs to generate the output of the reconstructed image sequence. It concatenates Gaussian maps for keypoints obtained from the initial frame to help the decoder inpaint occluded image regions. The function returns a tf.keras.Model object.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/vision.py\":98-125",
            "content": "  keypoints = stack_time(keypoints_list)\n  return tf.keras.Model(inputs=image_sequence, outputs=[keypoints, heatmaps])\ndef build_keypoints_to_images_net(cfg, image_shape):\n  \"\"\"Builds a model to reconstructs an image sequence from a keypoint sequence.\n  Model architecture:\n    (keypoint_sequence, image[0], keypoints[0]) --> reconstructed_image_sequence\n  For all frames image[t] we also we also concatenate the Gaussian maps for\n  the keypoints obtained from the initial frame image[0]. This helps the\n  decoder \"inpaint\" the image regions that are occluded by objects in the first\n  frame.\n  Args:\n    cfg: ConfigDict with model hyperparameters.\n    image_shape: Image shape tuple: (num_timesteps, H, W, C).\n  Returns:\n    A tf.keras.Model object.\n  \"\"\"\n  num_timesteps = cfg.observed_steps + cfg.predicted_steps\n  keypoints_shape = [num_timesteps, cfg.num_keypoints, 3]\n  keypoints_sequence = tf.keras.Input(shape=keypoints_shape, name='keypoints')\n  first_frame = tf.keras.Input(shape=image_shape[1:], name='first_frame')"
        },
        {
            "comment": "This code is building the network architecture for a video structure analysis model. It defines input layers (keypoints), appearance feature extractor, image decoder, and adjustment layers for channel numbers of decoder input and output image. The model aims to extract appearance features from the first frame and reconstruct images using Gaussian maps.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/vision.py\":126-150",
            "content": "  first_frame_keypoints = tf.keras.Input(\n      shape=keypoints_shape[1:], name='first_frame_keypoints')\n  # Build encoder net to extract appearance features from the first frame:\n  appearance_feature_extractor = build_image_encoder(\n      input_shape=image_shape[1:],\n      initial_num_filters=cfg.num_encoder_filters,\n      layers_per_scale=cfg.layers_per_scale,\n      **cfg.conv_layer_kwargs)\n  # Build image decoder that goes from Gaussian maps to reconstructed images:\n  num_encoder_output_channels = (\n      cfg.num_encoder_filters * image_shape[1] // cfg.heatmap_width)\n  input_shape = [\n      cfg.heatmap_width, cfg.heatmap_width, num_encoder_output_channels]\n  image_decoder = build_image_decoder(\n      input_shape=input_shape,\n      output_width=image_shape[1],\n      layers_per_scale=cfg.layers_per_scale,\n      **cfg.conv_layer_kwargs)\n  # Build layers to adjust channel numbers for decoder input and output image:\n  kwargs = dict(cfg.conv_layer_kwargs)\n  kwargs['kernel_size'] = 1\n  adjust_channels_of_decoder_input = layers.Conv2D("
        },
        {
            "comment": "This code builds and utilizes a Conv2D layer to adjust the output image's channels, followed by a Lambda layer for keypoints-to-maps conversion. It retrieves features and maps for the first frame while handling differences between obs and pred. The code then unstacks time in keypoints_sequence, loops over timesteps to convert keypoints to pixel maps, and reconstruct images.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/vision.py\":151-182",
            "content": "      num_encoder_output_channels, **kwargs)\n  kwargs = dict(cfg.conv_layer_kwargs)\n  kwargs['kernel_size'] = 1\n  kwargs['activation'] = None\n  adjust_channels_of_output_image = layers.Conv2D(\n      image_shape[-1], **kwargs)\n  # Build keypoints_to_maps layer:\n  keypoints_to_maps = layers.Lambda(\n      functools.partial(\n          ops.keypoints_to_maps,\n          sigma=cfg.keypoint_width,\n          heatmap_width=cfg.heatmap_width))\n  # Get features and maps for first frame:\n  # Note that we cannot use the Gaussian maps above because the\n  # first_frame_keypoints may be different than the keypoints (i.e. obs vs\n  # pred).\n  first_frame_features = appearance_feature_extractor(first_frame)\n  first_frame_gaussian_maps = keypoints_to_maps(first_frame_keypoints)\n  # Separate timesteps:\n  keypoints_list = unstack_time(keypoints_sequence)\n  image_list = []\n  # Loop over timesteps:\n  for keypoints in keypoints_list:\n    # Convert keypoints to pixel maps:\n    gaussian_maps = keypoints_to_maps(keypoints)\n    # Reconstruct image:"
        },
        {
            "comment": "This code snippet combines different representations of a video sequence to generate an image sequence. It uses convolutional layers, concatenation, and channel adjustments for the input and output images. The model then predicts the change from the first frame of the sequence by stacking timesteps, appending the first frame, and returning a Keras Model.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/vision.py\":183-206",
            "content": "    combined_representation = layers.Concatenate(axis=-1)(\n        [gaussian_maps, first_frame_features, first_frame_gaussian_maps])\n    combined_representation = add_coord_channels(combined_representation)\n    combined_representation = adjust_channels_of_decoder_input(\n        combined_representation)\n    decoded_representation = image_decoder(combined_representation)\n    image_list.append(adjust_channels_of_output_image(decoded_representation))\n  # Combine timesteps:\n  image_sequences = stack_time(image_list)\n  # Add in the first frame of the sequence such that the model only needs to\n  # predict the change from the first frame:\n  image_sequences = layers.Add()([image_sequences, first_frame[:, None, Ellipsis]])\n  return tf.keras.Model(\n      inputs=[keypoints_sequence, first_frame, first_frame_keypoints],\n      outputs=image_sequences)\ndef build_image_encoder(\n    input_shape, initial_num_filters=32, output_map_width=16,\n    layers_per_scale=1, **conv_layer_kwargs):\n  \"\"\"Extracts feature maps from images."
        },
        {
            "comment": "This code defines a function that creates an encoder for image processing. It takes input shape, number of filters, output map width, and layers per scale as arguments. The code checks if the ratio of input width to output map width is a perfect square; if not, it raises a ValueError.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/vision.py\":208-232",
            "content": "  The encoder iteratively halves the resolution and doubles the number of\n  filters until the size of the feature maps is output_map_width by\n  output_map_width.\n  Args:\n    input_shape: Shape of the input image (without batch dimension).\n    initial_num_filters: Number of filters to apply at the input resolution.\n    output_map_width: Width of the output feature maps.\n    layers_per_scale: How many additional size-preserving conv layers to apply\n      at each map scale.\n    **conv_layer_kwargs: Passed to layers.Conv2D.\n  Raises:\n    ValueError: If the width of the input image is not compatible with\n      output_map_width, i.e. if input_width/output_map_width is not a perfect\n      square.\n  \"\"\"\n  inputs = tf.keras.Input(shape=input_shape, name='encoder_input')\n  if np.log2(input_shape[0] / output_map_width) % 1:\n    raise ValueError(\n        'The ratio of input width and output_map_width must be a perfect '\n        'square, but got {} and {} with ratio {}'.format(\n            input_shape[0], output_map_width, inputs[0]/output_map_width))"
        },
        {
            "comment": "This code defines an image encoder and a function to build an image decoder. The image encoder expands the input image, reduces its resolution by doubling the number of filters, and applies downsampling blocks until the feature map width is equal to the specified output_map_width. Meanwhile, the image decoder aims to decode images from feature maps, iteratively increasing the resolution and halving the number of filters until the size of the feature maps reaches the specified output_width. Both functions use convolutional layers with adjustable parameters (layers_per_scale, conv_layer_kwargs) for flexibility in the model architecture.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/vision.py\":234-263",
            "content": "  # Expand image to initial_num_filters maps:\n  x = layers.Conv2D(initial_num_filters, **conv_layer_kwargs)(inputs)\n  for _ in range(layers_per_scale):\n    x = layers.Conv2D(initial_num_filters, **conv_layer_kwargs)(x)\n  # Apply downsampling blocks until feature map width is output_map_width:\n  width = int(inputs.get_shape()[1])\n  num_filters = initial_num_filters\n  while width > output_map_width:\n    num_filters *= 2\n    width //= 2\n    # Reduce resolution:\n    x = layers.Conv2D(num_filters, strides=2, **conv_layer_kwargs)(x)\n    # Apply additional layers:\n    for _ in range(layers_per_scale):\n      x = layers.Conv2D(num_filters, strides=1, **conv_layer_kwargs)(x)\n  return tf.keras.Model(inputs=inputs, outputs=x, name='image_encoder')\ndef build_image_decoder(\n    input_shape, output_width, layers_per_scale=1, **conv_layer_kwargs):\n  \"\"\"Decodes images from feature maps.\n  The encoder iteratively doubles the resolution and halves the number of\n  filters until the size of the feature maps is output_width.\n  Args:"
        },
        {
            "comment": "This code defines a function that takes input image shape, output width, and number of convolutional layers to apply at each map scale. It checks for compatibility between input and output widths. If the ratio is not a perfect square, it raises a ValueError. The function expands filters until there are 'filters_out' channels by upsampling the input image.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/vision.py\":264-290",
            "content": "    input_shape: Shape of the input image (without batch dimension).\n    output_width: Width of the output image.\n    layers_per_scale: How many additional size-preserving conv layers to apply\n      at each map scale.\n    **conv_layer_kwargs: Passed to layers.Conv2D.\n  Raises:\n    ValueError: If the width of the input feature maps is not compatible with\n      output_width, i.e. if output_width/input_map_width is not a perfect\n      square.\n  \"\"\"\n  feature_maps = tf.keras.Input(shape=input_shape, name='feature_maps')\n  num_levels = np.log2(output_width / input_shape[0])\n  if num_levels % 1:\n    raise ValueError(\n        'The ratio of output_width and input width must be a perfect '\n        'square, but got {} and {} with ratio {}'.format(\n            output_width, input_shape[0], output_width/input_shape[0]))\n  # Expand until we have filters_out channels:\n  x = feature_maps\n  num_filters = input_shape[-1]\n  def upsample(x):\n    new_size = [x.get_shape()[1] * 2, x.get_shape()[2] * 2]\n    return tf.image.resize_bilinear(x, new_size, align_corners=True)"
        },
        {
            "comment": "This code defines a function that creates an image decoder model and another function that calculates a heatmap penalty using L1-loss to encourage sparsity. The image decoder takes input feature maps, performs downsampling and convolutions, and returns output with the same spatial dimensions as input feature maps. The _get_heatmap_penalty function computes the mean of activation values in each heatmap and sums their absolute values to calculate the penalty.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/vision.py\":291-309",
            "content": "  for _ in range(int(num_levels)):\n    num_filters //= 2\n    x = layers.Lambda(upsample)(x)\n    # Apply additional layers:\n    for _ in range(layers_per_scale):\n      x = layers.Conv2D(num_filters, **conv_layer_kwargs)(x)\n  return tf.keras.Model(inputs=feature_maps, outputs=x, name='image_decoder')\ndef _get_heatmap_penalty(weight_matrix, factor):\n  \"\"\"L1-loss on mean heatmap activations, to encourage sparsity.\"\"\"\n  weight_shape = weight_matrix.shape.as_list()\n  assert len(weight_shape) == 4, weight_shape\n  heatmap_mean = tf.reduce_mean(weight_matrix, axis=(1, 2))\n  penalty = tf.reduce_mean(tf.abs(heatmap_mean))\n  return penalty * factor"
        }
    ]
}