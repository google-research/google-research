{
    "summary": "This code creates a PyTorch dataset for timeline modeling, handling variable-length video tokens and providing constants. It utilizes input data in dictionaries, pads sequences, and returns padded tensors for features, labels, and masks. Additionally, it defines classes for loading and validating Google Cloud Storage datasets with partitions and features.",
    "details": [
        {
            "comment": "Constructs a PyTorch dataset for timeline modeling, includes functions to collate topics with variable-length video tokens. Provides constants NUM_OF_TRAINING_SAMPLES, NUM_OF_VALIDATION_SAMPLES, NUM_OF_TEST_SAMPLES and MAX_NUM_CLUSTERS.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/dataset.py\":0-33",
            "content": "# coding=utf-8\n# Copyright 2023 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Construct PyTorch dataset for timeline modeling.\"\"\"\nimport glob\nimport os\nfrom typing import Dict\nimport tensorflow as tf\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import Dataset\nNUM_OF_TRAINING_SAMPLES = 9936\nNUM_OF_VALIDATION_SAMPLES = 1255\nNUM_OF_TEST_SAMPLES = 1220\nMAX_NUM_CLUSTERS = 24\ndef collate_topics(topic_dicts,\n                   max_num_cluster=MAX_NUM_CLUSTERS):\n  \"\"\"Customized batch collate function for padding variable-length video tokens."
        },
        {
            "comment": "This function takes a list of dictionaries containing 'video_features', 'cluster_text_features', and 'video_cluster_label' data, along with the maximum number of clusters. It returns a dictionary with 'video_features', 'cluster_text_features', 'video_cluster_label', 'video_padding_mask', and 'cluster_non_padding_mask' as tensors. The function handles variable-length video and cluster data within each batch, padding the features and labels accordingly.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/dataset.py\":35-53",
            "content": "  For example, if a batch has two data sample. The first one has 3 videos and 2\n  clusters, while the second one has 4 videos and 3 clusters. Then, the\n  data_batch['video_padding_mask'] for this batch is a (2,4) tensor: [[0, 0, 0,\n  1], [0, 0, 0, 0]], where 1 denotes video padding token. The\n  data_batch['cluster_non_padding_mask'] for this batch is a (2, 24) tensor:\n  [[1, 1, 0, ..., 0], [1, 1, 1, 0, ..., 0]], where 0 denotes the cluster padding\n  token. The features and labels are also padded accordingly.\n  Args:\n    topic_dicts (list[dict[str, torch.Tensor]]): the list of data to be batched,\n      where each data is a dict with keys 'video_features',\n      'cluster_text_features', and 'video_cluster_label'.\n    max_num_cluster: the maximum number of clusters in the dataset, which is\n      fixed to be 24 in our dataset.\n  Returns:\n    A dict with keys 'video_features', 'cluster_text_features',\n    'video_cluster_label', 'video_padding_mask', and 'cluster_non_padding_mask'.\n    Each value is a tensor. The first dimension of each value is batch_size,"
        },
        {
            "comment": "This code is creating a 'data_batch' with video features, padding masks, and cluster labels for a batch of topic dictionaries. The video features are padded to the maximum length of the input, and padding value 1 is used in the padding mask. Additionally, the first cluster sequence in each batch is padded to the max_num_cluster length. This ensures consistency across batches.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/dataset.py\":54-78",
            "content": "    which is also the length of the input .\n  \"\"\"\n  data_batch = {}\n  data_batch['video_features'] = pad_sequence(\n      [topic_dict['video_features'] for topic_dict in topic_dicts],\n      batch_first=True)\n  mask = [\n      torch.zeros_like(\n          topic_dict['video_features'][Ellipsis, 0].squeeze(-1),\n          dtype=torch.bool).view(-1) for topic_dict in topic_dicts\n  ]\n  data_batch['video_padding_mask'] = pad_sequence(\n      mask, batch_first=True, padding_value=1)\n  data_batch['video_cluster_label'] = pad_sequence(\n      [topic_dict['video_cluster_label'] for topic_dict in topic_dicts],\n      batch_first=True,\n      padding_value=-1)\n  # Pad the first cluster sequence to the max_num_cluster length\n  ## This ensures that the padded cluster num in each batch is max_num_cluster\n  cluster_non_padding_mask = [\n      torch.ones_like(\n          topic_dict['cluster_text_features'][Ellipsis, 0].squeeze(-1),\n          dtype=torch.bool).view(-1) for topic_dict in topic_dicts\n  ]\n  cluster_non_padding_mask[0] = torch.cat("
        },
        {
            "comment": "This code segment is part of a dataset class called TimelineDataset. It pads sequences, creates masks for non-padding clusters, and combines the data into a batch for training. The dataset is partitioned based on 'train', 'val', or 'test' labels. Feature_key refers to the key used to fetch features from the input, and feature_dim is the dimension of the extracted features.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/dataset.py\":79-106",
            "content": "      (cluster_non_padding_mask[0],\n       torch.zeros(\n           max_num_cluster - cluster_non_padding_mask[0].shape[0],\n           dtype=torch.bool).view(-1)),\n      dim=0)\n  data_batch['cluster_non_padding_mask'] = pad_sequence(\n      cluster_non_padding_mask, batch_first=True, padding_value=0)\n  topic_dicts[0]['cluster_text_features'] = torch.cat(\n      (topic_dicts[0]['cluster_text_features'],\n       torch.zeros(\n           (max_num_cluster - topic_dicts[0]['cluster_text_features'].shape[0],\n            topic_dicts[0]['cluster_text_features'].shape[-1]),\n           dtype=torch.float)),\n      dim=0)\n  data_batch['cluster_text_features'] = pad_sequence(\n      [topic_dict['cluster_text_features'] for topic_dict in topic_dicts],\n      batch_first=True, padding_value=0)\n  return data_batch\nclass TimelineDataset(Dataset):\n  \"\"\"The timline modeling dataset.\"\"\"\n  def __init__(self,\n               partition='train',\n               feature_key='vca_video_features_pulsar_embedding',\n               feature_dim=256,"
        },
        {
            "comment": "This code defines a class with an initializer that takes data partition as input, sets up the file path for the specified partition, and loads the dataset by parsing each TFRecord file. The code checks if the features in the data sample are valid before adding it to the dataset list. This class could be used for different partitions (train, validation, test) of a dataset stored on Google Cloud Storage.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/dataset.py\":107-130",
            "content": "               data_path=None):\n    super().__init__()\n    # Data paths on google cloud storage\n    if partition == 'train':\n      path = os.path.join(data_path, 'train-*.tfrecord')\n    elif partition == 'valid':\n      path = os.path.join(data_path, 'val-*.tfrecord')\n    elif partition == 'test':\n      path = os.path.join(data_path, 'test-*.tfrecord')\n    filenames = glob.glob(path)\n    self.dataset = []\n    raw_dataset = tf.data.TFRecordDataset(filenames)\n    for raw_record in raw_dataset:\n      data = {}\n      (video_features, video_cluster_label, timeline_url,\n       cluster_text_features) = self.parse_function(raw_record, feature_key)\n      # Ignore the data sample without valid features.\n      if video_features.shape[-1] == feature_dim:\n        data['video_features'] = video_features\n        data['video_cluster_label'] = video_cluster_label\n        data['timeline_url'] = timeline_url.decode('ascii')\n        data['cluster_text_features'] = cluster_text_features\n        self.dataset.append(data)\n    if partition == 'train':"
        },
        {
            "comment": "This code asserts that the dataset length matches the specified number of samples for training, validation, and testing partitions. It defines feature descriptions for context and sequence, then parses a single sequence example using tf.io.parse_single_sequence_example. Finally, it converts parsed video features and cluster text features to PyTorch tensors.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/dataset.py\":131-153",
            "content": "      assert len(self.dataset) == NUM_OF_TRAINING_SAMPLES\n    elif partition == 'valid':\n      assert len(self.dataset) == NUM_OF_VALIDATION_SAMPLES\n    elif partition == 'test':\n      assert len(self.dataset) == NUM_OF_TEST_SAMPLES\n  def parse_function(self, raw_record, feature_key):\n    context_description = {\n        'video_to_moment': tf.io.VarLenFeature(dtype=tf.int64),\n        'webpage_url': tf.io.FixedLenFeature([], dtype=tf.string)\n    }\n    sequence_description = {\n        feature_key: tf.io.VarLenFeature(dtype=tf.float32),\n        'moment_newsembed_embedding': tf.io.VarLenFeature(dtype=tf.float32)\n    }\n    contexts, feature_lists = tf.io.parse_single_sequence_example(\n        raw_record,\n        context_features=context_description,\n        sequence_features=sequence_description)\n    video_features = torch.from_numpy(\n        tf.sparse.to_dense(feature_lists[feature_key]).numpy())\n    cluster_text_features = torch.from_numpy(\n        tf.sparse.to_dense(feature_lists['moment_newsembed_embedding']).numpy())"
        },
        {
            "comment": "This code defines a dataset class for video timeline modeling. It includes methods for loading and accessing data from the dataset, as well as an example test dataset for validating the collate_topics function. The dataset consists of video features, video cluster labels, timeline URLs, and cluster text features.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/dataset.py\":154-183",
            "content": "    video_cluster_label = torch.from_numpy(\n        tf.sparse.to_dense(contexts['video_to_moment']).numpy())\n    timeline_url = contexts['webpage_url'].numpy()\n    return (video_features, video_cluster_label,\n            timeline_url, cluster_text_features)\n  def __len__(self):\n    return len(self.dataset)\n  def __getitem__(self, index):\n    return self.dataset[index]\nclass TimelineDatasetTest(Dataset):\n  \"\"\"A random dataset used for testing the collate_topics function only.\"\"\"\n  def __init__(self):\n    super().__init__()\n    self.dataset = []\n    # We randomly generated several data with certain number of videos and\n    # clusters. The we can verify if the batched data via collate_topics\n    # function is correct or not, in the test functions.\n    for i in range(10):\n      data = {}\n      data['video_features'] = torch.randn(i + 1, 4)\n      data['video_cluster_label'] = torch.randint(0, 2, (i + 1,))\n      data['cluster_text_features'] = torch.randn(i + 5, 8)\n      self.dataset.append(data)\n  def __len__(self):"
        },
        {
            "comment": "The code defines a dataset class with a method to return the length of the dataset and an indexing method to get a specific item from the dataset.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/dataset.py\":184-187",
            "content": "    return len(self.dataset)\n  def __getitem__(self, index):\n    return self.dataset[index]"
        }
    ]
}