{
    "summary": "This code tests an Autoencoder model's image encoder and decoder outputs' dimensions using TensorFlow's tf.test.TestCase in a video setup, while defining the model for image processing tasks with given hyperparameters and shapes of input tensors.",
    "details": [
        {
            "comment": "This code file contains test cases for the video_structure.vision module using TensorFlow's tf.test.TestCase. The code sets up a hyperparameter configuration and uses it in test models. It also imports necessary libraries and defines constants such as TESTDATA_DIR.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/vision_test.py\":0-35",
            "content": "# coding=utf-8\n# Copyright 2023 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Tests for video_structure.vision.\"\"\"\nimport os\nfrom absl import flags\nfrom absl.testing import absltest\nimport tensorflow.compat.v1 as tf\nfrom video_structure import datasets\nfrom video_structure import hyperparameters\nfrom video_structure import vision\nFLAGS = flags.FLAGS\nTESTDATA_DIR = 'video_structure/testdata'\nclass VisionTest(tf.test.TestCase):\n  def setUp(self):\n    # Hyperparameter config for test models:\n    self.cfg = hyperparameters.get_config()"
        },
        {
            "comment": "This code sets up a test environment for an Autoencoder model. It defines the configuration and data shapes, gets the training dataset, and initializes the Autoencoder model instance.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/vision_test.py\":36-62",
            "content": "    self.cfg.train_dir = os.path.join(FLAGS.test_srcdir, TESTDATA_DIR)\n    self.cfg.batch_size = 4\n    self.cfg.observed_steps = 2\n    self.cfg.predicted_steps = 2\n    self.cfg.heatmap_width = 16\n    self.cfg.layers_per_scale = 1\n    self.cfg.num_keypoints = 3\n    # Shapes of test dataset:\n    self.time_steps = self.cfg.observed_steps + self.cfg.predicted_steps\n    self.data_shapes = {\n        'filename': (None, self.time_steps),\n        'frame_ind': (None, self.time_steps),\n        'image': (None, self.time_steps, 64, 64, 3),\n        'true_object_pos': (None, self.time_steps, 0, 2)}\n    super().setUp()\n  def testAutoencoderTrainingLossGoesDown(self):\n    \"\"\"Tests a minimal Keras training loop for the non-dynamic model parts.\"\"\"\n    dataset, data_shapes = datasets.get_sequence_dataset(\n        data_dir=self.cfg.train_dir,\n        file_glob='acrobot*',\n        batch_size=self.cfg.batch_size,\n        num_timesteps=self.cfg.observed_steps + self.cfg.predicted_steps,\n        random_offset=True)\n    autoencoder = Autoencoder(self.cfg, data_shapes)"
        },
        {
            "comment": "This code snippet contains three different test cases: 1) Training an autoencoder using Adam optimizer, 2) Testing the shapes of output from the \"images_to_keypoints_net\" model, and 3) Testing the shapes of output from the \"keypoints_to_images_net\" model. It ensures that these models are producing outputs with correct dimensions.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/vision_test.py\":63-85",
            "content": "    optimizer = tf.keras.optimizers.Adam(lr=1e-4)\n    autoencoder.compile(optimizer)\n    history = autoencoder.fit(dataset, steps_per_epoch=1, epochs=3)\n    self.assertLess(history.history['loss'][-1], history.history['loss'][0])\n  def testImagesToKeypointsNetShapes(self):\n    model = vision.build_images_to_keypoints_net(\n        self.cfg, self.data_shapes['image'][1:])\n    images = tf.zeros((self.cfg.batch_size,) + self.data_shapes['image'][1:])\n    keypoints, heatmaps = model(images)\n    self.assertEqual(\n        keypoints.shape.as_list(),\n        [self.cfg.batch_size, self.time_steps, self.cfg.num_keypoints, 3])\n    self.assertEqual(\n        heatmaps.shape.as_list(),\n        [self.cfg.batch_size, self.time_steps, self.cfg.heatmap_width,\n         self.cfg.heatmap_width, 3])\n  def testKeypointsToImagesNetShapes(self):\n    model = vision.build_keypoints_to_images_net(\n        self.cfg, self.data_shapes['image'][1:])\n    keypoints = tf.zeros(\n        (self.cfg.batch_size, self.time_steps, self.cfg.num_keypoints, 3))"
        },
        {
            "comment": "The code is testing the shapes of encoded and decoded images in a video structure model. It builds an image encoder, generates encoded images, checks their shape against expected values, and builds an image decoder, generates reconstructed images, and verifies their shape. This ensures that the model's output conforms to its expected dimensions.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/vision_test.py\":86-108",
            "content": "    first_frame = tf.zeros(\n        (self.cfg.batch_size,) + self.data_shapes['image'][2:])\n    reconstructed_images = model([keypoints, first_frame, keypoints[:, 0, Ellipsis]])\n    self.assertEqual(\n        reconstructed_images.shape.as_list(),\n        [self.cfg.batch_size] + list(self.data_shapes['image'][1:]))\n  def testImageEncoderShapes(self):\n    model = vision.build_image_encoder(\n        self.data_shapes['image'][2:], **self.cfg.conv_layer_kwargs)\n    images = tf.zeros((self.cfg.batch_size,) + self.data_shapes['image'][2:])\n    encoded = model(images)\n    self.assertEqual(\n        encoded.shape.as_list()[:-1],\n        [self.cfg.batch_size, self.cfg.heatmap_width, self.cfg.heatmap_width])\n  def testImageDecoderShapes(self):\n    features_shape = [\n        self.cfg.batch_size, self.cfg.heatmap_width, self.cfg.heatmap_width, 64]\n    image_width = self.data_shapes['image'][-2]\n    model = vision.build_image_decoder(\n        features_shape[1:], output_width=image_width,\n        **self.cfg.conv_layer_kwargs)"
        },
        {
            "comment": "The code defines an Autoencoder model for image processing tasks. It takes a standard [batch_size, timesteps, H, W, C] image sequence as input, detects keypoints, and reconstructs the images without considering dynamics. The model architecture involves observing all frames, detecting keypoints, and reconstructing images. The Autoencoder is constructed with given hyperparameters and shapes of model input tensors.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/vision_test.py\":109-142",
            "content": "    output = model(tf.zeros(features_shape))\n    self.assertEqual(\n        output.shape.as_list()[:-1],\n        [self.cfg.batch_size, image_width, image_width])\nclass Autoencoder(tf.keras.Model):\n  \"\"\"Simple image autoencoder without dynamics.\n  This architecture is meant for testing the image-processing submodels.\n  Model architecture:\n    image_sequence --> keypoints --> reconstructed_image_sequence\n  The model takes a standard [batch_size, timesteps, H, W, C] image sequence as\n  input. It \"observes\" all frames, detects keypoints, and reconstructs the\n  images.\n  \"\"\"\n  def __init__(self, cfg, data_shapes):\n    \"\"\"Constructs the autoencoder.\n    Args:\n      cfg: ConfigDict with model hyperparameters.\n      data_shapes: Dict of shapes of model input tensors, as returned by\n        datasets.get_sequence_dataset.\n    \"\"\"\n    input_sequence = tf.keras.Input(\n        shape=data_shapes['image'][1:],\n        name='image')\n    image_shape = data_shapes['image'][1:]\n    keypoints, _ = vision.build_images_to_keypoints_net("
        },
        {
            "comment": "This code defines an autoencoder class in the vision_test.py file, takes input sequences and reconstructs them using keypoints to build a network, calculates loss using l2 norm, and initializes the class with specified inputs and outputs.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/vision_test.py\":143-158",
            "content": "        cfg, image_shape)(input_sequence)\n    reconstructed_sequence = vision.build_keypoints_to_images_net(\n        cfg, image_shape)([\n            keypoints,\n            input_sequence[:, 0, Ellipsis],\n            keypoints[:, 0, Ellipsis]])\n    super(Autoencoder, self).__init__(\n        inputs=input_sequence, outputs=reconstructed_sequence,\n        name='autoencoder')\n    self.add_loss(tf.nn.l2_loss(input_sequence - reconstructed_sequence))\nif __name__ == '__main__':\n  absltest.main()"
        }
    ]
}