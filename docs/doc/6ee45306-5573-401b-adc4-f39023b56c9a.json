{
    "summary": "This code introduces a video timeline model class with forward pass functionality, computing cluster and video encodings using attention-based computations and adjustable classifiers.",
    "details": [
        {
            "comment": "This code defines the main model for video timeline modeling, using a TimelineModel class. The class inherits from nn.Module and has an __init__ method with parameters for maximum number of clusters and videos. It also includes methods for AttentionHead and Encoder, and utilizes PositionalEncoding.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/model/model.py\":0-31",
            "content": "# coding=utf-8\n# Copyright 2023 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"The main model for video timeline modeling.\"\"\"\nfrom typing import Optional, Dict, Tuple\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom vtm.model.attention_head import AttentionHead\nfrom vtm.model.encoder import Encoder\nfrom vtm.model.encoder import PositionalEncoding\nclass TimelineModel(nn.Module):\n  \"\"\"Timeline model.\"\"\"\n  def __init__(self,\n               max_num_cluster,\n               max_num_video,"
        },
        {
            "comment": "This function initializes the model with various parameters. It sets `max_num_cluster`, `video_pe`, `semantics_aware_head`, `semantics_aware_head_pos`, `remove_video_and_cluster_encoders`, and `text_embedding_as_input`. The function initializes the cluster embedding matrix, video transform layer, and creates a video encoder.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/model/model.py\":32-55",
            "content": "               num_emb,\n               num_input_hidden_video,\n               num_hidden,\n               num_head,\n               num_layers,\n               video_pe=False,\n               dropout=0.1,\n               semantics_aware_head=False,\n               semantics_aware_head_pos='pos1',\n               remove_video_and_cluster_encoders=False,\n               text_embedding_as_input=False,\n               semantics_num_emb=256):\n    super().__init__()\n    self.max_num_cluster = max_num_cluster\n    self.video_pe = video_pe\n    self.semantics_aware_head = semantics_aware_head\n    self.semantics_aware_head_pos = semantics_aware_head_pos\n    self.remove_video_and_cluster_encoders = remove_video_and_cluster_encoders\n    self.text_embedding_as_input = text_embedding_as_input\n    self.cluster_emb = nn.Parameter(\n        nn.init.xavier_uniform_(torch.empty(max_num_cluster, num_emb)))\n    self.video_transform = nn.Linear(num_input_hidden_video, num_emb)\n    self.cluster_video_encoder = Encoder(num_emb, num_hidden, num_head,"
        },
        {
            "comment": "This code defines a model for video timeline modeling. It initializes Encoder and PositionalEncoding objects, and includes a forward pass function for the model. The model takes in batched data with keys like 'video_features', 'cluster_text_features', 'video_padding_mask'. This model is used to process video features and cluster text features, while also encoding positional information using positional encodings.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/model/model.py\":56-78",
            "content": "                                         num_layers, dropout)\n    if not self.remove_video_and_cluster_encoders:\n      self.cluster_encoder = Encoder(num_emb, num_hidden, num_head, num_layers,\n                                     dropout)\n      self.video_encoder = Encoder(num_emb, num_hidden, num_head, num_layers,\n                                   dropout)\n    if self.video_pe:\n      self.pe_video = PositionalEncoding(num_emb, max_num_video)\n    self.pe_cluster = PositionalEncoding(num_emb, max_num_cluster)\n    self.attention_head = AttentionHead(num_emb)\n  def forward(\n      self, data_batch\n  ):\n    \"\"\"Forward pass.\n    Args:\n      data_batch: input batched data, which is a dict with keys\n        'video_features', 'cluster_text_features', 'video_cluster_label',\n        'video_padding_mask', and 'cluster_non_padding_mask'. Each value is a\n        tensor. The first dimension of each value is batch_size.\n        'video_features': (batch_size, max_num_video_in_the_batch, feature_dim)\n        'video_padding_mask': (batch_size, max_num_video_in_the_batch)"
        },
        {
            "comment": "This function takes in batch video features, video padding mask, and cluster text features to produce normalized attention scores, intermediate cluster representations, and intermediate video representations if applicable. It applies transformations, optional positional encoding, and concatenates cluster and video inputs before producing the outputs.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/model/model.py\":79-101",
            "content": "        'cluster_text_features': (batch_size, max_num_clusters). Note that\n        max_num_clusters is 24, not the maximum number of clusters in the batch.\n        'cluster_non_padding_mask': (batch_size, max_num_clusters)\n        'video_cluster_label': (batch_size, max_num_video_in_the_batch)\n    Returns:\n      (1) The normalized attention scores (log_softmax) with shape (B,\n      max_num_video_in_batch, max_num_cluster).\n      (2) The intermediate cluster representations.\n      (3) The intermediate video representations, if applicable. Otherwise,\n      None.\n    \"\"\"\n    batch_video_x = data_batch['video_features']\n    batch_video_padding_mask = data_batch['video_padding_mask']\n    video_x = self.video_transform(batch_video_x)\n    if self.video_pe:\n      video_x = self.pe_video(video_x)\n    if self.text_embedding_as_input:\n      cluster_x = self.cluster_emb + data_batch['cluster_text_features']\n    else:\n      cluster_x = self.cluster_emb\n    # (B, max_num_cluster+max_num_video_in_batch, num_emb)\n    cluster_video_x = torch.cat("
        },
        {
            "comment": "This code is part of a video timeline modeling model. It calculates cluster and video encodings, and then performs attention-based computations to derive log_score based on the cluster and video encodings. The remove_video_and_cluster_encoders flag determines whether to use just cluster encodings or both cluster and video encodings for these calculations.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/model/model.py\":102-124",
            "content": "        (self.pe_cluster(cluster_x.expand(video_x.shape[0], -1, -1)), video_x),\n        dim=1)\n    # (B, max_num_cluster+max_num_video_in_batch, num_emb)\n    cluster_video_h = self.cluster_video_encoder(\n        cluster_video_x,\n        torch.cat((torch.zeros(\n            (video_x.shape[0], self.max_num_cluster),\n            dtype=batch_video_padding_mask.dtype).to(\n                batch_video_padding_mask.device), batch_video_padding_mask),\n                  dim=-1))\n    if self.remove_video_and_cluster_encoders:\n      log_score = self.attention_head(\n          cluster_video_h[:, self.max_num_cluster:, :],\n          cluster_video_h[:, 0:self.max_num_cluster, :])\n    else:\n      # (B, max_num_cluster, num_emb)\n      cluster_h = self.cluster_encoder(\n          cluster_video_h[:, 0:self.max_num_cluster, :])\n      # (B, max_num_video_in_batch, num_emb)\n      video_h = self.video_encoder(cluster_video_h[:, self.max_num_cluster:, :],\n                                   batch_video_padding_mask)\n      # (B, max_num_video_in_batch, max_num_cluster)"
        },
        {
            "comment": "The code defines a classifier model that takes in parameters like max_num_cluster, max_num_video, num_emb, etc. It has an attention head and an optional semantics-aware head at pos 1 or 2. The function returns log score, cluster_semantics_h or cluster_intermediate_h, and video_intermediate_h depending on whether the model is semantics aware or not.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/model/model.py\":125-153",
            "content": "      log_score = self.attention_head(video_h, cluster_h)\n    # Semantics-aware head at pos 1 or 2\n    if self.semantics_aware_head:\n      if self.semantics_aware_head_pos == 'pos1':\n        cluster_semantics_h = cluster_video_h[:, 0:self.max_num_cluster, :]\n      elif self.semantics_aware_head_pos == 'pos2':\n        cluster_semantics_h = cluster_h\n      return log_score, cluster_semantics_h, None\n    else:\n      cluster_intermediate_h = cluster_video_h[:, 0:self.max_num_cluster, :]\n      video_intermediate_h = cluster_video_h[:, self.max_num_cluster:, :]\n      return log_score, cluster_intermediate_h, video_intermediate_h\nclass ClassifierModel(nn.Module):\n  \"\"\"The baseline classifier model.\"\"\"\n  def __init__(self,\n               max_num_cluster,\n               max_num_video,\n               num_emb,\n               num_input_hidden_video,\n               num_hidden,\n               num_head,\n               num_layers,\n               video_pe=False,\n               dropout=0.1):\n    super().__init__()\n    self.max_num_cluster = max_num_cluster"
        },
        {
            "comment": "This code initializes the video module of a model, including a positional encoding if specified. It also defines a forward pass that takes in batched data and performs operations on 'video_features', 'cluster_text_features', 'video_padding_mask', and 'cluster_non_padding_mask'. The output of the forward pass is not explicitly mentioned, but it likely involves processing these inputs using linear layers, an encoder, and positional encoding if specified.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/model/model.py\":154-177",
            "content": "    self.video_pe = video_pe\n    self.video_transform = nn.Linear(num_input_hidden_video, num_emb)\n    self.video_encoder = Encoder(num_emb, num_hidden, num_head, num_layers,\n                                 dropout)\n    if self.video_pe:\n      self.pe_video = PositionalEncoding(num_emb, max_num_video)\n    self.head = nn.Linear(num_emb, max_num_cluster)\n  def forward(\n      self, data_batch\n  ):\n    \"\"\"Forward pass.\n    Args:\n      data_batch: input batched data, which is a dict with keys\n        'video_features', 'cluster_text_features', 'video_cluster_label',\n        'video_padding_mask', and 'cluster_non_padding_mask'. Each value is a\n        tensor. The first dimension of each value is batch_size.\n        'video_features': (batch_size, max_num_video_in_the_batch, feature_dim)\n        'video_padding_mask': (batch_size, max_num_video_in_the_batch)\n        'cluster_text_features': (batch_size, max_num_clusters). Note that\n        max_num_clusters is 24, not the maximum number of clusters in the batch.\n        'cluster_non_padding_mask': (batch_size, max_num_clusters)"
        },
        {
            "comment": "This function returns the normalized attention scores using log_softmax, and takes in video features, video padding mask, a transform for video features (if enabled), an encoder to process video features, and a head to generate attention scores. It also includes possible positional encoding if enabled. The output is a tuple containing the log softmax scores, and None for the other two variables.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/model/model.py\":178-195",
            "content": "        'video_cluster_label': (batch_size, max_num_video_in_the_batch) In the\n        classifier model, we do not use 'cluster_text_features' and\n        'cluster_non_padding_mask'.\n    Returns:\n      The normalized attention scores (log_softmax) with shape (B,\n      max_num_video_in_batch, max_num_cluster).\n    \"\"\"\n    batch_video_x = data_batch['video_features']\n    batch_video_padding_mask = data_batch['video_padding_mask']\n    video_x = self.video_transform(batch_video_x)\n    if self.video_pe:\n      video_x = self.pe_video(video_x)\n    # (B, max_num_video_in_batch, num_emb)\n    video_h = self.video_encoder(video_x, batch_video_padding_mask)\n    # (B, max_num_video_in_batch, max_num_cluster)\n    scores = self.head(video_h)\n    return F.log_softmax(scores, dim=-1), None, None"
        }
    ]
}