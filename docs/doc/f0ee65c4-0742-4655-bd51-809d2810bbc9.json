{
    "summary": "The script compares cluster assignments in video timeline modeling, calculating accuracy and metrics like Levenshtein distance, edit distance, and clustering quality. It also suggests improvements for temporal order in videos and outputs results in JSON format.",
    "details": [
        {
            "comment": "This code is an evaluation script for video timeline modeling. It calculates various metrics including video-to-cluster prediction accuracy, Levenshtein distance, edit distance, relative order prediction accuracy and clustering quality metrics using a given input file in the specified format.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/eval.py\":0-29",
            "content": "# coding=utf-8\n# Copyright 2023 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Evaluation script for video timeline modeling.\nThis script takes a single json file containing groundtruth and predictions, and\ncalculates the pre-defined evaluation metrics including:\n  - Video-to-cluster prediction accuracy\n  - Levenshtein distance\n  - Edit distance\n  - Relative order prediction accuracy\n  - Clustering quality metrics\nSample input format:\n  [\n    {\n      \"timeline_url\": \"foo\",\n      \"label\": [0, 0, 1, 2, 3, 3],"
        },
        {
            "comment": "The code snippet defines flags for input and output paths, as well as various evaluation metrics. It uses TensorFlow, tqdm (progress bar), and several scikit-learn cluster metrics for evaluating a video timeline model's performance. The provided function reads JSON files containing prediction and ground truth data to compute these metrics and save the results in an output file.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/eval.py\":30-66",
            "content": "      \"pred\": [0, 1, 1, 2, 3, 4]\n    },\n    ...\n  ]\nSample usage:\n  python3 -m vtm.eval --input_path=<input_path> --output_path=<output_path>\n\"\"\"\nfrom collections.abc import Sequence\nimport json\nfrom typing import Union\nfrom absl import app\nfrom absl import flags\nimport Levenshtein\nfrom sklearn.metrics.cluster import adjusted_rand_score\nfrom sklearn.metrics.cluster import completeness_score\nfrom sklearn.metrics.cluster import homogeneity_score\nfrom sklearn.metrics.cluster import v_measure_score\nimport tensorflow as tf\nimport tqdm\n_INPUT_PATH = flags.DEFINE_string(\n    'input_path',\n    None,\n    'File pattern to the prediction and groundtruth json file.',\n    required=True)\n_OUTPUT_PATH = flags.DEFINE_string('output_path', None,\n                                   'Path for saving the evaluation result.')\n_TIMELINE_TOTAL = 'timeline_total'\n_CLASSIFICATION_ACCURACY = 'classification_accuracy'\n_CLASSIFICATION_CORRECT = 'classification_correct'\n_CLASSIFICATION_TOTAL = 'classification_total'\n_ORDER_PAIRS_ACCURACY = 'order_pairs_accuracy'"
        },
        {
            "comment": "The code calculates the pairwise order comparison between elements within a given cluster assignment, generating a list of relative orders. The generated list is useful in evaluating the algorithm's performance with respect to temporal order. If there's only one element in the assignment, the order will be empty.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/eval.py\":67-94",
            "content": "_ORDER_PAIRS_CORRECT = 'order_pairs_correct'\n_ORDER_PAIRS_TOTAL = 'order_pairs_total'\n_HOMOGENEITY_SCORE = 'homogeneity_score'\n_COMPLETENESS_SCORE = 'completeness_score'\n_ADJUSTED_RAND_SCORE = 'adjusted_rand_score'\n_V_MEASURE_SCORE = 'v_measure_score'\n_NORMALIZED_LEVENSHTEIN_DISTANCE = 'normalized_levenshtein_distance'\n_NORMALIZED_CUSTOM_EDIT_DISTANCE = 'normalized_custom_edit_distance'\n_NORMALIZED_CUSTOM_EDIT_DISTANCE_SUM = 'normalized_custom_edit_distance_sum'\ndef _generate_order_pairs(assignments):\n  \"\"\"Generates a list of relative orders given cluster assignment.\n  Assume given assignment is [0, 2, 1, 1], then this function will generate a\n  list of pairwise order comparison, with length of (N * (N-1) // 2).\n    - 0 < 2: -1\n    - 0 < 1: -1\n    - 0 < 1: -1\n    - 2 > 1:  1\n    - 2 > 1:  1\n    - 1 = 1:  0\n  The generated sequence will be [-1, -1, -1, 1, 1, 0].\n  If there is only one element in the assignment then the order will be empty.\n  This aims at evaluating the algorithm with respect to the temporal orders."
        },
        {
            "comment": "The code discusses an evaluation metric for timeline modeling in videos. It defines three relations between arbitrary videos A and B, assigns them integer values (-1, 0, or 1), and applies equal penalties to misclassifications. Future improvements may consider different weights for these misclassifications.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/eval.py\":95-112",
            "content": "  Specifically, since there might be multiple \"reasonable\" timelines but only\n  a few groundtruth annotations, it is possible that number of nodes in the\n  timeline may differ from groundtruth and predictions. However, any reasonable\n  timelines should be consistent in the temporal order. We take this intuition\n  as an additional evaluation metric.\n  At this moment we define three relations regarding two arbitrary videos A & B:\n    (1) video A belongs to a cluster that is before video B (-1);\n    (2) video A and B belong to an identical cluster in timeline (0);\n    (3) video A belongs to a cluster that is after video B (1).\n  Currently we just apply equal penalty to all misclassifications, however,\n  we may consider using different weights to penalize misclassifications\n  differently. For example, misclassifying -1 to 0 might be less severe than\n  misclassifying -1 to 1, as the former may follow a different granularity on\n  clusters but the latter totally reversed temporal order.\n  Args:\n    assignments: list of integers representing video assignments."
        },
        {
            "comment": "This function evaluates a timeline by calculating video-to-cluster classification metrics and clustering quality metrics. It compares ground truth assignments to predicted assignments, counting correct matches and total assignments for accuracy calculation. It also calculates the homogeneity score between true and predicted labels. The results are stored in a dictionary with predefined keys for easy access.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/eval.py\":114-147",
            "content": "  Returns:\n    List of integers of relative order comparison.\n  \"\"\"\n  pair_list = []\n  for i, x in enumerate(assignments):\n    for y in assignments[i + 1:]:\n      if x == y:\n        pair_list.append(0)  # Both videos belong to the same cluster.\n      elif x < y:\n        pair_list.append(-1)  # First video happens before second video.\n      elif x > y:\n        pair_list.append(1)  # First video happens after second video.\n  return pair_list\ndef evaluate_timeline(groundtruths,\n                      predictions):\n  \"\"\"Evaluates a single timeline.\"\"\"\n  output = {}\n  # Video-to-cluster classification metrics.\n  classification_correct = sum(\n      p == g for p, g in zip(predictions, groundtruths))\n  classification_total = len(predictions)\n  output.update({\n      _CLASSIFICATION_TOTAL: classification_total,\n      _CLASSIFICATION_CORRECT: classification_correct,\n      _CLASSIFICATION_ACCURACY: classification_correct / classification_total,\n  })\n  # Clustering quality metrics.\n  output.update({\n      _HOMOGENEITY_SCORE:\n          homogeneity_score(labels_true=groundtruths, labels_pred=predictions),"
        },
        {
            "comment": "Calculates several metrics for comparison between ground truth and predictions: Completeness score, Adjusted Rand score, V-measure score, normalized Levenshtein distance, normalized custom edit distance sum. Also calculates relative order classification metrics using prediction and groundtruth order pairs.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/eval.py\":148-174",
            "content": "      _COMPLETENESS_SCORE:\n          completeness_score(labels_true=groundtruths, labels_pred=predictions),\n      _ADJUSTED_RAND_SCORE:\n          adjusted_rand_score(\n              labels_true=groundtruths, labels_pred=predictions),\n      _V_MEASURE_SCORE:\n          v_measure_score(labels_true=groundtruths, labels_pred=predictions)\n  })\n  # Edit distance metrics.\n  levenshtein = Levenshtein.distance(''.join(\n      chr(int(x)) for x in groundtruths), ''.join(\n          chr(int(x)) for x in predictions)) / len(groundtruths)\n  custom_edit_distance_sum = sum(\n      abs(x - y) / len(groundtruths) for x, y in zip(groundtruths, predictions))\n  output.update({\n      _NORMALIZED_LEVENSHTEIN_DISTANCE:\n          levenshtein,\n      _NORMALIZED_CUSTOM_EDIT_DISTANCE_SUM:\n          custom_edit_distance_sum,\n      _NORMALIZED_CUSTOM_EDIT_DISTANCE:\n          custom_edit_distance_sum / len(groundtruths),\n  })\n  # Relative order classification metrics.\n  prediction_order_pairs = _generate_order_pairs(predictions)\n  groundtruth_order_pairs = _generate_order_pairs(groundtruths)"
        },
        {
            "comment": "This code calculates and aggregates evaluation metrics for a dataset. It uses functions to summarize individual timeline evaluation results, keeping track of input paths, classification correct and total counts, order pair correct and total counts, and overall timeline count. The function returns an output dictionary containing these metrics.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/eval.py\":176-207",
            "content": "  order_pairs_correct = sum(\n      p == g for (p, g) in zip(prediction_order_pairs, groundtruth_order_pairs))\n  order_pairs_total = len(groundtruth_order_pairs)\n  output.update({\n      _ORDER_PAIRS_CORRECT: order_pairs_correct,\n      _ORDER_PAIRS_TOTAL: order_pairs_total,\n      _ORDER_PAIRS_ACCURACY: order_pairs_correct / order_pairs_total,\n  })\n  return output\ndef calculate_dataset_metric(\n    dataset_summary\n):\n  \"\"\"Aggregates individual timeline eval results to obtain dataset metric.\"\"\"\n  output = {'inputs': _INPUT_PATH.value}\n  def _aggregate_stats(target, stats):\n    return sum((x[target][stats] for x in dataset_summary))\n  def _summarize_dataset(target):\n    counters = {\n        _TIMELINE_TOTAL:\n            len(dataset_summary),\n        _CLASSIFICATION_CORRECT:\n            _aggregate_stats(target, _CLASSIFICATION_CORRECT),\n        _CLASSIFICATION_TOTAL:\n            _aggregate_stats(target, _CLASSIFICATION_TOTAL),\n        _ORDER_PAIRS_CORRECT:\n            _aggregate_stats(target, _ORDER_PAIRS_CORRECT),\n        _ORDER_PAIRS_TOTAL:"
        },
        {
            "comment": "This code calculates statistics for various metrics related to video timeline modeling. It divides the aggregated stats of each metric by the total number of timelines to get average scores for each metric.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/eval.py\":208-230",
            "content": "            _aggregate_stats(target, _ORDER_PAIRS_TOTAL),\n    }\n    timeline_total = counters[_TIMELINE_TOTAL]\n    timeline_level_average = {\n        _CLASSIFICATION_ACCURACY:\n            _aggregate_stats(target, _CLASSIFICATION_ACCURACY) / timeline_total,\n        _HOMOGENEITY_SCORE:\n            _aggregate_stats(target, _HOMOGENEITY_SCORE) / timeline_total,\n        _COMPLETENESS_SCORE:\n            _aggregate_stats(target, _COMPLETENESS_SCORE) / timeline_total,\n        _ADJUSTED_RAND_SCORE:\n            _aggregate_stats(target, _ADJUSTED_RAND_SCORE) / timeline_total,\n        _V_MEASURE_SCORE:\n            _aggregate_stats(target, _V_MEASURE_SCORE) / timeline_total,\n        _NORMALIZED_LEVENSHTEIN_DISTANCE:\n            _aggregate_stats(target, _NORMALIZED_LEVENSHTEIN_DISTANCE) /\n            timeline_total,\n        _NORMALIZED_CUSTOM_EDIT_DISTANCE:\n            _aggregate_stats(target, _NORMALIZED_CUSTOM_EDIT_DISTANCE) /\n            timeline_total,\n        _ORDER_PAIRS_ACCURACY:\n            _aggregate_stats(target, _ORDER_PAIRS_ACCURACY) / timeline_total,"
        },
        {
            "comment": "This code calculates video-level and timeline-level average metrics for classification accuracy, order pairs accuracy, and normalized custom edit distance. It returns these averages along with the counters and summarizes the dataset as raw and shrunk. The main function reads inputs from a file, iterates over each item, and adds the predictions to the dataset summary.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/eval.py\":231-266",
            "content": "    }\n    video_level_average = {\n        _CLASSIFICATION_ACCURACY:\n            counters[_CLASSIFICATION_CORRECT] / counters[_CLASSIFICATION_TOTAL],\n        _ORDER_PAIRS_ACCURACY:\n            counters[_ORDER_PAIRS_CORRECT] / counters[_ORDER_PAIRS_TOTAL],\n        _NORMALIZED_CUSTOM_EDIT_DISTANCE:\n            _aggregate_stats(target, 'normalized_custom_edit_distance_sum') /\n            counters[_CLASSIFICATION_TOTAL],\n    }\n    return {\n        'counters': counters,\n        'timeline_level_average': timeline_level_average,\n        'video_level_average': video_level_average,\n    }\n  # Evaluation metrics with raw predictions.\n  output.update({\n      'raw': _summarize_dataset('raw'),\n      'shrunk': _summarize_dataset('shrunk'),\n  })\n  return output\ndef main(argv):\n  if len(argv) > 1:\n    raise app.UsageError(f'Too many command-line arguments: {argv[1:]}')\n  with tf.io.gfile.GFile(_INPUT_PATH.value, 'r') as fp:\n    inputs = json.load(fp)\n  dataset_summary = []\n  for item in tqdm.tqdm(inputs):\n    predictions = item['pred']"
        },
        {
            "comment": "This code segment calculates and stores timeline summaries for a video dataset. It takes ground truths and predictions, shrinks the latter using a mapping dictionary, evaluates timelines, computes a dataset metric, and outputs it to a JSON file.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/eval.py\":267-286",
            "content": "    groundtruths = item['label']\n    shrink_mapping = {x: i for i, x in enumerate(sorted(set(predictions)))}\n    shrunk_predictions = [shrink_mapping[x] for x in predictions]\n    timeline_summary = {\n        'raw': evaluate_timeline(groundtruths, predictions),\n        'shrunk': evaluate_timeline(groundtruths, shrunk_predictions),\n    }\n    dataset_summary.append(timeline_summary)\n  dataset_metric = calculate_dataset_metric(dataset_summary)\n  with tf.io.gfile.GFile(_OUTPUT_PATH.value, 'w') as fp:\n    json.dump(dataset_metric, fp, indent=2)\nif __name__ == '__main__':\n  app.run(main)"
        }
    ]
}