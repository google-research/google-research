{
    "summary": "PyTorch code trains video timeline model using Transformer encoders, performs distillation, evaluates accuracy, logs data on TensorBoard, and saves best state.",
    "details": [
        {
            "comment": "This code is the entrypoint for training and testing a video timeline modeling model using PyTorch. It imports necessary libraries, handles arguments, and sets up the environment to run on XCloud with GPUs.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/main.py\":0-32",
            "content": "# coding=utf-8\n# Copyright 2023 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"ENTRYPOINT for training and testing the model for video timeline modeling.\nThis code is PyTorch-based, which aims to run on XCloud with GPUs.\n\"\"\"\nimport json\nimport os\nfrom typing import List, Tuple\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torch.utils import tensorboard\nfrom torch.utils.data import DataLoader"
        },
        {
            "comment": "This code defines several flags for dataset path, checkpoint directory, tensorboard directory, trained model path, maximum number of clusters, and maximum number of videos. These flags control the training process and inference using a pre-trained model for video timeline modeling.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/main.py\":33-54",
            "content": "from vtm.dataset import collate_topics\nfrom vtm.dataset import TimelineDataset\nfrom vtm.model.model import ClassifierModel\nfrom vtm.model.model import TimelineModel\n_FAIL_ON_CPU = flags.DEFINE_boolean('fail_on_cpu', False,\n                                    'fail if not run on GPU')\n_DATA_PATH = flags.DEFINE_string('data_path', None, 'The dataset path.')\n_CHECKPOINT_DIR = flags.DEFINE_string('checkpoint_dir', None,\n                                      'Directory for saving checkpoints.')\n_TENSORBOARD_DIR = flags.DEFINE_string(\n    'tensorboard_dir', None, 'Directory for saving tensorboard events.')\n_TRAINED_MODEL_PATH = flags.DEFINE_string(\n    'trained_model_path', None,\n    'If given, only run inference using the trained model.')\n# Problem hyperparas\n_MAX_NUM_CLUSTER = flags.DEFINE_integer('max_num_cluster', 24,\n                                        'max number of clusters')\n_MAX_NUM_VIDEO = flags.DEFINE_integer('max_num_video', 120,\n                                      'max number of videos')\n_VIDEO_FEATURE = flags.DEFINE_string('video_feature',"
        },
        {
            "comment": "This code segment is from the \"google-research/video_timeline_modeling/vtm/main.py\" file and contains various flags that control different aspects of the model. The 'vca_video_features_pulsar_embedding' flag defines the video input feature, while 'offline_distillation', 'online_distillation', and 'feature_distillation' are boolean flags that enable or disable distillation methods. The 'trained_teacher_model_path' flag specifies the pre-trained teacher model path used for distillation, and the 'run_baseline' flag determines whether to run the baseline model or the proposed model.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/main.py\":55-76",
            "content": "                                     'vca_video_features_pulsar_embedding',\n                                     'The used video input feature.')\n_OFFLINE_DISTILLATION = flags.DEFINE_boolean(\n    'offline_distillation', False,\n    ('Apply knowledge distillation if True.'\n     'The teacher model is the model with text embeddings as input.'))\n_TRAINED_TEACHER_MODEL_PATH = flags.DEFINE_string(\n    'trained_teacher_model_path', None,\n    'The pretrained teacher model path, used for distillation.')\n_ONLINE_DISTILLATION = flags.DEFINE_boolean(\n    'online_distillation', False,\n    ('Apply online knowledge distillation if True.'\n     'The teacher model is the model with text embeddings as input.'))\n_FEATURE_DISTILLATION = flags.DEFINE_boolean(\n    'feature_distillation', False,\n    ('Distill the intermediate features if True.'\n     'The teacher model is the model with text embeddings as input.'))\n# Model hyperparas\n_RUN_BASELINE = flags.DEFINE_boolean(\n    'run_baseline', False,\n    'Run the baseline model if True; otherwise run our model.')"
        },
        {
            "comment": "This code is using flags to define various settings for the video timeline modeling. These include whether to remove video and cluster encoders, use a semantics-aware head, apply contrastive loss, set temperature value, position of semantics-aware head, whether to include text embeddings as input, number of hidden dimensions for learnable cluster embeddings, and the number of input hidden video. Each flag is defined with default values that can be changed if needed.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/main.py\":77-97",
            "content": "_REMOVE_VIDEO_AND_CLUSTER_ENCODERS = flags.DEFINE_boolean(\n    'remove_video_and_cluster_encoders', False,\n    'Remove video and cluster corresponding encoders if True.')\n_SEMANTICS_AWARE_HEAD = flags.DEFINE_boolean(\n    'semantics_aware_head', False, 'Add the semantics-aware head if True.')\n_CONTRASTIVE_LOSS = flags.DEFINE_boolean(\n    'contrastive_loss', False,\n    ('Use contrastive loss for the semantics-aware head if True.'\n     'Otherwise, use the consine similarity loss.'))\n_TEMPERATURE = flags.DEFINE_float(\n    'temperature', 0.07, 'Temperature value used for contrastive loss.')\n_SEMANTICS_AWARE_HEAD_POS = flags.DEFINE_enum(\n    'semantics_aware_head_pos', 'pos1', ['pos1', 'pos2'],\n    'The position to place the semantics-aware head.')\n_TEXT_EMBEDDING_AS_INPUT = flags.DEFINE_boolean(\n    'text_embedding_as_input', False,\n    'Include the text embeddings as input if True.')\n_NUM_EMB = flags.DEFINE_integer(\n    'num_emb', 256,\n    'number of hidden dimensions for learnable cluster embeddings')\n_NUM_INPUT_HIDDEN_VIDEO = flags.DEFINE_integer("
        },
        {
            "comment": "This code defines various flags for hyperparameters in the Transformer encoders of a video timeline modeling algorithm. The parameters include number of hidden dimensions, attention heads, layers, dropout rate, and loss weights for different components. These flags can be used to customize the model's architecture and training process.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/main.py\":98-116",
            "content": "    'num_input_hidden_video', 256,\n    'number of hidden dimensions for input video embeddings')\n_NUM_HIDDEN = flags.DEFINE_integer(\n    'num_hidden', 256, 'number of hidden dimensions in Transformer encoders')\n_NUM_HEAD = flags.DEFINE_integer(\n    'num_head', 2, 'number of attention heads in Transformer encoders')\n_NUM_LAYERS = flags.DEFINE_integer('num_layers', 1,\n                                   'number of layers in Transformer encoders')\n_VIDEO_PE = flags.DEFINE_boolean('video_pe', False,\n                                 'if apply positional encoding to videos')\n_DROPOUT = flags.DEFINE_float('dropout', 0.1,\n                              'dropout rate in Transformer encoders')\n_SEMANTICS_LOSS_WEIGHT = flags.DEFINE_float(\n    'semantics_loss_weight', 1, 'the weight for the semantics-aware head loss')\n_DISTILLATION_LOSS_WEIGHT = flags.DEFINE_float(\n    'distillation_loss_weight', 0.1, 'the weight for the distillation loss')\n_TEACHER_LOSS_WEIGHT = flags.DEFINE_float(\n    'teacher_loss_weight', 1,\n    'the weight for the teacher model loss during online distillation')"
        },
        {
            "comment": "This code defines training hyperparameters and a function to check GPU availability. The batch size, number of epochs, log step size, learning rate, and weight decay are set as flags. The check_gpu() function logs GPU information and returns 'cuda' if available, otherwise 'cpu'.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/main.py\":118-138",
            "content": "# Training hyperparas\n_BATCH_SIZE = flags.DEFINE_integer('batch_size', 16, 'batch size')\n_EPOCHS = flags.DEFINE_integer('epochs', 10, 'epochs')\n_LOG_STEPSIZE = flags.DEFINE_integer('log_stepsize', 100, 'log step size')\n_LEARNING_RATE = flags.DEFINE_float('learning_rate', 0.01, 'learning rate')\n_WEIGHT_DECAY = flags.DEFINE_float('weight_decay', 0.00001, 'weight decay')\ndef check_gpu():\n  \"\"\"Print GPU info and return 'cuda' if found, 'cpu' otherwise.\"\"\"\n  try:\n    logging.info('FLAGS.fail_on_cpu: %s', _FAIL_ON_CPU.value)\n    logging.info('torch.__version__: %s', torch.__version__)\n    logging.info('torch.cuda.device_count(): %s', torch.cuda.device_count())\n    logging.info('torch.cuda.current_device(): %s', torch.cuda.current_device())\n    logging.info('torch.cuda.get_device_name(0): %s',\n                 torch.cuda.get_device_name(0))\n    logging.info('torch.cuda.is_available(0): %s', torch.cuda.is_available())\n    if torch.cuda.is_available():\n      return 'cuda'\n  except Exception as e:  # pylint: disable=broad-except"
        },
        {
            "comment": "Code initializes datasets for training, validation, and testing using TimelineDataset with specified feature key, feature dimension, and data path. The function get_dataset() returns the three datasets. If unable to run on CPU, it logs an error and exits, otherwise falls back to CPU.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/main.py\":139-171",
            "content": "    logging.warning(e)\n  if _FAIL_ON_CPU.value:\n    logging.error('Not able to run on CPU')\n    exit(1)\n  logging.error('Falling back to CPU.')\n  return 'cpu'\n# We cannot use built-in types (tuple/list/dict) for parametric annotations (\n# supported by python >=3.9), since the xcloud pre-built PyTorch image only\n# supports up to python 3.8.\ndef get_dataset():\n  \"\"\"Initialize datasets.\"\"\"\n  train_dataset = TimelineDataset(\n      partition='train',\n      feature_key=_VIDEO_FEATURE.value,\n      feature_dim=_NUM_INPUT_HIDDEN_VIDEO.value,\n      data_path=_DATA_PATH.value)\n  valid_dataset = TimelineDataset(\n      partition='valid',\n      feature_key=_VIDEO_FEATURE.value,\n      feature_dim=_NUM_INPUT_HIDDEN_VIDEO.value,\n      data_path=_DATA_PATH.value)\n  test_dataset = TimelineDataset(\n      partition='test',\n      feature_key=_VIDEO_FEATURE.value,\n      feature_dim=_NUM_INPUT_HIDDEN_VIDEO.value,\n      data_path=_DATA_PATH.value)\n  return train_dataset, valid_dataset, test_dataset\ndef train_epoch(timeline_model,\n                device,"
        },
        {
            "comment": "This code contains a training loop for one epoch in a video timeline modeling system. It includes handling of various options such as semantics-aware head and distillation techniques. The loop iterates over the train_loader, performs forward pass through the model, computes loss based on non-padding video tokens, and updates the model using the optimizer.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/main.py\":172-202",
            "content": "                train_loader,\n                optimizer,\n                teacher_model = None):\n  \"\"\"Training loop for one epoch.\"\"\"\n  timeline_model.train()\n  train_loss = 0\n  total_video = 0\n  if _SEMANTICS_AWARE_HEAD.value:\n    train_semantics_loss = 0\n    total_clusters = 0\n  if _OFFLINE_DISTILLATION.value:\n    teacher_model.eval()\n    train_distillation_loss = 0\n    total_clusters = 0\n  if _ONLINE_DISTILLATION.value:\n    teacher_model.train()\n    train_distillation_loss = 0\n    train_teacher_model_loss = 0\n    total_clusters = 0\n  for step, data_batch in enumerate(train_loader):\n    for key in data_batch:\n      data_batch[key] = data_batch[key].to(device)\n    optimizer.zero_grad()\n    if _SEMANTICS_AWARE_HEAD.value:\n      log_score, cluster_semantics_h, _ = timeline_model(data_batch)\n    else:\n      log_score, cluster_intermediate_h, video_intermediate_h = timeline_model(\n          data_batch)\n    # Only compute loss for non-padding video tokens\n    log_score = log_score.view(-1, _MAX_NUM_CLUSTER.value)\n    video_cluster_label = data_batch['video_cluster_label'].view(-1)"
        },
        {
            "comment": "This code calculates the loss for non-padding video cluster tokens in a semantics-aware head. It uses negative cosine similarity between each token and itself to compute the semantics loss. The temperature parameter controls the scale of the cosine similarities.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/main.py\":203-224",
            "content": "    video_non_padding_mask = ~data_batch['video_padding_mask'].view(-1)\n    loss = F.nll_loss(\n        log_score[video_non_padding_mask],\n        video_cluster_label[video_non_padding_mask],\n        reduction='sum')\n    # With semantics-aware head: compute loss for non-padding cluster tokens\n    if _SEMANTICS_AWARE_HEAD.value:\n      if _CONTRASTIVE_LOSS.value:\n        # (max_num_cluster, num_emb, batch_size)\n        cluster_semantics_h = cluster_semantics_h.permute((1, 2, 0))\n        # (max_num_cluster, max_num_cluster, batch_size)\n        cosine_similarity_pairwise = F.cosine_similarity(\n            cluster_semantics_h, cluster_semantics_h.unsqueeze(1), dim=-2)\n        # (batch_size, max_num_cluster, max_num_cluster)\n        cosine_similarity_pairwise = cosine_similarity_pairwise.permute(\n            (2, 0, 1)) / _TEMPERATURE.value\n        # (batch_size, max_num_cluster)\n        self_cosine_similarity = torch.diagonal(\n            cosine_similarity_pairwise, dim1=-2, dim2=-1)\n        semantics_loss = (\n            -self_cosine_similarity[data_batch['cluster_non_padding_mask']] +"
        },
        {
            "comment": "This code calculates a loss for a video timeline modeling task. It includes two parts: one is based on pairwise cosine similarity, and the other compares cluster semantics with text features. The combined losses are then backpropagated and updated using an optimizer. Logging information about the losses at specified intervals is also performed.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/main.py\":225-246",
            "content": "            torch.logsumexp(\n                cosine_similarity_pairwise[\n                    data_batch['cluster_non_padding_mask']],\n                dim=-1)).sum()\n      else:\n        semantics_loss = (1 - F.cosine_similarity(\n            cluster_semantics_h[data_batch['cluster_non_padding_mask']],\n            data_batch['cluster_text_features'][\n                data_batch['cluster_non_padding_mask']])).sum()\n      loss_sum = loss + _SEMANTICS_LOSS_WEIGHT.value * semantics_loss\n      loss_sum.backward()\n      optimizer.step()\n      if step % _LOG_STEPSIZE.value == 0:\n        logging.info('[%s/%s] Loss: %s',\n                     step * len(data_batch['video_features']),\n                     len(train_loader.dataset),\n                     loss.item() / video_non_padding_mask.sum().item())\n        logging.info(\n            '[%s/%s] Semantics Loss: %s',\n            step * len(data_batch['video_features']), len(train_loader.dataset),\n            semantics_loss.item() /\n            data_batch['cluster_non_padding_mask'].sum().item())"
        },
        {
            "comment": "This code snippet is part of a video modeling system. It calculates losses and performs knowledge distillation for offline learning, either using feature distillation or KL divergence between logits and teacher's logits. The total video and cluster counts are also updated.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/main.py\":247-268",
            "content": "      train_semantics_loss += semantics_loss.item()\n      train_loss += loss.item()\n      total_video += video_non_padding_mask.sum().item()\n      total_clusters += data_batch['cluster_non_padding_mask'].sum().item()\n    # Offline knowledge distillation\n    elif _OFFLINE_DISTILLATION.value:\n      with torch.no_grad():\n        teacher_log_score, teacher_cluster_h, teacher_video_h = teacher_model(\n            data_batch)\n      teacher_log_score = teacher_log_score.view(-1, _MAX_NUM_CLUSTER.value)\n      if _FEATURE_DISTILLATION.value:\n        distillation_cluster_loss = torch.norm(\n            teacher_cluster_h - cluster_intermediate_h, dim=-1).sum()\n        distillation_video_loss = torch.norm(\n            teacher_video_h[~data_batch['video_padding_mask']] -\n            video_intermediate_h[~data_batch['video_padding_mask']],\n            dim=-1).sum()\n        distillation_loss = distillation_cluster_loss + distillation_video_loss\n      else:\n        distillation_loss = F.kl_div(\n            log_score[video_non_padding_mask],"
        },
        {
            "comment": "This code calculates and updates the loss for video and distillation tasks in a model training process. It also keeps track of the total losses, number of videos, and clusters throughout the training. The logging information shows the current step, total length of training data, and per-video/cluster loss values.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/main.py\":269-289",
            "content": "            teacher_log_score[video_non_padding_mask],\n            reduction='sum',\n            log_target=True)\n      loss_sum = loss + _DISTILLATION_LOSS_WEIGHT.value * distillation_loss\n      loss_sum.backward()\n      optimizer.step()\n      if step % _LOG_STEPSIZE.value == 0:\n        logging.info('[%s/%s] Loss: %s',\n                     step * len(data_batch['video_features']),\n                     len(train_loader.dataset),\n                     loss.item() / video_non_padding_mask.sum().item())\n        logging.info(\n            '[%s/%s] Distillation Loss: %s',\n            step * len(data_batch['video_features']), len(train_loader.dataset),\n            distillation_loss.item() /\n            (video_non_padding_mask.sum().item() +\n             torch.numel(data_batch['cluster_non_padding_mask'])))\n      train_distillation_loss += distillation_loss.item()\n      train_loss += loss.item()\n      total_video += video_non_padding_mask.sum().item()\n      total_clusters += data_batch['cluster_non_padding_mask'].sum().item()"
        },
        {
            "comment": "This code performs online knowledge distillation in a video timeline modeling framework. It compares teacher and student model outputs, calculates cluster and video loss components, and combines them to generate the final distillation loss. If feature distillation is enabled, it calculates the distance between teacher and student cluster and video features, otherwise it uses Kullback-Leibler (KL) divergence to measure the difference between log scores from both models. The teacher model's loss is also calculated using negative log likelihood (NLL) loss.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/main.py\":291-312",
            "content": "    # Online knowledge distillation\n    elif _ONLINE_DISTILLATION.value:\n      teacher_log_score, teacher_cluster_h, teacher_video_h = teacher_model(\n          data_batch)\n      teacher_log_score = teacher_log_score.view(-1, _MAX_NUM_CLUSTER.value)\n      if _FEATURE_DISTILLATION.value:\n        distillation_cluster_loss = torch.norm(\n            teacher_cluster_h - cluster_intermediate_h, dim=-1).sum()\n        distillation_video_loss = torch.norm(\n            teacher_video_h[~data_batch['video_padding_mask']] -\n            video_intermediate_h[~data_batch['video_padding_mask']],\n            dim=-1).sum()\n        distillation_loss = distillation_cluster_loss + distillation_video_loss\n      else:\n        distillation_loss = F.kl_div(\n            log_score[video_non_padding_mask],\n            teacher_log_score[video_non_padding_mask],\n            reduction='sum',\n            log_target=True)\n      teacher_model_loss = F.nll_loss(\n          teacher_log_score[video_non_padding_mask],\n          video_cluster_label[video_non_padding_mask],"
        },
        {
            "comment": "The code calculates the total loss, performs backward propagation, updates model weights using an optimizer, and logs the current loss, teacher model loss, and distillation loss for each iteration. It also accumulates the distillation loss in the training process.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/main.py\":313-333",
            "content": "          reduction='sum')\n      loss_sum = (loss + _DISTILLATION_LOSS_WEIGHT.value * distillation_loss\n                  + _TEACHER_LOSS_WEIGHT.value * teacher_model_loss)\n      loss_sum.backward()\n      optimizer.step()\n      if step % _LOG_STEPSIZE.value == 0:\n        logging.info('[%s/%s] Loss: %s',\n                     step * len(data_batch['video_features']),\n                     len(train_loader.dataset),\n                     loss.item() / video_non_padding_mask.sum().item())\n        logging.info(\n            '[%s/%s] Teacher Model Loss: %s',\n            step * len(data_batch['video_features']), len(train_loader.dataset),\n            teacher_model_loss.item() / video_non_padding_mask.sum().item())\n        logging.info(\n            '[%s/%s] Distillation Loss: %s',\n            step * len(data_batch['video_features']), len(train_loader.dataset),\n            distillation_loss.item() /\n            (video_non_padding_mask.sum().item() +\n             torch.numel(data_batch['cluster_non_padding_mask'])))\n      train_distillation_loss += distillation_loss.item()"
        },
        {
            "comment": "This code calculates the loss for a video timeline modeling task. It tracks the total training loss, teacher model loss, and counts the number of videos and clusters seen so far. Depending on the selected option (_SEMANTICS_AWARE_HEAD, _OFFLINE_DISTILLATION, or _ONLINE_DISTILLATION), it returns different metrics for evaluation. The metrics are calculated by dividing the respective losses by the total number of videos and clusters seen during training.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/main.py\":334-357",
            "content": "      train_loss += loss.item()\n      train_teacher_model_loss += teacher_model_loss.item()\n      total_video += video_non_padding_mask.sum().item()\n      total_clusters += data_batch['cluster_non_padding_mask'].sum().item()\n    else:\n      loss.backward()\n      optimizer.step()\n      if step % _LOG_STEPSIZE.value == 0:\n        logging.info('[%s/%s] Loss: %s',\n                     step * len(data_batch['video_features']),\n                     len(train_loader.dataset),\n                     loss.item() / video_non_padding_mask.sum().item())\n      train_loss += loss.item()\n      total_video += video_non_padding_mask.sum().item()\n  if _SEMANTICS_AWARE_HEAD.value:\n    return train_loss / total_video, train_semantics_loss / total_clusters, None\n  elif _OFFLINE_DISTILLATION.value:\n    return train_loss / total_video, train_distillation_loss / (\n        total_video + total_clusters), None\n  elif _ONLINE_DISTILLATION.value:\n    return train_loss / total_video, train_distillation_loss / (\n        total_video + total_clusters), train_teacher_model_loss / total_video,"
        },
        {
            "comment": "This code is a part of an evaluation function for the Video Timeline Modeling task. It measures the video to cluster accuracy by iterating through the data loader, computing log scores and predictions using the timeline model. It then calculates the accuracy by comparing the predicted cluster labels with the ground truth labels and divides the total correct predictions by the total videos processed. The function returns the video-to-cluster accuracy.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/main.py\":358-382",
            "content": "  else:\n    return train_loss / total_video, None, None\ndef evaluate(timeline_model, device,\n             loader):\n  \"\"\"Evaluation pipeline for measuring video to cluster accuracy (float).\"\"\"\n  timeline_model.eval()\n  video_to_cluster_correct = 0\n  total_video = 0\n  with torch.no_grad():\n    for data_batch in loader:\n      for key in [\n          'video_features', 'video_padding_mask', 'video_cluster_label'\n      ]:\n        data_batch[key] = data_batch[key].to(device)\n      log_score, _, _ = timeline_model(data_batch)\n      log_score = log_score.view(-1, _MAX_NUM_CLUSTER.value)\n      prediction = log_score.argmax(dim=1, keepdim=False)\n      video_cluster_label = data_batch['video_cluster_label'].view(-1)\n      video_non_padding_mask = ~data_batch['video_padding_mask'].view(-1)\n      video_to_cluster_correct += prediction[video_non_padding_mask].eq(\n          video_cluster_label[video_non_padding_mask]).sum().item()\n      total_video += video_non_padding_mask.sum().item()\n  video_to_cluster_accuracy = video_to_cluster_correct / total_video"
        },
        {
            "comment": "The function performs video classification inference using a pre-trained timeline model. It iterates through the dataset, one batch at a time, calculating log scores and predictions for each video. The correct cluster labels are compared to predicted labels for non-padding parts of videos, incrementing the accuracy count accordingly.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/main.py\":383-408",
            "content": "  return video_to_cluster_accuracy\ndef inference(timeline_model, device,\n              dataset):\n  \"\"\"Inference with one-by-one processing.\"\"\"\n  timeline_model.eval()\n  video_to_cluster_correct = 0\n  total_video = 0\n  final_predictions = []\n  loader = DataLoader(\n      dataset, batch_size=1, shuffle=False, collate_fn=collate_topics)\n  with torch.no_grad():\n    for i, data_batch in enumerate(loader):\n      data_prediction = {}\n      for key in [\n          'video_features', 'video_padding_mask', 'video_cluster_label'\n      ]:\n        data_batch[key] = data_batch[key].to(device)\n      log_score, _, _ = timeline_model(data_batch)\n      log_score = log_score.view(-1, _MAX_NUM_CLUSTER.value)\n      prediction = log_score.argmax(dim=1, keepdim=False)\n      video_cluster_label = data_batch['video_cluster_label'].view(-1)\n      video_non_padding_mask = ~data_batch['video_padding_mask'].view(-1)\n      video_to_cluster_correct += prediction[video_non_padding_mask].eq(\n          video_cluster_label[video_non_padding_mask]).sum().item()"
        },
        {
            "comment": "This code snippet contains functions related to model saving and prediction. The `main` function starts the job, sets device for GPU usage, and logs important parameters. The `save_model` function saves model and optimizer states as checkpoints. The code also includes a logic to calculate video-to-cluster accuracy in prediction.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/main.py\":409-437",
            "content": "      total_video += video_non_padding_mask.sum().item()\n      data_prediction['timeline_url'] = dataset[i]['timeline_url']\n      data_prediction['pred'] = prediction.tolist()\n      data_prediction['label'] = video_cluster_label.tolist()\n      final_predictions.append(data_prediction)\n  video_to_cluster_accuracy = video_to_cluster_correct / total_video\n  return final_predictions, video_to_cluster_accuracy\ndef save_model(model, optimizer, output_dir):\n  \"\"\"Save model to GCS.\"\"\"\n  os.makedirs(output_dir, exist_ok=True)\n  # Will overwrite existing previously saved model.\n  torch.save(model.state_dict(), os.path.join(output_dir, 'model.pt'))\n  torch.save(\n      {\n          'model_state_dict': model.state_dict(),\n          'optimizer_state_dict': optimizer.state_dict(),\n      }, os.path.join(output_dir, 'checkpoint.tar'))\ndef main(_):\n  logging.info('Job started')\n  device = torch.device(check_gpu())\n  torch.cuda.empty_cache()\n  logging.info('FLAGS.epochs: %s', _EPOCHS.value)\n  logging.info('FLAGS.batch_size: %s', _BATCH_SIZE.value)"
        },
        {
            "comment": "The code checks the FLAGS and determines which model to run. If running a baseline model, it initializes the ClassifierModel with specific values. Otherwise, if semantics-aware head is not enabled, it initializes TimelineModel without semantics-aware head and possibly video/cluster encoders as well. This code manages the model initialization based on different FLAGS settings.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/main.py\":438-458",
            "content": "  logging.info('FLAGS.learning_rate: %s', _LEARNING_RATE.value)\n  logging.info('FLAGS.weight_decay: %s', _WEIGHT_DECAY.value)\n  if _RUN_BASELINE.value:\n    logging.info('Running baseline model.')\n    model = ClassifierModel(_MAX_NUM_CLUSTER.value, _MAX_NUM_VIDEO.value,\n                            _NUM_EMB.value, _NUM_INPUT_HIDDEN_VIDEO.value,\n                            _NUM_HIDDEN.value, _NUM_HEAD.value,\n                            _NUM_LAYERS.value, _VIDEO_PE.value, _DROPOUT.value)\n  else:\n    if _SEMANTICS_AWARE_HEAD.value:\n      logging.info('Running our complete model.')\n    else:\n      if _REMOVE_VIDEO_AND_CLUSTER_ENCODERS.value:\n        logging.info('Running our model without the semantics-aware head.')\n        logging.info('and the video and cluster encoders.')\n      else:\n        logging.info('Running our model without the semantics-aware head.')\n    model = TimelineModel(_MAX_NUM_CLUSTER.value, _MAX_NUM_VIDEO.value,\n                          _NUM_EMB.value, _NUM_INPUT_HIDDEN_VIDEO.value,\n                          _NUM_HIDDEN.value, _NUM_HEAD.value, _NUM_LAYERS.value,"
        },
        {
            "comment": "This code is creating a teacher model for knowledge distillation using the TimelineModel class. It checks and asserts certain conditions before instantiating the model with specified parameters. If offline distillation is enabled, it loads the state dictionary of the teacher model. The model is then placed on a device (GPU) using DataParallel.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/main.py\":459-476",
            "content": "                          _VIDEO_PE.value, _DROPOUT.value,\n                          _SEMANTICS_AWARE_HEAD.value,\n                          _SEMANTICS_AWARE_HEAD_POS.value,\n                          _REMOVE_VIDEO_AND_CLUSTER_ENCODERS.value,\n                          _TEXT_EMBEDDING_AS_INPUT.value)\n    if _OFFLINE_DISTILLATION.value or _ONLINE_DISTILLATION.value:\n      assert not _SEMANTICS_AWARE_HEAD.value\n      assert not _REMOVE_VIDEO_AND_CLUSTER_ENCODERS.value\n      teacher_model = TimelineModel(\n          _MAX_NUM_CLUSTER.value, _MAX_NUM_VIDEO.value, _NUM_EMB.value,\n          _NUM_INPUT_HIDDEN_VIDEO.value, _NUM_HIDDEN.value, _NUM_HEAD.value,\n          _NUM_LAYERS.value, _VIDEO_PE.value, _DROPOUT.value,\n          _SEMANTICS_AWARE_HEAD.value, _SEMANTICS_AWARE_HEAD_POS.value,\n          _REMOVE_VIDEO_AND_CLUSTER_ENCODERS.value, True)\n      teacher_model = nn.DataParallel(teacher_model).to(device)\n      if _OFFLINE_DISTILLATION.value:\n        logging.info('Performing offline knowledge distillation.')\n        teacher_model.load_state_dict("
        },
        {
            "comment": "Code snippet checks the value of `_TRAINED_TEACHER_MODEL_PATH` and `_ONLINE_DISTILLATION`. If a trained teacher model is present, it loads the model for inference on validation and test datasets. If no model is found or online distillation is enabled, the code proceeds to train the timeline model using specified parameters. The final accuracy of both validation and test sets are logged after either training or loading a pre-trained model.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/main.py\":477-494",
            "content": "            torch.load(\n                os.path.join(_TRAINED_TEACHER_MODEL_PATH.value, 'model.pt')))\n        logging.info('Loaded pretrained teacher model.')\n      elif _ONLINE_DISTILLATION.value:\n        logging.info('Performing online knowledge distillation.')\n  train_dataset, valid_dataset, test_dataset = get_dataset()\n  trained_model_path = _TRAINED_MODEL_PATH.value\n  if trained_model_path is not None:\n    logging.info('Run inference only.')\n    timeline_model = nn.DataParallel(model).to(device)\n    timeline_model.load_state_dict(\n        torch.load(os.path.join(trained_model_path, 'model.pt')))\n    valid_predictions, final_valid_v2c_acc = inference(timeline_model, device,\n                                                       valid_dataset)\n    test_predictions, final_test_v2c_acc = inference(timeline_model, device,\n                                                     test_dataset)\n    logging.info('Final Valid Acc %s', final_valid_v2c_acc)\n    logging.info('Final Test Acc %s', final_test_v2c_acc)"
        },
        {
            "comment": "This code section saves valid and test predictions as JSON files, initializes the timeline model on a specific device, defines an optimizer based on distillation settings, possibly adds a TensorBoard writer for visualization, and creates train and valid loaders.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/main.py\":495-524",
            "content": "    with open(os.path.join(trained_model_path, 'valid_prediction.json'),\n              'w') as f:\n      json.dump(valid_predictions, f)\n    with open(os.path.join(trained_model_path, 'test_prediction.json'),\n              'w') as f:\n      json.dump(test_predictions, f)\n  else:\n    timeline_model = nn.DataParallel(model).to(device)\n    if _OFFLINE_DISTILLATION.value or _ONLINE_DISTILLATION.value:\n      optimizer = optim.Adam(\n          list(timeline_model.parameters()) + list(teacher_model.parameters()),\n          lr=_LEARNING_RATE.value,\n          weight_decay=_WEIGHT_DECAY.value)\n    else:\n      optimizer = optim.Adam(\n          timeline_model.parameters(),\n          lr=_LEARNING_RATE.value,\n          weight_decay=_WEIGHT_DECAY.value)\n    if _TENSORBOARD_DIR.value:\n      writer = tensorboard.SummaryWriter(_TENSORBOARD_DIR.value)\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=_BATCH_SIZE.value,\n        shuffle=True,\n        collate_fn=collate_topics)\n    valid_loader = DataLoader(\n        valid_dataset,"
        },
        {
            "comment": "This code is initializing training and testing datasets, setting the best validation accuracy, and iterating through each epoch of the model's training. If semantics-aware mode is enabled, it calculates training and semantics losses; if offline distillation mode is enabled, it also calculates distillation loss. Logging information about losses for each iteration.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/main.py\":525-547",
            "content": "        batch_size=_BATCH_SIZE.value,\n        shuffle=False,\n        collate_fn=collate_topics)\n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=_BATCH_SIZE.value,\n        shuffle=False,\n        collate_fn=collate_topics)\n    best_valid_v2c_acc = 0\n    for epoch in range(1, _EPOCHS.value + 1):\n      logging.info('Epoch %s of %s', epoch, _EPOCHS.value)\n      if _SEMANTICS_AWARE_HEAD.value:\n        train_loss, semantics_loss, _ = train_epoch(timeline_model, device,\n                                                    train_loader, optimizer)\n        logging.info('Loss %s', train_loss)\n        logging.info('Semantics loss %s', semantics_loss)\n      elif _OFFLINE_DISTILLATION.value:\n        train_loss, distillation_loss, _ = train_epoch(timeline_model, device,\n                                                       train_loader, optimizer,\n                                                       teacher_model)\n        logging.info('Loss %s', train_loss)\n        logging.info('Distillation loss %s', distillation_loss)"
        },
        {
            "comment": "This code checks the value of _ONLINE_DISTILLATION and then either trains the model with distillation loss or without. After training, it evaluates the accuracy on train, validation, and test datasets. If the validation accuracy is higher than the previous best, it saves the current state of the model and optimizer. The logging outputs various loss values and training accuracy.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/main.py\":548-566",
            "content": "      elif _ONLINE_DISTILLATION.value:\n        train_loss, distillation_loss, train_teacher_model_loss = train_epoch(\n            timeline_model, device, train_loader, optimizer, teacher_model)\n        logging.info('Loss %s', train_loss)\n        logging.info('Teacher Model Loss %s', train_teacher_model_loss)\n        logging.info('Distillation loss %s', distillation_loss)\n      else:\n        train_loss, _, _ = train_epoch(timeline_model, device, train_loader,\n                                       optimizer)\n        logging.info('Loss %s', train_loss)\n      train_v2c_acc = evaluate(timeline_model, device, train_loader)\n      valid_v2c_acc = evaluate(timeline_model, device, valid_loader)\n      test_v2c_acc = evaluate(timeline_model, device, test_loader)\n      if valid_v2c_acc > best_valid_v2c_acc:\n        best_valid_v2c_acc = valid_v2c_acc\n        final_test_v2c_acc = test_v2c_acc\n        if _CHECKPOINT_DIR.value:\n          save_model(timeline_model, optimizer, _CHECKPOINT_DIR.value)\n      logging.info('Training Acc %s', train_v2c_acc)"
        },
        {
            "comment": "The code logs validation and test accuracy, and adds scalar values for various losses and accuracy metrics during training to TensorBoard. It flushes the TensorBoard writer after adding all the scalars.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/main.py\":567-585",
            "content": "      logging.info('Valid Acc %s', valid_v2c_acc)\n      logging.info('Test Acc %s', test_v2c_acc)\n      logging.info('Best Valid Acc So Far %s', best_valid_v2c_acc)\n      logging.info('Final Test Acc So Far %s', final_test_v2c_acc)\n      if _TENSORBOARD_DIR.value:\n        if _SEMANTICS_AWARE_HEAD.value:\n          writer.add_scalar('Semantics Loss/train', semantics_loss, epoch)\n        if _OFFLINE_DISTILLATION.value:\n          writer.add_scalar('Distillation Loss/train', distillation_loss, epoch)\n        if _ONLINE_DISTILLATION.value:\n          writer.add_scalar('Distillation Loss/train', distillation_loss, epoch)\n          writer.add_scalar('Teacher Model Loss/train',\n                            train_teacher_model_loss, epoch)\n        writer.add_scalar('Loss/train', train_loss, epoch)\n        writer.add_scalar('Accuracy/train', train_v2c_acc, epoch)\n        writer.add_scalar('Accuracy/valid', valid_v2c_acc, epoch)\n        writer.add_scalar('Accuracy/test', test_v2c_acc, epoch)\n        logging.info('Flushing TensorBoard writer')"
        },
        {
            "comment": "Flushing the writer and closing it if a tensorboard directory is specified, logging information that the job has finished. The code seems to be part of a TensorFlow or similar program's main execution loop in a Python script.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_timeline_modeling/vtm/main.py\":586-595",
            "content": "        writer.flush()\n    if _TENSORBOARD_DIR.value:\n      writer.close()\n  logging.info('Job finished')\nif __name__ == '__main__':\n  app.run(main)"
        }
    ]
}