{
    "summary": "This code defines TensorFlow pipeline functions to load sequence datasets, generates a dataset object with various arguments, and includes utility functions for data manipulation. The main function splits sequence dictionaries into chunks while handling missing keys and returns shuffled sequence chunks as a TensorFlow dataset.",
    "details": [
        {
            "comment": "This code imports various libraries and defines functions to load sequence datasets into a TensorFlow pipeline. It takes in directory, batch size, number of timesteps, file glob pattern for the data files, and an optional parameter for randomizing the offsets of the timesteps. The dataset is returned as a TensorFlow Dataset object.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/datasets.py\":0-30",
            "content": "# coding=utf-8\n# Copyright 2023 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Load sequence datasets into tf.data.Dataset pipeline.\"\"\"\nimport functools\nimport os\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n# Data fields used by the model:\nREQUIRED_DATA_FIELDS = ['image', 'true_object_pos']\ndef get_sequence_dataset(data_dir,\n                         batch_size,\n                         num_timesteps,\n                         file_glob='*.npz',\n                         random_offset=True,"
        },
        {
            "comment": "This code generates a TensorFlow dataset from numpy image sequences in the specified directory. It takes arguments such as data_dir, batch_size, num_timesteps, file_glob, random_offset, repeat_dataset, and seed to create a tf.data.Dataset object. If no data files are found in data_dir, it raises a RuntimeError.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/datasets.py\":31-55",
            "content": "                         repeat_dataset=True,\n                         seed=0):\n  \"\"\"Returns a tf.data.Dataset object for a Numpy image sequence dataset.\n  Args:\n    data_dir: Directory containing Numpy files where each file contains an image\n      sequence and ground-truth object coordinates.\n    batch_size: Desired number of sequences per batch in the output dataset.\n    num_timesteps: Desired sequence length in the output dataset.\n    file_glob: Glob pattern to sub-select files in data_dir.\n    random_offset: If True, a random number of frames will be dropped from the\n      start of the input sequence before dividing it into chunks of length\n      num_timesteps.\n    repeat_dataset: If True, output dataset will repeat forever.\n    seed: Random seed for shuffling.\n  Returns:\n    A tf.data.Dataset, or a one-shot iterator of the dataset if return_iterator\n    is True.\n  Raises:\n    RuntimeError: If no data files are found in data_dir.\n  \"\"\"\n  # Find files for dataset. Each file contains a sequence of arbitrary length:"
        },
        {
            "comment": "This code reads data files from a specified directory, shuffles them in place, creates a TensorFlow dataset, divides sequences into chunks, interleaves the chunks to form batches, shuffles and batches the data again, then prepares it for efficient loading.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/datasets.py\":56-82",
            "content": "  file_glob = file_glob if '.npz' in file_glob else file_glob + '.npz'\n  filenames = sorted(tf.gfile.Glob(os.path.join(data_dir, file_glob)))\n  if not filenames:\n    raise RuntimeError('No data files match {}.'.format(\n        os.path.join(data_dir, file_glob)))\n  # Deterministic in-place shuffle:\n  np.random.RandomState(seed).shuffle(filenames)\n  # Create dataset:\n  dtypes, pre_chunk_shapes = _read_data_types_and_shapes(filenames)\n  dataset = tf.data.Dataset.from_generator(\n      lambda: _read_numpy_sequences(filenames), dtypes, pre_chunk_shapes)\n  if repeat_dataset:\n    dataset = dataset.repeat()\n  # Divide sequences into num_timesteps chunks:\n  chunk_fn = functools.partial(\n      _chunk_sequence, chunk_length=num_timesteps, random_offset=random_offset)\n  dataset = dataset.interleave(chunk_fn, cycle_length=batch_size)\n  # Format dataset:\n  dataset = dataset.shuffle(\n      100 * batch_size, seed=seed, reshuffle_each_iteration=True)\n  dataset = dataset.batch(batch_size, drop_remainder=True)\n  dataset = dataset.prefetch(buffer_size=None)  # None = Auto-tune"
        },
        {
            "comment": "This code defines a function that reads Numpy files containing sequence data from disk and returns the data in a dictionary format. It also handles unreadable files by skipping them for traceability. The data is formatted to suit TensorFlow requirements before being returned.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/datasets.py\":84-117",
            "content": "  # Get shapes after data formatting:\n  format_shape = lambda shape: (None,) + tuple(shape.as_list()[1:])\n  shapes = {\n      key: format_shape(shape) for key, shape in dataset.output_shapes.items()}\n  return dataset, shapes\ndef _read_numpy_sequences(filenames):\n  \"\"\"Generator that reads Numpy files with sequence data from disk into a dict.\n  Unreadable files (i.e. files that cause an IOError) will be skipped.\n  For traceability, fields containing the filename and frame number will be\n  added to the sequence dict.\n  Args:\n    filenames: List of paths to Numpy files.\n  Yields:\n    Dict containing Numpy arrays corresponding to the data for one sequence.\n  \"\"\"\n  for filename in filenames:\n    try:\n      with tf.gfile.Open(filename, 'rb') as f:\n        sequence_dict = {k: v for k, v in np.load(f).items()}\n    except IOError as e:\n      print('Caught IOError: \"{}\". Skipping file {}.'.format(e, filename))\n    # Format data:\n    sequence_dict = _choose_data_fields(sequence_dict)\n    sequence_dict = {\n        k: _adjust_precision_for_tf(v) for k, v in sequence_dict.items()}"
        },
        {
            "comment": "The code reads in a sequence of data, formats the image data, adds filename and frame index arrays for traceability. It then yields the modified sequence dictionary. The function _choose_data_fields takes a dictionary and returns a new one containing only fields required by the model. If a required key is not present, it adds dummy ground truth if the missing key is 'true_object_pos', otherwise it raises a ValueError.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/datasets.py\":118-143",
            "content": "    sequence_dict['image'] = _format_image_data(sequence_dict['image'])\n    # Add filename and frame arrays for traceability:\n    num_frames = list(sequence_dict.values())[0].shape[0]\n    sequence_dict['frame_ind'] = np.arange(num_frames, dtype=np.int32)\n    sequence_dict['filename'] = np.full(num_frames, os.path.basename(filename))\n    yield sequence_dict\ndef _choose_data_fields(data_dict):\n  \"\"\"Returns a new dict containing only fields required by the model.\"\"\"\n  output_dict = {}\n  for k in REQUIRED_DATA_FIELDS:\n    if k in data_dict:\n      output_dict[k] = data_dict[k]\n    elif k == 'true_object_pos':\n      # Create dummy ground truth if it's not in the dict:\n      tf.logging.log_first_n(tf.logging.WARN,\n                             'Found no true_object_pos in data, adding dummy.',\n                             1)\n      num_timesteps = data_dict['image'].shape[0]\n      output_dict['true_object_pos'] = np.zeros([num_timesteps, 0, 2])\n    else:\n      raise ValueError(\n          'Required key \"{}\" is not in the  dict with keys {}.'.format("
        },
        {
            "comment": "This code contains several utility functions for handling and adjusting data types, shapes, and formats. The main function is `_chunk_sequence`, which takes a dictionary of sequence data and chunks it into pieces of specified length. Other functions include `_adjust_precision_for_tf` to change data precision for TensorFlow compatibility, `_format_image_data` to format image data in the range [-0.5, 0.5], `_read_data_types_and_shapes` to read and return data types and shapes of all keys in a dataset, and `_chunk_sequence` to split sequence dictionary into chunks.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/datasets.py\":144-175",
            "content": "              k, list(data_dict.keys())))\n  return output_dict\ndef _adjust_precision_for_tf(array):\n  \"\"\"Adjusts precision for TensorFlow.\"\"\"\n  if array.dtype == np.float64:\n    return array.astype(np.float32)\n  if array.dtype == np.int64:\n    return array.astype(np.int32)\n  return array\ndef _format_image_data(image):\n  \"\"\"Formats the uint8 input image to float32 in the range [-0.5, 0.5].\"\"\"\n  if not np.issubdtype(image.dtype, np.uint8):\n    raise ValueError('Expected image to be of type {}, but got type {}.'.format(\n        np.uint8, image.dtype))\n  return image.astype(np.float32) / 255.0 - 0.5\ndef _read_data_types_and_shapes(filenames):\n  \"\"\"Gets dtypes and shapes for all keys in the dataset.\"\"\"\n  sequences = _read_numpy_sequences(filenames)\n  sequence = next(sequences)\n  sequences.close()\n  dtypes = {k: tf.as_dtype(v.dtype) for k, v in sequence.items()}\n  shapes = {k: (None,) + v.shape[1:] for k, v in sequence.items()}\n  return dtypes, shapes\ndef _chunk_sequence(sequence_dict, chunk_length, random_offset=False):"
        },
        {
            "comment": "This function takes a dictionary of sequence tensors and splits them into chunks. It receives the chunk length, which determines the size of each chunk, and a boolean for random offset, which if set, randomly selects an offset to ensure at least one chunk is created. It returns a tf.data.Dataset containing the sequence chunks.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/datasets.py\":176-204",
            "content": "  \"\"\"Splits a dict of sequence tensors into a batch of chunks.\n  This function does not expect a batch of sequences, but a single sequence.\n  Args:\n    sequence_dict: dict of tensors with time along the first dimension.\n    chunk_length: Size of chunks the sequence will be split into.\n    random_offset: Start chunking from a random offset in the sequence,\n      enforcing that at least one chunk is generated.\n  Returns:\n    tf.data.Dataset of sequence chunks.\n  \"\"\"\n  length = tf.shape(list(sequence_dict.values())[0])[0]\n  if random_offset:\n    num_chunks = tf.maximum(1, length // chunk_length - 1)\n    output_length = num_chunks * chunk_length\n    max_offset = length - output_length\n    offset = tf.random_uniform((), 0, max_offset + 1, dtype=tf.int32)\n  else:\n    num_chunks = length // chunk_length\n    output_length = num_chunks * chunk_length\n    offset = 0\n  chunked = {}\n  for key, tensor in sequence_dict.items():\n    tensor = tensor[offset:offset + output_length]\n    chunked_shape = [num_chunks, chunk_length] + tensor.shape[1:].as_list()"
        },
        {
            "comment": "This code chunks the tensor based on a given shape and shuffles it using a seed generated from the filename. The chunked data is then returned as a TensorFlow dataset after being shuffled.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/datasets.py\":205-210",
            "content": "    chunked[key] = tf.reshape(tensor, chunked_shape)\n  filename = sequence_dict['filename'][0]\n  seed = tf.strings.to_hash_bucket_fast(filename, num_buckets=2**62)\n  return tf.data.Dataset.from_tensor_slices(chunked).shuffle(\n      tf.cast(length, tf.int64), seed=seed)"
        }
    ]
}