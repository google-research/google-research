{
    "summary": "This code constructs a TensorFlow VRNN model for video keypoint prediction, using RNN cells and decoders, with KLDivergence class, ScheduledSampling layer, and latent belief distribution sampling.",
    "details": [
        {
            "comment": "This code snippet is for building the VRNN (Video Recurrent Neural Network) dynamics model. It takes observed keypoints from a keypoint detector as input and returns the keypoints decoded from the model's latent belief. The model only uses input, hidden states, and recurrent layers. It is written in TensorFlow and contains a Keras layers module for building the network structure.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/dynamics.py\":0-31",
            "content": "# coding=utf-8\n# Copyright 2023 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Dynamics-related components of the structured video representation model.\nThese components model the dynamics of the keypoints extracted by the vision\ncomponents.\n\"\"\"\nimport tensorflow.compat.v1 as tf\nlayers = tf.keras.layers\ndef build_vrnn(cfg):\n  \"\"\"Builds the VRNN dynamics model.\n  The model takes observed keypoints from the keypoint detector as input and\n  returns keypoints decoded from the model's latent belief. The model only uses"
        },
        {
            "comment": "This code builds a model for predicting keypoints using a dynamics model. It includes an RNN cell, prior and posterior networks, and decoders. The observed steps are taken from the input, while predicted steps are generated by the model. Scheduled sampling is used to gradually transition between true and sampled values in both observed and predicted steps. The code also defines the number of timesteps based on observed and predicted steps.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/dynamics.py\":32-59",
            "content": "  the observed keypoints for the first cfg.observed_steps steps. For remaining\n  cfg.predicted_steps steps, it predicts keypoints using only the dynamics\n  model.\n  Args:\n    cfg: Hyperparameter ConfigDict.\n  Returns:\n    A tf.keras.Model and the KL loss tensor.\n  \"\"\"\n  # Build model components. All of the weights are shared across observed and\n  # predicted timesteps:\n  rnn_cell = layers.GRUCell(cfg.num_rnn_units)\n  prior_net = build_prior_net(cfg)\n  posterior_net = build_posterior_net(cfg)\n  decoder = build_decoder(cfg)\n  scheduled_sampler_obs = ScheduledSampling(\n      p_true_start=cfg.scheduled_sampling_p_true_start_obs,\n      p_true_end=cfg.scheduled_sampling_p_true_end_obs,\n      ramp_steps=cfg.scheduled_sampling_ramp_steps)\n  scheduled_sampler_pred = ScheduledSampling(\n      p_true_start=cfg.scheduled_sampling_p_true_start_pred,\n      p_true_end=cfg.scheduled_sampling_p_true_end_pred,\n      ramp_steps=cfg.scheduled_sampling_ramp_steps)\n  # Format inputs:\n  num_timesteps = cfg.observed_steps + cfg.predicted_steps"
        },
        {
            "comment": "This code implements a Variational Recurrent Neural Network (VRNN) for predicting keypoints of a video over time. It processes observed steps and then predicted steps to generate output keypoints and a reconstruction of input keypoints. The RNN cell is initialized, and the code iterates through each step to generate the keypoints using _vrnn_iteration function.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/dynamics.py\":60-84",
            "content": "  input_keypoints_stack = tf.keras.Input(\n      (num_timesteps, cfg.num_keypoints, 3), name='vrnn_input')\n  input_keypoints_list = layers.Lambda(lambda x: tf.unstack(x, axis=1))(\n      input_keypoints_stack)\n  # Initialize loop variables:\n  rnn_state = rnn_cell.get_initial_state(\n      batch_size=tf.shape(input_keypoints_stack)[0], dtype=tf.float32)\n  output_keypoints_list = [None] * num_timesteps\n  kl_div_list = [None] * cfg.observed_steps\n  # Process observed steps:\n  for t in range(cfg.observed_steps):\n    output_keypoints_list[t], rnn_state, kl_div_list[t] = _vrnn_iteration(\n        cfg, input_keypoints_list[t], rnn_state, rnn_cell, prior_net, decoder,\n        scheduled_sampler_obs, posterior_net)\n  # Process predicted steps:\n  for t in range(cfg.observed_steps, num_timesteps):\n    output_keypoints_list[t], rnn_state, _ = _vrnn_iteration(\n        cfg, input_keypoints_list[t], rnn_state, rnn_cell, prior_net, decoder,\n        scheduled_sampler_pred)\n  output_keypoints_stack = layers.Lambda(lambda x: tf.stack(x, axis=1))("
        },
        {
            "comment": "This function performs one timestep of the Variational Recurrent Neural Network (VRNN), taking input keypoints, previous recurrent state, RNN cell, prior net, decoder, and a scheduled sampler as arguments. It returns updated output keypoints stack and Kullback-Leibler divergence stack.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/dynamics.py\":85-113",
            "content": "      output_keypoints_list)\n  kl_div_stack = layers.Lambda(lambda x: tf.stack(x, axis=1))(kl_div_list)\n  return tf.keras.Model(\n      inputs=input_keypoints_stack,\n      outputs=[output_keypoints_stack, kl_div_stack],\n      name='vrnn')\ndef _vrnn_iteration(cfg,\n                    input_keypoints,\n                    rnn_state,\n                    rnn_cell,\n                    prior_net,\n                    decoder,\n                    scheduled_sampler,\n                    posterior_net=None):\n  \"\"\"Performs one timestep of the VRNN.\n  Args:\n    cfg: ConfigDict with model hyperparameters.\n    input_keypoints: [batch_size, num_keypoints, 3] tensor (one timestep of\n      the sequence returned by the keypoint detector).\n    rnn_state: Previous recurrent state.\n    rnn_cell: A Keras RNN cell object (e.g. tf.layers.GRUCell) that holds the\n      dynamics model.\n    prior_net: A tf.keras.Model that computes the prior latent belief from the\n      previous RNN state.\n    decoder: A tf.keras.Model that decodes the latent belief into keypoints."
        },
        {
            "comment": "This code defines a function that takes input keypoints and RNN state, and based on the presence of a posterior_net, calculates the mean and standard deviation for latent belief distribution either from prior_net alone or combining with observed keypoints. It also computes the KL divergence between the prior and posterior (if provided). This code is used in making predictions based on prior only when no posterior_net is available, allowing prediction without observation access.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/dynamics.py\":114-137",
            "content": "    scheduled_sampler: Keras layer instance that performs scheduled sampling.\n    posterior_net: (Optional) A tf.keras.Model that computes the posterior\n      latent belief, given observed keypoints and the previous RNN state. If no\n      posterior_net is supplied, prior latent belief is used for predictions.\n  Returns:\n    Three tensors: The output keypoints, the new RNN state, and the KL\n    divergence between the prior and posterior (None if no posterior_net is\n    provided).\n  \"\"\"\n  shape = input_keypoints.shape.as_list()[1:]\n  observed_keypoints_flat = layers.Reshape([shape[0] * shape[1]])(\n      input_keypoints)\n  # Obtain parameters mean, std for the latent belief distibution:\n  mean_prior, std_prior = prior_net(rnn_state)\n  if posterior_net:\n    mean, std = posterior_net([rnn_state, observed_keypoints_flat])\n    kl_divergence = KLDivergence(cfg.kl_annealing_steps)(\n        [mean_prior, std_prior, mean, std])\n  else:\n    # Having no posterior_net means that this cell is used to make predictions\n    # based on the prior only, without having access to observations. In this"
        },
        {
            "comment": "This code is from the \"video_structure/dynamics.py\" file and it seems to be related to a video processing model. It uses layers and samplers to compute mean and standard deviation, sample beliefs, decode them into keypoints, and step the RNN forward. The tf.stop_gradient function is used to stop gradients in certain parts of the code likely for performance or other optimization reasons. It also mentions a future TODO where the need for stop_gradients should be reviewed.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/dynamics.py\":138-160",
            "content": "    # case, no posterior is available to compute the KL term of the\n    # variational objective. We therefore cannot train the prior net during\n    # predicted steps. Since a reconstruction error is still generated, we\n    # need to stop the gradients explicitly to ensure the prior net is not\n    # updated based on these errors:\n    mean = layers.Lambda(tf.stop_gradient)(mean_prior)\n    std = layers.Lambda(tf.stop_gradient)(std_prior)\n    kl_divergence = None\n  # Sample a belief from the distribution and decode it into keypoints:\n  sampler = SampleBestBelief(\n      cfg.num_samples_for_bom,\n      decoder,\n      use_mean_instead_of_sample=cfg.use_deterministic_belief)\n  latent_belief, output_keypoints_flat = sampler(\n      [mean, std, rnn_state, observed_keypoints_flat])\n  output_keypoints = layers.Reshape(shape)(output_keypoints_flat)\n  # TODO(mjlm): Think through where we need stop_gradients.\n  # Step the RNN forward:\n  keypoints_for_rnn = scheduled_sampler([\n      observed_keypoints_flat, output_keypoints_flat])"
        },
        {
            "comment": "The code defines a function to build a prior network and decoder. The prior network takes the RNN state as input, outputs means and standard deviations for latent codes. The decoder takes the RNN state and latent code as inputs and outputs keypoints. Both functions return Keras Model objects.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/dynamics.py\":162-192",
            "content": "  rnn_input = layers.Concatenate(axis=-1)([keypoints_for_rnn, latent_belief])\n  _, rnn_state = rnn_cell(rnn_input, [rnn_state])\n  rnn_state = rnn_state[0]  # rnn_cell needs state to be wrapped in list.\n  return output_keypoints, rnn_state, kl_divergence\ndef build_prior_net(cfg):\n  \"\"\"Computes the prior belief over current keypoints, given past information.\n  rnn_state[t-1] --> prior_mean[t], prior_std[t]\n  Args:\n    cfg: Hyperparameter ConfigDict.\n  Returns:\n    Keras Model object.\n  \"\"\"\n  rnn_state = tf.keras.Input(shape=[cfg.num_rnn_units], name='rnn_state')\n  hidden = layers.Dense(cfg.prior_net_dim, **cfg.dense_layer_kwargs)(rnn_state)\n  means = layers.Dense(cfg.latent_code_size, name='means')(hidden)\n  stds_raw = layers.Dense(cfg.latent_code_size)(hidden)\n  stds = layers.Lambda(\n      lambda x: tf.nn.softplus(x) + 1e-4, name='stds')(stds_raw)\n  return tf.keras.Model(inputs=rnn_state, outputs=[means, stds], name='prior')\ndef build_decoder(cfg):\n  \"\"\"Decodes keypoints from the latent belief.\n  rnn_state[t-1], latent_code[t] --> keypoints[t]"
        },
        {
            "comment": "This code is defining two Keras models: a decoder and a posterior network. The decoder takes rnn_state and latent_code as inputs, concatenates them, passes through dense layers, and outputs keypoints. The posterior network takes rnn_state and keypoints as inputs, concatenates them, passes through dense layers, and outputs posterior mean and standard deviation. Both models are built using the Keras API with specified input shapes and activation functions.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/dynamics.py\":194-224",
            "content": "  Args:\n    cfg: Hyperparameter ConfigDict.\n  Returns:\n    Keras Model object.\n  \"\"\"\n  rnn_state = tf.keras.Input(shape=[cfg.num_rnn_units], name='rnn_state')\n  latent_code = tf.keras.Input(shape=[cfg.latent_code_size], name='latent_code')\n  hidden = layers.Concatenate()([rnn_state, latent_code])\n  hidden = layers.Dense(128, **cfg.dense_layer_kwargs)(hidden)\n  keypoints = layers.Dense(cfg.num_keypoints * 3, activation=tf.nn.tanh)(\n      hidden)\n  return tf.keras.Model(\n      inputs=[rnn_state, latent_code], outputs=keypoints, name='decoder')\ndef build_posterior_net(cfg):\n  \"\"\"Incorporates observed information into the latent belief.\n  rnn_state[t-1], observed_keypoints[t] --> posterior_mean[t], posterior_std[t]\n  Args:\n    cfg: Hyperparameter ConfigDict.\n  Returns:\n    Keras Model object.\n  \"\"\"\n  rnn_state = tf.keras.Input(shape=[cfg.num_rnn_units], name='rnn_state')\n  keypoints = tf.keras.Input(shape=[cfg.num_keypoints * 3], name='keypoints')\n  hidden = layers.Concatenate()([rnn_state, keypoints])\n  hidden = layers.Dense(cfg.posterior_net_dim, **cfg.dense_layer_kwargs)(hidden)"
        },
        {
            "comment": "This code defines a class that estimates the KL divergence between two distributions. It consists of a model with hidden layers, and a training step counter to track the number of steps during training. The KLDivergence class inherits from TrainingStepCounter and provides a method for calculating the KL divergence.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/dynamics.py\":225-253",
            "content": "  means = layers.Dense(cfg.latent_code_size, name='means')(hidden)\n  stds_raw = layers.Dense(cfg.latent_code_size)(hidden)\n  stds = layers.Lambda(\n      lambda x: tf.nn.softplus(x) + 1e-4, name='stds')(stds_raw)\n  return tf.keras.Model(\n      inputs=[rnn_state, keypoints], outputs=[means, stds], name='posterior')\nclass TrainingStepCounter(layers.Layer):\n  \"\"\"Provides a class attribute that contains the training step count.\"\"\"\n  def __init__(self, **kwargs):\n    self.uses_learning_phase = True\n    super().__init__(**kwargs)\n  def build(self, input_shape):\n    self.train_step = self.add_weight(\n        name='train_step', shape=[], initializer='zeros', trainable=False)\n    increment = tf.cast(tf.keras.backend.learning_phase(), tf.float32)\n    increment_op = tf.assign_add(self.train_step, increment)\n    self.add_update(increment_op)\n    super().build(input_shape)\n  def reset_states(self):\n    self.train_step.set_value(0)\nclass KLDivergence(TrainingStepCounter):\n  \"\"\"Returns the KL divergence between the prior and posterior distributions."
        },
        {
            "comment": "This code defines a ScheduledSampling layer that implements scheduled sampling for teacher forcing in RNN training. It takes the kl_annealing_steps parameter to anneal the KL divergence from 0 to final value over this many steps and returns the Kl_div value after summing it over distribution dimensions.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/dynamics.py\":255-281",
            "content": "  Attributes:\n    kl_annealing_steps: The returned KL divergence value will be linearly\n      annealed from 0 to the final value over this many training steps.\n  \"\"\"\n  def __init__(self, kl_annealing_steps=0, **kwargs):\n    self.kl_annealing_steps = kl_annealing_steps\n    super().__init__(**kwargs)\n  def call(self, inputs):\n    mean_prior, std_prior, mean, std = inputs\n    prior = tf.distributions.Normal(mean_prior, std_prior)\n    posterior = tf.distributions.Normal(mean, std)\n    kl_div = tf.distributions.kl_divergence(posterior, prior)\n    kl_div = tf.reduce_sum(kl_div, axis=-1)  # Sum over distribution dimensions.\n    if self.kl_annealing_steps:\n      kl_div *= tf.minimum(self.train_step / self.kl_annealing_steps, 1.0)\n    return kl_div\nclass ScheduledSampling(TrainingStepCounter):\n  \"\"\"Keras layer that implements scheduled sampling for teacher forcing.\n  See https://arxiv.org/abs/1506.03099.\n  For training an RNN, teacher forcing (i.e. providing the ground-truth inputs,\n  rather than the previous RNN outputs) can stabilize training. However, this"
        },
        {
            "comment": "This code defines a layer implementing linear scheduled sampling for RNN input distributions during training. It randomly mixes ground truth and RNN predictions, gradually reducing the ground-truth probability as training progresses. Disable by setting p_true_start and p_true_end to 0. Applied only during learning phase and returns predicted input during testing.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/dynamics.py\":282-307",
            "content": "  means that the RNN input distribution will be different during training and\n  inference. Scheduled sampling randomly mixes ground truth and RNN predictions\n  during training, slowly ramping down the ground-truth probablility as training\n  progresses. Thereby, training is stabilized initially and then becomes\n  gradually more realistic.\n  This layer implements a linear schedule.\n  To disable scheduled sampling, set p_true_start and p_true_end to 0.\n  Scheduled sampling is only applied during the learning phase (i.e. when\n  tf.keras.backend.learning_phase() is True). During testing, the layer always\n  returns the \"pred\" (i.e. second) input tensor.\n  Attributes:\n    p_true_start: Initial probability of sampling the \"true\" input.\n    p_true_end: Final probability of sampling the \"true\" input.\n    ramp_steps: Number of training steps over which the output will ramp from\n        p_true_start to p_true_end.\n  Returns:\n    Tensor containing either the true or the predicted input.\n  \"\"\"\n  def __init__(\n      self, p_true_start=1.0, p_true_end=0.2, ramp_steps=10000, **kwargs):"
        },
        {
            "comment": "This function implements a layer that chooses the best keypoints from multiple latent belief samples. It computes the probability of selecting ground truth based on training step, and then randomly selects either true or pred values during training and testing phases.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/dynamics.py\":308-338",
            "content": "    self.ramp_steps = ramp_steps\n    self.p_true_start = p_true_start\n    self.p_true_end = p_true_end\n    super().__init__(**kwargs)\n  def call(self, inputs):\n    \"\"\"Inputs should be [true, pred], each with size [batch, ...].\"\"\"\n    true, pred = inputs\n    # Compute current probability of choosing the ground truth:\n    ramp = self.train_step / self.ramp_steps\n    ramp = tf.minimum(ramp, 1.0)\n    p_true = self.p_true_start - (self.p_true_start - self.p_true_end) * ramp\n    # Flip a coin based on p_true:\n    return_true = tf.less(tf.random.uniform([]), p_true)\n    # During testing, use `pred` tensor (i.e. no teacher forcing):\n    return_true = tf.keras.backend.in_train_phase(return_true, False)\n    return tf.keras.backend.switch(return_true, true, pred)\nclass SampleBestBelief(layers.Layer):\n  \"\"\"Chooses the best keypoints from a number of latent belief samples.\n  This layer implements the \"best of many\" sample objective proposed in\n  https://arxiv.org/abs/1806.07772.\n  \"Best\" is defined to mean closest in Euclidean distance to the keypoints"
        },
        {
            "comment": "This code defines a class that takes input latent belief distribution and draws samples from it. The number of samples to draw is specified by the `num_samples` parameter, and the decoding of latent belief into keypoints is done through the `coordinate_decoder`. If `use_mean_instead_of_sample` is True, it uses the mean of the latent belief distribution instead of drawing samples. The class also inherits from another class and uses a learning phase.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/dynamics.py\":339-367",
            "content": "  observed by the vision model.\n  Attributes:\n    num_samples: Number of samples to choose the best from.\n    coordinate_decoder: tf.keras.Model object that decodes the latent belief\n      into keypoints.\n    use_mean_instead_of_sample: If true, do not sample, but just use the mean of\n      the latent belief distribution.\n  \"\"\"\n  def __init__(self,\n               num_samples,\n               coordinate_decoder,\n               use_mean_instead_of_sample=False,\n               **kwargs):\n    self.num_samples = num_samples\n    self.coordinate_decoder = coordinate_decoder\n    self.use_mean_instead_of_sample = use_mean_instead_of_sample\n    self.uses_learning_phase = True\n    super().__init__(**kwargs)\n  def call(self, inputs):\n    latent_mean, latent_std, rnn_state, observed_keypoints_flat = inputs\n    # Draw latent samples:\n    if self.use_mean_instead_of_sample:\n      sampled_latent = tf.stack([latent_mean] * self.num_samples)\n    else:\n      distribution = tf.distributions.Normal(loc=latent_mean, scale=latent_std)"
        },
        {
            "comment": "This code samples a distribution, decodes the samples into keypoints coordinates using a coordinate decoder, computes the L2 prediction loss for each sample and selects the best one based on the loss. The output shape is [input_shape[-1], input_shape[0]].",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/dynamics.py\":368-392",
            "content": "      sampled_latent = distribution.sample(sample_shape=(self.num_samples,))\n    sampled_latent_list = tf.unstack(sampled_latent)\n    # Decode samples into coordinates:\n    # sampled_keypoints has shape [num_samples, batch_size, 3 * num_keypoints].\n    sampled_keypoints = tf.stack([\n        self.coordinate_decoder([rnn_state, latent])\n        for latent in sampled_latent_list\n    ])\n    # If we have only 1 sample, we can just return that:\n    if self.num_samples == 1:\n      return [sampled_latent_list[0], sampled_keypoints[0]]\n    # Compute L2 prediction loss for all samples (note that this includes both\n    # the x,y-coordinates and the keypoint scale):\n    sample_losses = tf.reduce_mean(\n        (sampled_keypoints - observed_keypoints_flat[tf.newaxis, Ellipsis])**2.0,\n        axis=-1)  # Mean across keypoints.\n    # Choose the sample based on the loss:\n    return _choose_sample(sampled_latent, sampled_keypoints, sample_losses)\n  def compute_output_shape(self, input_shape):\n    return [input_shape[-1], input_shape[0]]"
        },
        {
            "comment": "This function selects the best sample based on its loss. During training, it chooses the sample with the lowest loss, while during inference, it selects the first sample regardless of the loss. It takes latent and keypoint samples along with their corresponding losses as input. The function then finds the indices of the samples with the lowest loss, converts them to int32, and retrieves the best latent and keypoint samples using these indices.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/dynamics.py\":395-418",
            "content": "def _choose_sample(sampled_latent, sampled_keypoints, sample_losses):\n  \"\"\"Returns the first or lowest-loss sample, depending on learning phase.\n  During training, the sample with the lowest loss is returned.\n  During inference, the first sample is returned without regard to the loss.\n  Args:\n    sampled_latent: [num_samples, batch_size, latent_code_size] tensor.\n    sampled_keypoints: [num_samples, batch_size, 3 * num_keypoints] tensor.\n    sample_losses: [num_samples, batch_size] tensor.\n  Returns:\n    Two tensors: latent and keypoint representation of the best sample.\n  \"\"\"\n  # Find the indices of the samples with the lowest loss:\n  best_sample_ind = tf.argmin(sample_losses, axis=0)  # Shape is [batch_size].\n  best_sample_ind = tf.cast(best_sample_ind, tf.int32)\n  batch_ind = tf.range(tf.shape(sampled_latent)[1], dtype=tf.int32)\n  indices = tf.stack([best_sample_ind, batch_ind], axis=-1)\n  # Only keep the best keypoints and latent sample:\n  best_latent = tf.gather_nd(sampled_latent, indices)\n  best_keypoints = tf.gather_nd(sampled_keypoints, indices)"
        },
        {
            "comment": "This code snippet checks if the program is in training mode or not, then selects the best sample during training and the first sample during inference. It uses the tf.keras.backend.in_train_phase function to make this decision.",
            "location": "\"/media/root/Prima/works/google-research/docs/src/video_structure/dynamics.py\":420-425",
            "content": "  # During training, return the best sample. During inference, return the\n  # first sample:\n  return [\n      tf.keras.backend.in_train_phase(best_latent, sampled_latent[0]),\n      tf.keras.backend.in_train_phase(best_keypoints, sampled_keypoints[0]),\n  ]"
        }
    ]
}