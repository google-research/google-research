{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_EEHLUogjT_"
      },
      "outputs": [],
      "source": [
        "# Licensed under the Apache License, Version 2.0\n",
        "\n",
        "# inference.py\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "SUPPORTED_MODELS = ['gemma', 'llama']\n",
        "\n",
        "class InferencePlatform(ABC):\n",
        "  \"\"\"An abstract class for the LLM inference platform we use.\"\"\"\n",
        "  @abstractmethod\n",
        "  def predict(self, prompt: str) -\u003e str:\n",
        "    pass\n",
        "\n",
        "class HuggingFace(InferencePlatform):\n",
        "  \"\"\"An implementation for using HuggingFace as the platform for LLM inference.\"\"\"\n",
        "  def __init__(self):\n",
        "    self._tokenizer = None\n",
        "    self._model = None\n",
        "\n",
        "  def authenticate(self, huggingface_token):\n",
        "    os.environ['HF_TOKEN'] = huggingface_token\n",
        "\n",
        "  def setup_model(self, model_name: str):\n",
        "    if model_name not in SUPPORTED_MODELS:\n",
        "      raise ValueError(f'Unsupported model: {model_name}')\n",
        "    if model_name == 'gemma':\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\")\n",
        "      self.model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\")\n",
        "    else:\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
        "      self.model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
        "\n",
        "  def predict(self, prompt: str) -\u003e str:\n",
        "    inputs = self.tokenizer.tokenize(prompt)\n",
        "    generate_ids = self.model.generate(inputs.input_ids)\n",
        "    return self.tokenizer.batch_decode(generate_ids, skip_special_tokens=True)\n",
        "\n",
        "class VertexAI(InferencePlatform):\n",
        "  \"\"\"An implementation for using Google Cloud's Vertex AI as the platform for\n",
        "  LLM inference.\"\"\"\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def predict(self, prompt: str) -\u003e str:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCd-PrA4jhFB"
      },
      "outputs": [],
      "source": [
        "# tree_utils.py\n",
        "\n",
        "def build_tree(dataset_path):\n",
        "  \"\"\"Builds and returns the hierarchical document tree.\"\"\"\n",
        "  pass\n",
        "\n",
        "def search_tree(doc_tree, inference_platform: InferencePlatform):\n",
        "  \"\"\"Searches the hierarchical document tree recursively.\"\"\"\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBYpaeRzloJe"
      },
      "outputs": [],
      "source": [
        "# run.py\n",
        "\n",
        "HF_TOKEN = \"\" # add your hugging face access token here\n",
        "\n",
        "def main():\n",
        "  inference_platform = HuggingFace()\n",
        "  inference_platform.authenticate(HF_TOKEN)\n",
        "  inference_platform.setup_model('llama')\n",
        "\n",
        "  dataset_path = \"\"\n",
        "  # Build hierarchical document tree.\n",
        "  doc_tree = build_tree(dataset_path)\n",
        "\n",
        "  # Search tree recursively.\n",
        "  search_tree(doc_tree, inference_platform)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlW4G2Imn7LB"
      },
      "outputs": [],
      "source": [
        "# visualize.py\n",
        "\n",
        "# TODO(james): See if we can reuse the original visualization from the paper."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/grp/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
