{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_EEHLUogjT_"
      },
      "outputs": [],
      "source": [
        "# Licensed under the Apache License, Version 2.0\n",
        "\n",
        "#CHECKING OUT FLASH ATTENTION\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!pip install accelerate\n",
        "# !pip install flash-attn\n",
        "!pip install flash-attn --no-build-isolation\n",
        "\n",
        "\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import T5Model, T5ForConditionalGeneration\n",
        "import os\n",
        "import torch\n",
        "\n",
        "SUPPORTED_MODEL_DICT = {\n",
        "  'gemma7b' : \"google/gemma-7b\",\n",
        "  'gemma2b' : \"google/gemma-2b\",\n",
        "  'llama3'  : \"meta-llama/Meta-Llama-3-8B\",\n",
        "  't5-large' : \"google-t5/t5-large\",\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "class InferencePlatform(ABC):\n",
        "  \"\"\"An abstract class for the LLM inference platform we use.\"\"\"\n",
        "  @abstractmethod\n",
        "  def predict(self, prompt: str) -> str:\n",
        "    pass\n",
        "\n",
        "class HuggingFace(InferencePlatform):\n",
        "  \"\"\"An implementation for using HuggingFace as the platform for LLM inference.\"\"\"\n",
        "  def __init__(self):\n",
        "    self._tokenizer = None\n",
        "    self._model = None\n",
        "\n",
        "  def authenticate(self, huggingface_token):\n",
        "    os.environ['HF_TOKEN'] = huggingface_token\n",
        "\n",
        "  def setup_model(self, model_name: str):\n",
        "    if model_name not in SUPPORTED_MODEL_DICT:\n",
        "      raise ValueError(f'Unsupported model: {model_name}')\n",
        "    self.model_name=model_name\n",
        "    if model_name in ['gemma2b','gemma7b','llama3']:\n",
        "      hf_path = SUPPORTED_MODEL_DICT[model_name]\n",
        "      LOCAL=False\n",
        "      #LOCAL=True\n",
        "\n",
        "      if not LOCAL:\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(hf_path)\n",
        "        #self.model = AutoModelForCausalLM.from_pretrained(hf_path)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(hf_path,device_map='auto',torch_dtype=torch.float16,attn_implementation=\"flash_attention_2\")\n",
        "      else:\n",
        "        #TODO: update local path\n",
        "        local_path = hf_path.split('/')[1]\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"gdrive/My Drive/Colab Notebooks/gemma-2b-tokenizer\", local_files_only=True,device_map='auto')\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\"gdrive/My Drive/Colab Notebooks/gemma-2b-model\", local_files_only=True,device_map='auto',torch_dtype=torch.float16,attn_implementation=\"flash_attention_2\")\n",
        "\n",
        "    else:\n",
        "      hf_path = SUPPORTED_MODEL_DICT[model_name]\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(hf_path)\n",
        "      self.model = T5ForConditionalGeneration.from_pretrained(hf_path)\n",
        "\n",
        "\n",
        "  def predict(self, prompt: str) -> str:\n",
        "    # inputs = self.tokenizer.tokenize(prompt)\n",
        "    # inputs = self.tokenizer.tokenize(prompt, return_tensors='pt')\n",
        "      # prompt_list,\n",
        "      # return_tensors=\"pt\",\n",
        "      # padding=True,\n",
        "      # return_attention_mask=True,\n",
        "#    inputs = self.tokenizer(prompt, return_tensors='pt')\n",
        "    inputs = self.tokenizer(prompt, return_tensors='pt').to(\"cuda\")\n",
        "    generate_ids = self.model.generate(inputs.input_ids)\n",
        "    return self.tokenizer.batch_decode(generate_ids, skip_special_tokens=True)\n",
        "\n",
        "  def predict_from_tokens(self, input_tokens):\n",
        "    generate_ids = self.model.generate(input_tokens)\n",
        "    return self.tokenizer.batch_decode(generate_ids, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "class VertexAI(InferencePlatform):\n",
        "  \"\"\"An implementation for using Google Cloud's Vertex AI as the platform for\n",
        "  LLM inference.\"\"\"\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def predict(self, prompt: str) -> str:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "HF_TOKEN = \"\" #inside colab secret\n",
        "\n",
        "inference_platform = HuggingFace()\n",
        "inference_platform.authenticate(HF_TOKEN)\n",
        "inference_platform.setup_model('gemma2b')\n"
      ],
      "metadata": {
        "id": "pLOaVp_KI9Q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_prompt_list_from_doc(\n",
        "    prompt_style, ctx, question, max_num_of_passages=None\n",
        "):\n",
        "  \"\"\"Gets list of prompts based on the prompt style provided.\"\"\"\n",
        "  prompt_list = []\n",
        "  prompt = \"\"\n",
        "  prompt_header = (\n",
        "      \"Write a high-quality answer for the given question using only the\"\n",
        "      \" provided search results (some of which might be irrelevant).\\n\"\n",
        "  )\n",
        "  prompt += prompt_header\n",
        "  prompt_list.append(prompt_header)\n",
        "\n",
        "  for dd, doc in enumerate(ctx):\n",
        "    if dd < max_num_of_passages:\n",
        "      prompt_dd = (\n",
        "          f\"Document [{str(dd+1)}] (Title: {doc['title']}) {doc['text']}\\n\"\n",
        "      )\n",
        "      prompt += prompt_dd\n",
        "      prompt_list.append(prompt_dd)\n",
        "\n",
        "  prompt_footer = f\"Question: {question}\\nAnswer:\"\n",
        "  prompt += prompt_footer\n",
        "  prompt_list.append(prompt_footer)\n",
        "  return prompt_list\n",
        "\n"
      ],
      "metadata": {
        "id": "k6Ko2QpbqDHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# path_for_query_and_documents = (\"contriever_msmarco_nq/nq-open-oracle-top1000.jsonl\")\n",
        "path_for_query_and_documents = \"gdrive/My Drive/Colab Notebooks/litm-NQ-data/nq-open-30_total_documents_gold_at_0.jsonl\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with open(path_for_query_and_documents, \"r\") as json_file:\n",
        "  json_list = list(json_file)\n",
        "\n"
      ],
      "metadata": {
        "id": "fTazaDbZqsAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bhS421ymSw6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fAWHgieOIAkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dR4oyNR_SsQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "(1) instead of using the 10%/30% thresholding to explore the hierarchy, we can use your idea of choosing\n",
        "     the top-1,2,3 documents to explore (since this will give more consistent results with respect to time taken\n",
        "      - and requires fewer permutations/samples)\n",
        "\n",
        "(2) we can use Banzhaf with a smaller number of permutations/samples. possibly as small as two or three random samples\n",
        "    (using dual samples should converge very quickly for baznhaf).  it is worth noting that the speculative decoding\n",
        "    really picks up in speed when we use a larger number of samples, so maybe the downside of not implementing spec dec\n",
        "     could be hedged in this way.\n",
        "\n",
        "(3) I am currently working with Pragun to implement more general hierarchies.  I think if we plug this in to some very\n",
        "    simple examples (e.g. code prediction, JSON formatted inputs, and dialogue agents), it would become a lot easier\n",
        "    for us to advertise our work to other teams\n",
        "'''"
      ],
      "metadata": {
        "id": "YQqYYDlcpf0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Somehow update in real-time for a demo\n",
        "# -- need to show the 'currently checking paragraphs'\n",
        "# -- then something like 'currently checking sentences'\n",
        "# -- -- would like the JS to be interactive while the model is running in the background"
      ],
      "metadata": {
        "id": "CbnAdISG5ooK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#scaffolding to explore needs to be the top #3 documents and each of their sentences\n",
        "\n",
        "#maybe the best from a visualization standpoint (which is running online in the background)\n",
        "# would be to do a DFS style approach"
      ],
      "metadata": {
        "id": "0vR2yMYCS_rZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "t=inference_platform.tokenizer"
      ],
      "metadata": {
        "id": "HTQGwlL7xLWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6YGl0QILpK_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AKwgckxcpLB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UAP6GXXyxRMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def sliced_process_documents_from_index(json_list_gi, x, nop, pass_sparse, t):\n",
        "def sliced_process_documents_from_index(result, sparse_vec, t):\n",
        "  \"\"\"Processes documents from index.\"\"\"\n",
        "  ctx = result[\"ctxs\"]\n",
        "  question = result[\"question\"]\n",
        "\n",
        "\n",
        "  search_results = \"\"\n",
        "  search_results_list = []\n",
        "  # for dd,doc_ind in enumerate(pass_sparse):\n",
        "  dd=0\n",
        "  for doc_ind,on_or_off in enumerate(sparse_vec):\n",
        "    if on_or_off:\n",
        "      doc = ctx[doc_ind]\n",
        "      search_results_list.append( f\"Document [{str(dd+1)}] (Title: {doc['title']}) {doc['text']}\\n\" )\n",
        "      dd+=1\n",
        "  search_results = ''.join(search_results_list)\n",
        "\n",
        "\n",
        "  prompt = (\n",
        "      \"Write a high-quality answer for the given question using only the\"\n",
        "      \" provided search results (some of which might be\"\n",
        "      f\" irrelevant).\\n\\n{search_results}\\nQuestion: {question}\\nAnswer:\"\n",
        "  )\n",
        "  tokens = t.encode(prompt)\n",
        "  return tokens\n",
        "  #token_strings = [t.decode(thing) for thing in tokens]"
      ],
      "metadata": {
        "id": "71mqI6ZYuXkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def sliced_process_documents_from_index_with_hierarchy(result, sparse_vec, t):\n",
        "  \"\"\"Processes documents from index.\"\"\"\n",
        "  ctx = result[\"ctxs\"]\n",
        "  question = result[\"question\"]\n",
        "\n",
        "\n",
        "  search_results = \"\"\n",
        "  search_results_list = []\n",
        "  # for dd,doc_ind in enumerate(pass_sparse):\n",
        "  dd=0\n",
        "  doc_remap = {}\n",
        "  for doc_ind,on_or_off in enumerate(sparse_vec):\n",
        "    if on_or_off:\n",
        "      doc_remap[dd]=doc_ind\n",
        "      doc = ctx[doc_ind]\n",
        "      search_results_list.append( f\"Document [{str(dd+1)}] (Title: {doc['title']}) {doc['text']}\\n\" )\n",
        "      dd+=1\n",
        "  search_results = ''.join(search_results_list)\n",
        "  nop=dd\n",
        "\n",
        "\n",
        "  prompt = (\n",
        "      \"Write a high-quality answer for the given question using only the\"\n",
        "      \" provided search results (some of which might be\"\n",
        "      f\" irrelevant).\\n\\n{search_results}\\nQuestion: {question}\\nAnswer:\"\n",
        "  )\n",
        "  tokens = t.encode(prompt)\n",
        "  ###return tokens\n",
        "  token_strings = [t.decode(thing) for thing in tokens]\n",
        "\n",
        "\n",
        "\n",
        "  #doc_id = t.encode(\"Document\")[0] #BOS in Gemma\n",
        "  doc_id = t.encode(\"Document\")[1]\n",
        "  doc_indices = np.argwhere(np.array(tokens) == doc_id)[:, 0]\n",
        "  doc_indices = list(doc_indices)\n",
        "\n",
        "  #q_id = t.encode(\"Question\")[0] #BOS in Gemma\n",
        "  q_id = t.encode(\"Question\")[1]\n",
        "  q_ind = np.argwhere(np.array(tokens) == q_id)[:, 0]\n",
        "  doc_indices.extend(list(q_ind))\n",
        "  #print('doc_indices',doc_indices)\n",
        "\n",
        "  first_layer = {}\n",
        "  for dd in range(nop):\n",
        "    doc_span_tup_dd = (\n",
        "        doc_indices[dd],\n",
        "        doc_indices[dd + 1],\n",
        "    )\n",
        "\n",
        "    doc_dd_splits = []\n",
        "    doc_dd = token_strings[doc_span_tup_dd[0] : doc_span_tup_dd[1]]\n",
        "    doc_dd_toks = tokens[doc_span_tup_dd[0] : doc_span_tup_dd[1]]\n",
        "\n",
        "    ###doc = ctx[dd] #no sir!\n",
        "    doc = ctx[doc_remap[dd]]\n",
        "    prompt_end_ind = len( t.encode(\"Document [\"+str(dd+1)+\"] (Title: \"+doc[\"title\"]+\")\") ) - 1\n",
        "\n",
        "    doc_dd_splits.append(0)\n",
        "    doc_dd_splits.append(prompt_end_ind)\n",
        "\n",
        "    things_to_split = [\".\",  \",\", ]\n",
        "\n",
        "    for xx in range(prompt_end_ind, len(doc_dd)):\n",
        "      if doc_dd[xx] in things_to_split:\n",
        "        doc_dd_splits.append(xx + 1)\n",
        "    last = len(doc_dd)\n",
        "    if last not in doc_dd_splits:\n",
        "      doc_dd_splits.append(last)\n",
        "\n",
        "    queue = [7904]  # arbitrary token used for splitting\n",
        "    toktups = []\n",
        "    for xx in range(len(doc_dd_toks)):\n",
        "      queue.append(doc_dd_toks[xx])\n",
        "      if len(t.decode(queue).split(\" \")) > 2:\n",
        "        toktups.append(queue[1:-1])\n",
        "        while len(queue) > 2:\n",
        "          queue.pop(0)\n",
        "    toktups.append(queue[1:])\n",
        "\n",
        "    second_layer = {}\n",
        "    for dd_ph in range(len(doc_dd_splits) - 1):\n",
        "      phrase_span_tup_ph = (\n",
        "          doc_dd_splits[dd_ph],\n",
        "          doc_dd_splits[dd_ph + 1],\n",
        "      )\n",
        "      third_layer = {}\n",
        "      word_pos = 0\n",
        "      xx = 0\n",
        "      for _, toktup in enumerate(toktups):\n",
        "        if (\n",
        "            word_pos >= phrase_span_tup_ph[0]\n",
        "            and word_pos + len(toktup) <= phrase_span_tup_ph[1]\n",
        "        ):\n",
        "          word_tup = (\n",
        "              word_pos - phrase_span_tup_ph[0],\n",
        "              word_pos + len(toktup) - phrase_span_tup_ph[0],\n",
        "          )\n",
        "          third_layer[xx] = (None, word_tup)\n",
        "          xx += 1\n",
        "        word_pos += len(toktup)\n",
        "      second_layer[dd_ph] = (third_layer, phrase_span_tup_ph)\n",
        "    first_layer[dd] = (second_layer, doc_span_tup_dd)\n",
        "  return first_layer, tokens"
      ],
      "metadata": {
        "id": "zCQz2U1C4Bis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x5XTwZVM3rRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2QZF4k80E1Is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iPGK0yg7E2bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def document_level_interpretability_steps(result, D=10, S=10, doc_masking_type=None, doc_interp_style=\"DOC_BANZ_50\"):\n",
        "\n",
        "  # doc_masking_type = \"DOC_SLICE\"\n",
        "  # doc_masking_type = \"DOC_PAD\"\n",
        "\n",
        "\n",
        "  # doc_interp_style = \"DOC_INCLUSION_0\"\n",
        "  # doc_interp_style = \"DOC_BANZ_50\"\n",
        "  # doc_interp_style = \"DOC_REMOVAL_100\"\n",
        "  MAX_NEW_TOKENS = 10\n",
        "  extra_verbose=True\n",
        "  extra_verbose=False\n",
        "\n",
        "\n",
        "  list_of_perms=[]\n",
        "  start_time = time.time()\n",
        "  new_gen_list=[]\n",
        "  new_gen_list_list_doc=[]\n",
        "  new_conf_list_doc=[]\n",
        "\n",
        "  if doc_masking_type==\"DOC_PAD\":\n",
        "    first_layer,tokens = sliced_process_documents_from_index_with_hierarchy(result, torch.ones(D), t)\n",
        "    input_tokens = torch.Tensor(tokens).long().to(\"cuda\")\n",
        "    L=len(tokens)\n",
        "\n",
        "  for s in range(S):\n",
        "    print('s',s)\n",
        "    if doc_interp_style==\"DOC_BANZ_50\":\n",
        "      conf = np.random.rand(D) > 0.5\n",
        "      list_of_perms.append(conf)\n",
        "    elif doc_interp_style==\"DOC_REMOVAL_100\":\n",
        "      conf = np.ones(D,dtype=bool)\n",
        "    elif doc_interp_style==\"DOC_INCLUSION_0\":\n",
        "      conf = np.zeros(D,dtype=bool)\n",
        "    elif doc_interp_style==\"DOC_SHAP\":\n",
        "      unif = np.random.rand()\n",
        "      conf = np.random.rand(D) > unif\n",
        "    else:\n",
        "      raise Exception(\"doc interp method not available\")\n",
        "    if extra_verbose:\n",
        "      print(conf)\n",
        "\n",
        "\n",
        "    if doc_masking_type==\"DOC_SLICE\":\n",
        "      sparsity_grid = torch.zeros((D+1,D),dtype=int)\n",
        "      for d in range(D):\n",
        "        if conf[d]:\n",
        "          sparsity_grid[:,d] = 1\n",
        "      sparsity_grid[np.arange(D)+1,np.arange(D)] = 1-sparsity_grid[np.arange(D)+1,np.arange(D)]\n",
        "\n",
        "\n",
        "    if doc_masking_type==\"DOC_PAD\":\n",
        "      sparsity2 = torch.zeros((D+1,L),dtype=int).to(\"cuda\")\n",
        "      sparsity2[:,:first_layer[0][1][0]]   = 1 #prefix\n",
        "      sparsity2[:,first_layer[D-1][1][1]:] = 1 #suffix\n",
        "      for d in range(D):\n",
        "        doc_span_tup_dd = first_layer[d][1]\n",
        "        if conf[d]:\n",
        "          sparsity2[:,doc_span_tup_dd[0]:doc_span_tup_dd[1]] = 1\n",
        "      for d in range(D):\n",
        "        doc_span_tup_dd = first_layer[d][1]\n",
        "        sparsity2[d+1,doc_span_tup_dd[0]:doc_span_tup_dd[1]] = 1 - sparsity2[d+1,doc_span_tup_dd[0]:doc_span_tup_dd[1]]\n",
        "\n",
        "      sparsity3 = sparsity2.clone()\n",
        "      prefix=first_layer[0][1][0]\n",
        "      suffix=first_layer[D-1][1][1]\n",
        "      sparsity3[:,prefix:suffix] = 1 - sparsity3[:,prefix:suffix]\n",
        "\n",
        "    # conf_tup_1 = (conf,)\n",
        "    # conf_tup_2 = (~conf,)\n",
        "    conf_tup_1 = (list(conf),)\n",
        "    conf_tup_2 = (list(~conf),)\n",
        "    new_conf_list_doc.append(conf_tup_1)\n",
        "    new_conf_list_doc.append(conf_tup_2)\n",
        "\n",
        "\n",
        "\n",
        "    new_gen_list=[]\n",
        "    for ss in range(D+1):\n",
        "      if extra_verbose:\n",
        "        print(ss)\n",
        "      if doc_masking_type==\"DOC_SLICE\":\n",
        "        tokens = sliced_process_documents_from_index(result,sparsity_grid[ss],t)\n",
        "        input_ss = torch.Tensor(tokens).long().to(\"cuda\")[None]\n",
        "      elif doc_masking_type==\"DOC_PAD\":\n",
        "        input_ss = input_tokens[None]*sparsity2[ss][None]\n",
        "      else:\n",
        "        raise Exception(\"doc masking method not available\")\n",
        "\n",
        "      generate_ids = inference_platform.model.generate(input_ss, max_new_tokens=MAX_NEW_TOKENS)\n",
        "      new_generate_ids = generate_ids[:,input_ss.shape[1]:]\n",
        "      new_gen_list.append(new_generate_ids.cpu())\n",
        "      if extra_verbose:\n",
        "        print( inference_platform.tokenizer.batch_decode(new_generate_ids, skip_special_tokens=True) )\n",
        "        #print(new_generate_ids)\n",
        "        print(time.time()-start_time)\n",
        "\n",
        "      if doc_masking_type==\"DOC_SLICE\":\n",
        "        tokens = sliced_process_documents_from_index(result,1-sparsity_grid[ss],t)\n",
        "        input_ss = torch.Tensor(tokens).long().to(\"cuda\")[None]\n",
        "      elif doc_masking_type==\"DOC_PAD\":\n",
        "        input_ss = input_tokens[None]*sparsity3[ss][None]\n",
        "      else:\n",
        "        raise Exception(\"doc masking method not available\")\n",
        "\n",
        "      generate_ids = inference_platform.model.generate(input_ss, max_new_tokens=MAX_NEW_TOKENS)\n",
        "      new_generate_ids = generate_ids[:,input_ss.shape[1]:]\n",
        "      new_gen_list.append(new_generate_ids.cpu())\n",
        "      if extra_verbose:\n",
        "        print( inference_platform.tokenizer.batch_decode(new_generate_ids, skip_special_tokens=True) )\n",
        "        #print(new_generate_ids)\n",
        "        print(time.time()-start_time)\n",
        "        print()\n",
        "    new_gen_list_list_doc.append(new_gen_list)\n",
        "  print(time.time()-start_time)\n",
        "  pass\n",
        "  # return new_gen_list_list_doc\n",
        "  return new_gen_list_list_doc,new_conf_list_doc\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F7Mg2rWqMeMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def sentence_level_interpretability_steps(result, D=10, S=5, d_to_check=0, doc_masking_type=\"DOC_SLICE\", doc_interp_style=None,\n",
        "                                                                           sen_masking_type=None, sen_interp_style = \"SEN_BANZ_50\"):\n",
        "\n",
        "  # doc_masking_type = \"DOC_SLICE\"\n",
        "  # doc_masking_type = \"DOC_PAD\"\n",
        "  # doc_interp_style = \"DOC_BANZ_50\"\n",
        "  # doc_interp_style = \"DOC_REMOVAL_100\"\n",
        "\n",
        "  # sen_masking_type = \"SEN_SLICE\"\n",
        "  # sen_masking_type = \"SEN_PAD\"\n",
        "  # sen_interp_style = \"SEN_BANZ_50\"\n",
        "  # sen_interp_style = \"SEN_SHAP\"\n",
        "\n",
        "  MAX_NEW_TOKENS = 10\n",
        "  extra_verbose=True\n",
        "  extra_verbose=False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  unchanging_context_interp_styles=[\"DOC_REMOVAL_100\"]\n",
        "\n",
        "  if doc_interp_style in unchanging_context_interp_styles:\n",
        "    first_layer_ss, tokens = sliced_process_documents_from_index_with_hierarchy(result,torch.ones(D),t)\n",
        "    L_ss = len(tokens)\n",
        "    input_tokens = torch.Tensor(tokens).long().to(\"cuda\")\n",
        "\n",
        "\n",
        "  list_of_perms=[]\n",
        "\n",
        "  start_time = time.time()\n",
        "  new_gen_list=[]\n",
        "  new_gen_list_list_sen=[]\n",
        "  new_conf_list_sen=[]\n",
        "\n",
        "\n",
        "  for s in range(S):\n",
        "    print('s',s)\n",
        "    if doc_interp_style==\"DOC_BANZ_50\":\n",
        "      conf = np.random.rand(D) > 0.5\n",
        "      list_of_perms.append(conf)\n",
        "    elif doc_interp_style==\"DOC_REMOVAL_100\":\n",
        "      conf = np.ones(D,dtype=bool)\n",
        "    elif doc_interp_style==\"DOC_SHAP\":\n",
        "      unif = np.random.rand()\n",
        "      conf = np.random.rand(D) > unif\n",
        "    else:\n",
        "      raise Exception(\"doc interp method not available\")\n",
        "    if extra_verbose:\n",
        "      print(conf)\n",
        "\n",
        "\n",
        "    sparsity_grid = torch.zeros((1,D),dtype=int)\n",
        "    s_to_check = -1\n",
        "    s_d = 0\n",
        "    for d in range(D):\n",
        "      if d==d_to_check:\n",
        "        sparsity_grid[:,d] = 1\n",
        "        s_to_check=s_d\n",
        "      if conf[d]:\n",
        "        sparsity_grid[:,d] = 1\n",
        "        s_d+=1\n",
        "    D2 = int(np.array(torch.sum(sparsity_grid)))\n",
        "\n",
        "\n",
        "    if doc_interp_style not in unchanging_context_interp_styles:\n",
        "      first_layer_ss, tokens = sliced_process_documents_from_index_with_hierarchy(result,sparsity_grid[0],t)\n",
        "      L_ss = len(tokens)\n",
        "      input_tokens = torch.Tensor(tokens).long().to(\"cuda\")\n",
        "\n",
        "\n",
        "    if True:\n",
        "      second_layer = first_layer_ss[s_to_check][0]\n",
        "      local_dim = len(second_layer.keys())\n",
        "\n",
        "    sparsity2 = torch.zeros((local_dim+1,L_ss),dtype=int).to(\"cuda\")\n",
        "    sparsity2[:,:first_layer_ss[0][1][0]]   = 1 #prefix\n",
        "    sparsity2[:,first_layer_ss[D2-1][1][1]:] = 1 #suffix\n",
        "    if doc_masking_type==\"DOC_SLICE\":\n",
        "      for d in range(D2):\n",
        "        doc_span_tup_dd = first_layer_ss[d][1]\n",
        "        if d!=s_to_check: #include everything but the doc to check (already sliced)\n",
        "          sparsity2[:,doc_span_tup_dd[0]:doc_span_tup_dd[1]] = 1\n",
        "    elif doc_masking_type==\"DOC_PAD\":\n",
        "      for d in range(D2):\n",
        "        doc_span_tup_dd = first_layer_ss[d][1]\n",
        "        if conf[d] and d!=s_to_check: #only include certain docs (because padding), make sure not to include doc to check\n",
        "          sparsity2[:,doc_span_tup_dd[0]:doc_span_tup_dd[1]] = 1\n",
        "    else:\n",
        "      raise Exception(\"doc masking method not available\")\n",
        "\n",
        "\n",
        "    if True:\n",
        "      if sen_interp_style == \"SEN_BANZ_50\":\n",
        "        conf_ll = np.random.rand(local_dim) > 0.5\n",
        "      elif sen_interp_style == \"SEN_SHAP\":\n",
        "        unif_ll = np.random.rand()\n",
        "        conf_ll = np.random.rand(local_dim) > unif_ll\n",
        "      for l in range(local_dim):\n",
        "        doc_span_tup_dd = first_layer_ss[s_to_check][1]\n",
        "        doc_span_tup_dd_ll = second_layer[l][1]\n",
        "        if conf_ll[l]:\n",
        "          sparsity2[:,doc_span_tup_dd[0]+doc_span_tup_dd_ll[0]:doc_span_tup_dd[0]+doc_span_tup_dd_ll[1]] = 1\n",
        "\n",
        "      for l in range(local_dim):\n",
        "        doc_span_tup_dd = first_layer_ss[s_to_check][1]\n",
        "        doc_span_tup_dd_ll = second_layer[l][1]\n",
        "        if True: #bugfix...\n",
        "          sparsity2[l+1,doc_span_tup_dd[0]+doc_span_tup_dd_ll[0]:doc_span_tup_dd[0]+doc_span_tup_dd_ll[1]] = 1-sparsity2[l+1,doc_span_tup_dd[0]+doc_span_tup_dd_ll[0]:doc_span_tup_dd[0]+doc_span_tup_dd_ll[1]]\n",
        "\n",
        "      sparsity3 = sparsity2.clone()\n",
        "      prefix=first_layer_ss[s_to_check][1][0]\n",
        "      suffix=first_layer_ss[s_to_check][1][1]\n",
        "      sparsity3[:,prefix:suffix] = 1 - sparsity3[:,prefix:suffix]\n",
        "\n",
        "      # conf_tup_1 = (conf,conf_ll)\n",
        "      # conf_tup_2 = (conf,~conf_ll)\n",
        "      conf_tup_1 = (list(conf),list(conf_ll))\n",
        "      conf_tup_2 = (list(conf),list(conf_ll))\n",
        "      new_conf_list_sen.append(conf_tup_1)\n",
        "      new_conf_list_sen.append(conf_tup_2)\n",
        "\n",
        "      if False: #super verbose on word parsing\n",
        "        pass\n",
        "        patch = tokens[doc_span_tup_dd[0]:doc_span_tup_dd[1]]\n",
        "        for l in range(local_dim):\n",
        "          range_ = second_layer[l][1]\n",
        "          print(l,'\\t\\\"'+t.decode(patch[range_[0]:range_[1]])+'\\\"')\n",
        "\n",
        "\n",
        "    new_gen_list=[]\n",
        "    for ss in range(local_dim+1):\n",
        "      if extra_verbose:\n",
        "        print(ss)\n",
        "\n",
        "      if sen_masking_type==\"SEN_SLICE\": #v5,v4\n",
        "        input_ss = input_tokens[sparsity2[ss]==1][None]\n",
        "      elif sen_masking_type==\"SEN_PAD\": #v3\n",
        "        input_ss = input_tokens[None]*sparsity2[ss][None]\n",
        "      else:\n",
        "        raise Exception(\"sen masking method not available\")\n",
        "\n",
        "\n",
        "      generate_ids = inference_platform.model.generate(input_ss, max_new_tokens=MAX_NEW_TOKENS)\n",
        "      new_generate_ids = generate_ids[:,input_ss.shape[1]:]\n",
        "      new_gen_list.append(new_generate_ids.cpu())\n",
        "      if extra_verbose:\n",
        "        print( inference_platform.tokenizer.batch_decode(new_generate_ids, skip_special_tokens=True) )\n",
        "        #print(new_generate_ids)\n",
        "        print(time.time()-start_time)\n",
        "\n",
        "      if sen_masking_type==\"SEN_SLICE\": #v5,v4\n",
        "        input_ss = input_tokens[sparsity3[ss]==1][None]\n",
        "      elif sen_masking_type==\"SEN_PAD\": #v3\n",
        "        input_ss = input_tokens[None]*sparsity3[ss][None]\n",
        "      else:\n",
        "        raise Exception(\"sen masking method not available\")\n",
        "\n",
        "      generate_ids = inference_platform.model.generate(input_ss, max_new_tokens=MAX_NEW_TOKENS)\n",
        "      new_generate_ids = generate_ids[:,input_ss.shape[1]:]\n",
        "      new_gen_list.append(new_generate_ids.cpu())\n",
        "      if extra_verbose:\n",
        "        print( inference_platform.tokenizer.batch_decode(new_generate_ids, skip_special_tokens=True) )\n",
        "        #print(new_generate_ids)\n",
        "        print(time.time()-start_time)\n",
        "        print()\n",
        "    new_gen_list_list_sen.append(new_gen_list)\n",
        "  print(time.time()-start_time)\n",
        "  pass\n",
        "  # return new_gen_list_list_sen\n",
        "  return new_gen_list_list_sen,new_conf_list_sen\n",
        "\n"
      ],
      "metadata": {
        "id": "DWqmeIL2R2Yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def word_level_interpretability_steps(result, D=10, S=5, d_to_check=0, sen_to_check=0,\n",
        "                                      doc_masking_type=\"DOC_SLICE\", doc_interp_style=None,\n",
        "                                      sen_masking_type=None, sen_interp_style = \"SEN_BANZ_50\",\n",
        "                                      wor_masking_type=None):\n",
        "\n",
        "  # doc_masking_type = \"DOC_SLICE\"\n",
        "  # doc_masking_type = \"DOC_PAD\"\n",
        "  # doc_interp_style = \"DOC_BANZ_50\"\n",
        "  # doc_interp_style = \"DOC_REMOVAL_100\"\n",
        "\n",
        "  # sen_masking_type = \"SEN_SLICE\"\n",
        "  # sen_masking_type = \"SEN_PAD\"\n",
        "  # sen_interp_style = \"SEN_BANZ_50\"\n",
        "\n",
        "  # wor_masking_type = \"WOR_SLICE\" #all meaningless as of now\n",
        "  # wor_masking_type = \"WOR_PAD\"\n",
        "  # wor_interp_style = \"WOR_BANZ_50\"\n",
        "\n",
        "  MAX_NEW_TOKENS = 10\n",
        "  extra_verbose=True\n",
        "  extra_verbose=False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  unchanging_context_interp_styles=[\"DOC_REMOVAL_100\"]\n",
        "\n",
        "  if doc_interp_style in unchanging_context_interp_styles:\n",
        "    first_layer_ss, tokens = sliced_process_documents_from_index_with_hierarchy(result,torch.ones(D),t)\n",
        "    L_ss = len(tokens)\n",
        "    input_tokens = torch.Tensor(tokens).long().to(\"cuda\")\n",
        "\n",
        "\n",
        "  list_of_perms=[]\n",
        "\n",
        "  start_time = time.time()\n",
        "  new_gen_list=[]\n",
        "  new_gen_list_list_wor=[]\n",
        "  new_conf_list_wor=[]\n",
        "\n",
        "\n",
        "  for s in range(S):\n",
        "    print('s',s)\n",
        "    if doc_interp_style==\"DOC_BANZ_50\":\n",
        "      conf = np.random.rand(D) > 0.5\n",
        "      list_of_perms.append(conf)\n",
        "    elif doc_interp_style==\"DOC_REMOVAL_100\":\n",
        "      conf = np.ones(D,dtype=bool)\n",
        "    elif doc_interp_style==\"DOC_SHAP\":\n",
        "      unif = np.random.rand()\n",
        "      conf = np.random.rand(D) > unif\n",
        "    else:\n",
        "      raise Exception(\"doc interp method not available\")\n",
        "    if extra_verbose:\n",
        "      print(conf)\n",
        "\n",
        "\n",
        "    sparsity_grid = torch.zeros((1,D),dtype=int)\n",
        "    s_to_check = -1\n",
        "    s_d = 0\n",
        "    for d in range(D):\n",
        "      if d==d_to_check:\n",
        "        sparsity_grid[:,d] = 1\n",
        "        s_to_check=s_d\n",
        "      if conf[d]:\n",
        "        sparsity_grid[:,d] = 1\n",
        "        s_d+=1\n",
        "    D2 = int(np.array(torch.sum(sparsity_grid)))\n",
        "\n",
        "\n",
        "    if doc_interp_style not in unchanging_context_interp_styles:\n",
        "      first_layer_ss, tokens = sliced_process_documents_from_index_with_hierarchy(result,sparsity_grid[0],t)\n",
        "      L_ss = len(tokens)\n",
        "      input_tokens = torch.Tensor(tokens).long().to(\"cuda\")\n",
        "\n",
        "\n",
        "    if True:\n",
        "      second_layer_ss = first_layer_ss[s_to_check][0]\n",
        "      local_dim = len(second_layer_ss.keys())\n",
        "      third_layer_ss = second_layer_ss[sen_to_check][0]\n",
        "      word_local_dim = len(third_layer_ss.keys())\n",
        "      if extra_verbose:\n",
        "        print('third_layer_ss',third_layer_ss)\n",
        "        print('local_dim',local_dim)\n",
        "        print('word_local_dim',word_local_dim)\n",
        "\n",
        "    sparsity2 = torch.zeros((word_local_dim+1,L_ss),dtype=int).to(\"cuda\")\n",
        "    sparsity2[:,:first_layer_ss[0][1][0]]   = 1 #prefix\n",
        "    sparsity2[:,first_layer_ss[D2-1][1][1]:] = 1 #suffix\n",
        "    if doc_masking_type==\"DOC_SLICE\":\n",
        "      for d in range(D2):\n",
        "        doc_span_tup_dd = first_layer_ss[d][1]\n",
        "        if d!=s_to_check: #include everything but the doc to check (already sliced)\n",
        "          sparsity2[:,doc_span_tup_dd[0]:doc_span_tup_dd[1]] = 1\n",
        "    elif doc_masking_type==\"DOC_PAD\":\n",
        "      for d in range(D2):\n",
        "        doc_span_tup_dd = first_layer_ss[d][1]\n",
        "        if conf[d] and d!=s_to_check: #only include certain docs (because padding), make sure not to include doc to check\n",
        "          sparsity2[:,doc_span_tup_dd[0]:doc_span_tup_dd[1]] = 1\n",
        "    else:\n",
        "      raise Exception(\"doc masking method not available\")\n",
        "\n",
        "\n",
        "    if True:\n",
        "      if sen_interp_style == \"SEN_BANZ_50\":\n",
        "        conf_ll = np.random.rand(local_dim) > 0.5\n",
        "      elif sen_interp_style == \"SEN_SHAP\":\n",
        "        unif_ll = np.random.rand()\n",
        "        conf_ll = np.random.rand(local_dim) > unif_ll\n",
        "      for l in range(local_dim):\n",
        "        doc_span_tup_dd = first_layer_ss[s_to_check][1]\n",
        "        doc_span_tup_dd_ll = second_layer_ss[l][1]\n",
        "        if conf_ll[l] and l!=sen_to_check:\n",
        "          sparsity2[:,doc_span_tup_dd[0]+doc_span_tup_dd_ll[0]:doc_span_tup_dd[0]+doc_span_tup_dd_ll[1]] = 1\n",
        "\n",
        "      # for l in range(local_dim):\n",
        "      #   doc_span_tup_dd = first_layer_ss[s_to_check][1]\n",
        "      #   doc_span_tup_dd_ll = second_layer[l][1]\n",
        "      #   if True: #bugfix...\n",
        "      #     sparsity2[l+1,doc_span_tup_dd[0]+doc_span_tup_dd_ll[0]:doc_span_tup_dd[0]+doc_span_tup_dd_ll[1]] = 1-sparsity2[l+1,doc_span_tup_dd[0]+doc_span_tup_dd_ll[0]:doc_span_tup_dd[0]+doc_span_tup_dd_ll[1]]\n",
        "\n",
        "\n",
        "      if sen_interp_style == \"SEN_BANZ_50\":\n",
        "        conf_ww = np.random.rand(word_local_dim) > 0.5\n",
        "      elif sen_interp_style == \"SEN_SHAP\":\n",
        "        unif_ww = np.random.rand()\n",
        "        conf_ww = np.random.rand(word_local_dim) > unif_ll\n",
        "      for w in range(word_local_dim):\n",
        "        third_layer_ss = second_layer_ss[sen_to_check][0]\n",
        "        doc_span_tup_dd = first_layer_ss[s_to_check][1]\n",
        "        doc_span_tup_dd_ll = second_layer_ss[sen_to_check][1]\n",
        "        doc_span_tup_dd_ll_ww = third_layer_ss[w][1]\n",
        "        if conf_ww[w]:\n",
        "          #why did I do it like this...\n",
        "          sparsity2[:,doc_span_tup_dd[0]+doc_span_tup_dd_ll[0]+doc_span_tup_dd_ll_ww[0]:doc_span_tup_dd[0]+doc_span_tup_dd_ll[0]+doc_span_tup_dd_ll_ww[1]] = 1\n",
        "\n",
        "      for w in range(word_local_dim):\n",
        "        third_layer_ss = second_layer_ss[sen_to_check][0]\n",
        "        doc_span_tup_dd = first_layer_ss[s_to_check][1]\n",
        "        doc_span_tup_dd_ll = second_layer_ss[sen_to_check][1]\n",
        "        doc_span_tup_dd_ll_ww = third_layer_ss[w][1]\n",
        "        if True:\n",
        "          sparsity2[w+1,doc_span_tup_dd[0]+doc_span_tup_dd_ll[0]+doc_span_tup_dd_ll_ww[0]:doc_span_tup_dd[0]+doc_span_tup_dd_ll[0]+doc_span_tup_dd_ll_ww[1]] = 1-sparsity2[w+1,doc_span_tup_dd[0]+doc_span_tup_dd_ll[0]+doc_span_tup_dd_ll_ww[0]:doc_span_tup_dd[0]+doc_span_tup_dd_ll[0]+doc_span_tup_dd_ll_ww[1]]\n",
        "\n",
        "\n",
        "      if False: #super verbose on word parsing\n",
        "        pass\n",
        "        patch = tokens[doc_span_tup_dd[0]+doc_span_tup_dd_ll[0]:doc_span_tup_dd[0]+doc_span_tup_dd_ll[1]]\n",
        "        for w in range(word_local_dim):\n",
        "          range_ = third_layer[w][1]\n",
        "          print(w,'\\t\\\"'+t.decode(patch[range_[0]:range_[1]])+'\\\"')\n",
        "\n",
        "\n",
        "      sparsity3 = sparsity2.clone()\n",
        "      prefix=first_layer_ss[s_to_check][1][0] + second_layer_ss[sen_to_check][1][0]\n",
        "      suffix=first_layer_ss[s_to_check][1][0] + second_layer_ss[sen_to_check][1][1]\n",
        "      sparsity3[:,prefix:suffix] = 1 - sparsity3[:,prefix:suffix]\n",
        "      ##print(sparsity3[:,prefix:suffix])\n",
        "\n",
        "    # conf_tup_1 = (conf,conf_ll,conf_ww)\n",
        "    # conf_tup_2 = (conf,conf_ll,~conf_ww)\n",
        "    conf_tup_1 = (list(conf),list(conf_ll),list(conf_ww))\n",
        "    conf_tup_2 = (list(conf),list(conf_ll),list(~conf_ww))\n",
        "    new_conf_list_wor.append(conf_tup_1)\n",
        "    new_conf_list_wor.append(conf_tup_2)\n",
        "\n",
        "\n",
        "\n",
        "    new_gen_list=[]\n",
        "    for ss in range(word_local_dim+1):\n",
        "      if extra_verbose:\n",
        "        print(ss)\n",
        "\n",
        "      if sen_masking_type==\"SEN_SLICE\": #v5,v4\n",
        "        input_ss = input_tokens[sparsity2[ss]==1][None]\n",
        "      elif sen_masking_type==\"SEN_PAD\": #v3\n",
        "        input_ss = input_tokens[None]*sparsity2[ss][None]\n",
        "      else:\n",
        "        raise Exception(\"sen masking method not available\")\n",
        "\n",
        "\n",
        "      generate_ids = inference_platform.model.generate(input_ss, max_new_tokens=MAX_NEW_TOKENS)\n",
        "      new_generate_ids = generate_ids[:,input_ss.shape[1]:]\n",
        "      new_gen_list.append(new_generate_ids.cpu())\n",
        "      if extra_verbose:\n",
        "        print( inference_platform.tokenizer.batch_decode(new_generate_ids, skip_special_tokens=True) )\n",
        "        #print(new_generate_ids)\n",
        "        print(time.time()-start_time)\n",
        "\n",
        "      if sen_masking_type==\"SEN_SLICE\":\n",
        "        input_ss = input_tokens[sparsity3[ss]==1][None]\n",
        "      elif sen_masking_type==\"SEN_PAD\":\n",
        "        input_ss = input_tokens[None]*sparsity3[ss][None]\n",
        "      else:\n",
        "        raise Exception(\"sen masking method not available\")\n",
        "\n",
        "      generate_ids = inference_platform.model.generate(input_ss, max_new_tokens=MAX_NEW_TOKENS)\n",
        "      new_generate_ids = generate_ids[:,input_ss.shape[1]:]\n",
        "      new_gen_list.append(new_generate_ids.cpu())\n",
        "      if extra_verbose:\n",
        "        print( inference_platform.tokenizer.batch_decode(new_generate_ids, skip_special_tokens=True) )\n",
        "        #print(new_generate_ids)\n",
        "        print(time.time()-start_time)\n",
        "        print()\n",
        "    new_gen_list_list_wor.append(new_gen_list)\n",
        "  print(time.time()-start_time)\n",
        "  pass\n",
        "  # return new_gen_list_list_wor\n",
        "  return new_gen_list_list_wor,new_conf_list_wor\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2MuK-KCiDiJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Noid9fVITOCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## using sunflowers to track banzhaf changes"
      ],
      "metadata": {
        "id": "6Yd307SH8dPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_conjugate_to_sunflower(new_gen_list_list, new_conf_list=None, root=None):\n",
        "  S=len(new_gen_list_list)\n",
        "  D_loc = len(new_gen_list_list[0])//2 - 1\n",
        "  #print('S',S,'D_loc',D_loc)\n",
        "\n",
        "  if new_conf_list is None:\n",
        "    new_conf_list = [None]*S\n",
        "\n",
        "  sunflower_list = []\n",
        "  for ss in range(S):\n",
        "    pass\n",
        "\n",
        "    new_gen_list = new_gen_list_list[ss]\n",
        "\n",
        "    for parity in range(2): #both conjugate pairs\n",
        "      sunflower = {}\n",
        "      sunflower['size'] = D_loc\n",
        "      sunflower['conferences'] = new_conf_list[ss]\n",
        "      for d in range(-1,D_loc):\n",
        "        batch_ind = 0\n",
        "        out_tok_list = list(new_gen_list[d*2+2+parity].cpu().numpy()[batch_ind])\n",
        "        sunflower[d] = out_tok_list\n",
        "      sunflower_list.append( sunflower )\n",
        "\n",
        "\n",
        "  sunflower_object = {\n",
        "      'root'  : root,\n",
        "      'D_loc' : D_loc,\n",
        "      'S_loc' : 2*S,\n",
        "      'sunflower_list' : sunflower_list,\n",
        "  }\n",
        "  return sunflower_object"
      ],
      "metadata": {
        "id": "KHrT3mBMQlTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scores_from_sunflower(sunflower_obj):\n",
        "  D_loc = sunflower_obj['D_loc']\n",
        "  S_loc = sunflower_obj['S_loc']\n",
        "\n",
        "  scores_arr = np.zeros((D_loc,4),dtype=int)\n",
        "\n",
        "  for ss in range(S_loc):\n",
        "    sunflower = sunflower_obj['sunflower_list'][ss]\n",
        "    base = sunflower[-1]\n",
        "    for dl in range(D_loc):\n",
        "      new = sunflower[dl]\n",
        "\n",
        "      #print(base)\n",
        "      #print(new)\n",
        "\n",
        "      if base!=new: #match full list\n",
        "        scores_arr[dl,0]+=1\n",
        "\n",
        "      if base[0]!=new[0]: #match first token\n",
        "        scores_arr[dl,1]+=1\n",
        "\n",
        "  return scores_arr\n"
      ],
      "metadata": {
        "id": "yZQNSgCJ8pCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def sunflower_collection_to_saveable_json(sunflower_collection):\n",
        "#   pass\n"
      ],
      "metadata": {
        "id": "lN1X6nv0Zfoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accumulate_potential_outputs(sunflower_collection, root_list=None):\n",
        "  if root_list is None:\n",
        "    root_list=list(sunflower_collection.keys())\n",
        "\n",
        "  list_of_tok_lists = []\n",
        "  dict_of_tok_lists = {}\n",
        "  for root in root_list:\n",
        "    sunflower = sunflower_collection[root]\n",
        "\n",
        "    for sample in sunflower['sunflower_list']:\n",
        "      D_loc = sample['size']\n",
        "      for d in range(-1,D_loc):\n",
        "        tok_list = sample[d]\n",
        "        if tok_list not in list_of_tok_lists:\n",
        "          list_of_tok_lists.append( tok_list )\n",
        "          dict_of_tok_lists[tuple(tok_list)] = 1\n",
        "        else:\n",
        "          dict_of_tok_lists[tuple(tok_list)] += 1\n",
        "\n",
        "  return list_of_tok_lists,dict_of_tok_lists"
      ],
      "metadata": {
        "id": "I0BRpLejiGfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def accumulate_scores_at_specified_root(sunflower_collection, root, list_of_tok_lists):\n",
        "  sunflower_obj = sunflower_collection[root]\n",
        "  LTL = len(list_of_tok_lists)\n",
        "  D_loc = sunflower_obj['D_loc']\n",
        "  S_loc = sunflower_obj['S_loc']\n",
        "  print('D_loc',D_loc)\n",
        "  print('S_loc',S_loc)\n",
        "\n",
        "  scores_tens = np.zeros( (D_loc,LTL,4),dtype=int )\n",
        "  #print('scores_tens',scores_tens.shape)\n",
        "  for ss in range(S_loc):\n",
        "    sunflower = sunflower_obj['sunflower_list'][ss]\n",
        "    #print(sunflower)\n",
        "    conf = sunflower['conferences'][-1]\n",
        "\n",
        "    # base = sunflower[-1]\n",
        "    base = copy.copy(sunflower[-1])\n",
        "    for d in range(D_loc):\n",
        "      # new = sunflower[d]\n",
        "      new = copy.copy(sunflower[d])\n",
        "      # print('\\t',d,base,new)\n",
        "\n",
        "      # if conf[d]: #switch order for inclusion\n",
        "      #   # temp=base;base=new;new=temp; #BUG...\n",
        "      #   temp=copy.copy(base);base=copy.copy(new);new=copy.copy(temp);\n",
        "\n",
        "      # if base!=new: #match full list\n",
        "      #   new_ind = list_of_tok_lists.index(new)\n",
        "      #   scores_tens[d,new_ind,0]+=1\n",
        "\n",
        "      # if base[0]!=new[0]: #match first token\n",
        "      #   new_ind = list_of_tok_lists.index(new)\n",
        "      #   scores_tens[d,new_ind,1]+=1\n",
        "\n",
        "\n",
        "      #did this version finally fix it?\n",
        "      if base!=new: #match full list\n",
        "        new_ind = list_of_tok_lists.index(new)\n",
        "        if conf[d]:\n",
        "          new_ind = list_of_tok_lists.index(base)\n",
        "        scores_tens[d,new_ind,0]+=1\n",
        "\n",
        "      if base[0]!=new[0]: #match first token\n",
        "        new_ind = list_of_tok_lists.index(new)\n",
        "        if conf[d]:\n",
        "          new_ind = list_of_tok_lists.index(base)\n",
        "        scores_tens[d,new_ind,1]+=1\n",
        "\n",
        "\n",
        "  # print(np.sum(scores_tens,axis=1))\n",
        "\n",
        "  # return scores_tens\n",
        "  return scores_tens.astype(float)/S_loc\n"
      ],
      "metadata": {
        "id": "VYooNAAgZfug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_first_token_representative(list_of_tok_lists):\n",
        "  repn_list_of_tok_lists_first = []\n",
        "  repn_list_of_tok_lists = []\n",
        "  remap_dict = {}\n",
        "\n",
        "  for tt,tok_list in enumerate(list_of_tok_lists):\n",
        "    tok=tok_list[0]\n",
        "    if tok in repn_list_of_tok_lists_first:\n",
        "      remap_dict[tt] = repn_list_of_tok_lists_first.index(tok)\n",
        "    else:\n",
        "      repn_list_of_tok_lists_first.append(tok)\n",
        "      repn_list_of_tok_lists.append(tok_list)\n",
        "      remap_dict[tt] = repn_list_of_tok_lists_first.index(tok)\n",
        "\n",
        "  LTL = len(list_of_tok_lists)\n",
        "  RTL = len(repn_list_of_tok_lists)\n",
        "  remap_array = np.zeros((LTL,RTL),dtype=int)\n",
        "  for tt in range(LTL):\n",
        "    remap_array[tt,remap_dict[tt]] = 1\n",
        "\n",
        "  return repn_list_of_tok_lists_first, repn_list_of_tok_lists, remap_dict, remap_array\n"
      ],
      "metadata": {
        "id": "PEl3DoshiKEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extra_verbose = True\n",
        "# extra_verbose = False\n",
        "\n",
        "\n",
        "def convert_sunflower_collection_to_visualizable_json(sunflower_collection, first_layer, question, score_index=0):\n",
        "  list_of_tok_lists,dict_of_tok_lists = accumulate_potential_outputs(sunflower_collection)\n",
        "  inds = np.argsort( -np.array(list(dict_of_tok_lists.values())) )\n",
        "  sorted_list_of_out_strings = ([t.decode(list_of_tok_lists[i]) for i in inds])\n",
        "  sorted_list_of_tok_lists = ([(list_of_tok_lists[i]) for i in inds])\n",
        "  print('sorted_list_of_out_strings',)\n",
        "  print(sorted_list_of_out_strings)\n",
        "\n",
        "  repn_list_of_tok_lists_first, repn_list_of_tok_lists, remap_dict, remap_array = get_first_token_representative(sorted_list_of_tok_lists)\n",
        "  repn_sorted_list_of_out_strings = ([t.decode(thing) for thing in repn_list_of_tok_lists_first])\n",
        "  repn_sorted_list_of_out_strings = ([t.decode(thing) for thing in repn_list_of_tok_lists])\n",
        "  print('repn_sorted_list_of_out_strings',)\n",
        "  print(repn_sorted_list_of_out_strings)\n",
        "  if False:\n",
        "    print('remap_array',remap_array.shape)\n",
        "    print(remap_array)\n",
        "\n",
        "  full_json = {\n",
        "      \"children\"     : [],\n",
        "      # \"text_outputs\" : sorted_list_of_out_strings,\n",
        "      # \"text_outputs\" : repn_sorted_list_of_out_strings,\n",
        "      # \"question\"     : \"who got the first nobel prize in physics\",\n",
        "      \"question\"     : question,\n",
        "  }\n",
        "  if USING_REPN_STRINGS:\n",
        "    full_json[\"text_outputs\"] = repn_sorted_list_of_out_strings\n",
        "  else:\n",
        "    full_json[\"text_outputs\"] = sorted_list_of_out_strings\n",
        "\n",
        "  D_0 = len(first_layer.keys())\n",
        "  root_list = []\n",
        "  # print(D_0)\n",
        "\n",
        "  scores_tens_doc = accumulate_scores_at_specified_root(sunflower_collection, (), sorted_list_of_tok_lists)\n",
        "  # score_index = 1; #match first token only\n",
        "  # score_index = 0; #any change across the seqeunce\n",
        "  # print(np.matmul(scores_tens_doc[:,:,score_index],remap_array))\n",
        "  # print(np.matmul(scores_tens_doc[0,:,score_index],remap_array))\n",
        "  if USING_REPN_STRINGS:\n",
        "    scores_tens_doc = np.tensordot(scores_tens_doc,remap_array,axes=(1,0))\n",
        "\n",
        "  # print(np.sum(scores_tens_doc,axis=1))\n",
        "\n",
        "  for d_0 in range(D_0):\n",
        "    # print('\\t',d_0)\n",
        "    doc_span_tup_dd = first_layer[d_0][1]\n",
        "    text_d0 = t.decode(tokens[doc_span_tup_dd[0]:doc_span_tup_dd[1]])\n",
        "\n",
        "    ####scores_d0 = np.matmul(scores_tens_doc[d,:,score_index],remap_array) #'d'='d_0' was the bug... really?\n",
        "    # scores_d0 = np.matmul(scores_tens_doc[d_0,:,score_index],remap_array)\n",
        "    scores_d0 = scores_tens_doc[d_0,:,score_index]\n",
        "    # print('scores_d0',scores_d0)\n",
        "\n",
        "    doc_dict = {\n",
        "        \"index1\" : d_0,\n",
        "        \"text\" : text_d0,\n",
        "        \"children\" : [],\n",
        "        \"layer1_tspans\": [],\n",
        "        # \"scores\" : list(scores_tens_doc[d,:,score_index]),\n",
        "        # \"scores\" : str(list(scores_tens_doc[d,:,score_index])),\n",
        "        \"scores\" : str(list(scores_d0)),\n",
        "    }\n",
        "    full_json[\"children\"].append( doc_dict ) #TODO\n",
        "\n",
        "\n",
        "    root_0 = (d_0,)\n",
        "    if root_0 in sunflower_collection:\n",
        "      second_layer = first_layer[d_0][0]\n",
        "      pass\n",
        "\n",
        "      scores_tens_sen = accumulate_scores_at_specified_root(sunflower_collection, root_0, sorted_list_of_tok_lists)\n",
        "      if USING_REPN_STRINGS:\n",
        "        scores_tens_sen = np.tensordot(scores_tens_sen,remap_array,axes=(1,0))\n",
        "      D_1 = len(second_layer.keys())\n",
        "      for d_1 in range(D_1):\n",
        "        # print('\\t\\t',d_1)\n",
        "        doc_span_tup_dd_ll = second_layer[d_1][1]\n",
        "        text_d1 = t.decode(tokens[doc_span_tup_dd[0]+doc_span_tup_dd_ll[0]:doc_span_tup_dd[0]+doc_span_tup_dd_ll[1]])\n",
        "\n",
        "        # scores_d1 = np.matmul(scores_tens_sen[d_1,:,score_index],remap_array)\n",
        "        scores_d1 = scores_tens_sen[d_1,:,score_index]\n",
        "        # print('scores_d1',scores_d1)\n",
        "\n",
        "        sen_dict = {\n",
        "            \"index1\" : d_0,\n",
        "            \"index2\" : d_1,\n",
        "            \"text\" : text_d1,\n",
        "            \"children\" : [],\n",
        "            \"scores\" : str(list(scores_d1)),\n",
        "        }\n",
        "        full_json[\"children\"][d_0][\"children\"].append( sen_dict )\n",
        "\n",
        "\n",
        "        root_1 = (d_0,d_1)\n",
        "        if root_1 in sunflower_collection:\n",
        "          third_layer = second_layer[d_1][0]\n",
        "          pass\n",
        "\n",
        "          scores_tens_wor = accumulate_scores_at_specified_root(sunflower_collection, root_1, sorted_list_of_tok_lists)\n",
        "          if USING_REPN_STRINGS:\n",
        "            scores_tens_wor = np.tensordot(scores_tens_wor,remap_array,axes=(1,0))\n",
        "          D_2 = len(third_layer.keys())\n",
        "          for d_2 in range(D_2):\n",
        "            # print('\\t\\t\\t',d_2)\n",
        "            doc_span_tup_dd_ll_ww = third_layer[d_2][1]\n",
        "            text_d2 = t.decode(tokens[doc_span_tup_dd[0]+doc_span_tup_dd_ll[0]+doc_span_tup_dd_ll_ww[0]:doc_span_tup_dd[0]+doc_span_tup_dd_ll[0]+doc_span_tup_dd_ll_ww[1]])\n",
        "\n",
        "            #scores_d2 = np.matmul(scores_tens_wor[d_2,:,score_index],remap_array)\n",
        "            scores_d2 = scores_tens_wor[d_2,:,score_index]\n",
        "            # print('scores_d2',scores_d2)\n",
        "\n",
        "            wor_dict = {\n",
        "                \"index1\" : d_0,\n",
        "                \"index2\" : d_1,\n",
        "                \"index3\" : d_2,\n",
        "                \"text\" : text_d2,\n",
        "                \"children\" : [],\n",
        "                \"scores\" : str(list(scores_d2)),\n",
        "            }\n",
        "            full_json[\"children\"][d_0][\"children\"][d_1][\"children\"].append( wor_dict )\n",
        "\n",
        "  pass\n",
        "  return full_json\n"
      ],
      "metadata": {
        "id": "YCkAWLygiKKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ow3VwrTah1na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eba6WlFBSzsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for query_id in range(100):\n",
        "  json_str = json_list[query_id]\n",
        "  result = json.loads(json_str)\n",
        "  question = result[\"question\"]\n",
        "  print(query_id,'\\t',question+\"?\")\n",
        "  print()\n"
      ],
      "metadata": {
        "id": "UWHjOaBwSzun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## specific instance of doing the interpretation and saving"
      ],
      "metadata": {
        "id": "a2sU36ZAh2ad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "interp_session_id = str(uuid.uuid4()).replace('-','_')\n",
        "print(interp_session_id)\n",
        "print()\n",
        "\n",
        "\n",
        "query_id = 0  # index of query within the file\n",
        "# query_id = 1\n",
        "# query_id = 2\n",
        "# query_id = 3\n",
        "# query_id = 4\n",
        "query_id = 18\n",
        "json_str = json_list[query_id]\n",
        "result = json.loads(json_str)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "DOC_MASK   = \"DOC_SLICE\"    #'DOC_SLICE' or 'DOC_PAD'\n",
        "SEN_MASK   = \"SEN_SLICE\"    #'SEN_SLICE' or 'SEN_PAD'\n",
        "\n",
        "DOC_INTERP = \"DOC_BANZ_50\"\n",
        "DOC_INTERP = \"DOC_SHAP\"\n",
        "SEN_INTERP = \"SEN_BANZ_50\"\n",
        "SEN_INTERP = \"SEN_SHAP\"\n",
        "\n",
        "\n",
        "#DOC_SAMPLES = 10; SEN_SAMPLES = 5; WOR_SAMPLES = 5;\n",
        "DOC_SAMPLES = 10; SEN_SAMPLES = 10; WOR_SAMPLES = 10;\n",
        "TOP_SENT = 3\n",
        "TOP_WORD = 2\n",
        "\n",
        "\n",
        "USING_REPN_STRINGS = True\n",
        "# USING_REPN_STRINGS = False\n",
        "\n",
        "ACCUMULATE_DIFF = \"different_full_generation\" #different anywhere\n",
        "# ACCUMULATE_DIFF = \"different_first_token\"     #already different at front\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ctx = result[\"ctxs\"]\n",
        "question = result[\"question\"]\n",
        "answers = result[\"answers\"]\n",
        "print(\"question\", question)\n",
        "print(\"answers\", answers)\n",
        "\n",
        "\n",
        "D=10\n",
        "first_layer,tokens = sliced_process_documents_from_index_with_hierarchy(result,torch.ones(D),t)\n",
        "print(t.decode(tokens))\n",
        "JSON_PATH = \"gdrive/My Drive/Colab Notebooks/\" + \"textgenshap-data-json/\" + interp_session_id + '_data.json'"
      ],
      "metadata": {
        "id": "b-27E6FlCZbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_index=0\n",
        "if ACCUMULATE_DIFF==\"different_first_token\":\n",
        "  score_index=1\n",
        "\n",
        "def save_json_constructor():\n",
        "  def save_json_now(sunflower_collection):\n",
        "\n",
        "    full_json = convert_sunflower_collection_to_visualizable_json(sunflower_collection,first_layer,question,score_index)\n",
        "\n",
        "    print(full_json[\"text_outputs\"])\n",
        "    print('full_json')\n",
        "    print(full_json)\n",
        "    with open(JSON_PATH, 'w', encoding='utf-8') as f:\n",
        "        json.dump(full_json, f, indent=4)\n",
        "\n",
        "  return save_json_now\n",
        "\n",
        "save_json_now = save_json_constructor()"
      ],
      "metadata": {
        "id": "QiRgX1V7YGjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lLPag3faiiJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sunflower_collection = {}\n",
        "\n",
        "\n",
        "if False: #dont do this for now because I am not visualizing it anyways\n",
        "  new_gen_list_list_doc_inc, _ = document_level_interpretability_steps(result,S=1,doc_masking_type=\"DOC_SLICE\",doc_interp_style=\"DOC_INCLUSION_0\")\n",
        "  new_gen_list_list_doc_rem, _ = document_level_interpretability_steps(result,S=1,doc_masking_type=\"DOC_SLICE\",doc_interp_style=\"DOC_REMOVAL_100\")\n",
        "\n",
        "  sunflower_obj_inc = convert_conjugate_to_sunflower(new_gen_list_list_doc_inc)\n",
        "  sunflower_collection['inc'] = sunflower_obj_inc\n",
        "  sunflower_obj_rem = convert_conjugate_to_sunflower(new_gen_list_list_doc_rem)\n",
        "  sunflower_collection['rem'] = sunflower_obj_rem\n",
        "\n",
        "\n",
        "\n",
        "# new_gen_list_list_doc_1, conf_list_doc = document_level_interpretability_steps(result,doc_masking_type=\"DOC_SLICE\")\n",
        "new_gen_list_list_doc_1, conf_list_doc = document_level_interpretability_steps(result,S=DOC_SAMPLES,doc_masking_type=DOC_MASK,doc_interp_style=DOC_INTERP)\n",
        "root=()\n",
        "sunflower_obj_doc = convert_conjugate_to_sunflower(new_gen_list_list_doc_1,conf_list_doc,root=root)\n",
        "sunflower_collection[root] = sunflower_obj_doc\n",
        "save_json_now(sunflower_collection)\n",
        "\n",
        "\n",
        "\n",
        "#START THE DFS\n",
        "scores_arr = scores_from_sunflower(sunflower_obj_doc)\n",
        "D=10\n",
        "for d in range(D):\n",
        "  score_allToks=scores_arr[d,0]\n",
        "  score_firstTok=scores_arr[d,1]\n",
        "  print('d',d,'\\t',score_firstTok,'  \\t',score_allToks)\n",
        "print()\n",
        "print()\n",
        "\n",
        "\n",
        "list_of_tok_lists,dict_of_tok_lists = accumulate_potential_outputs(sunflower_collection)\n",
        "inds = np.argsort( -np.array(list(dict_of_tok_lists.values())) )\n",
        "sorted_list_of_out_strings = ([t.decode(list_of_tok_lists[i]) for i in inds])\n",
        "sorted_list_of_tok_lists = ([(list_of_tok_lists[i]) for i in inds])\n",
        "scores_tens_doc = accumulate_scores_at_specified_root(sunflower_collection, (), sorted_list_of_tok_lists)\n",
        "scores_arr = np.sum(scores_tens_doc,axis=1)\n",
        "for d in range(D):\n",
        "  score_allToks=scores_arr[d,0]\n",
        "  score_firstTok=scores_arr[d,1]\n",
        "  print('d',d,'\\t',score_firstTok,'  \\t',score_allToks)\n",
        "print()\n",
        "print()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "queue = np.argsort(-scores_arr[:,1])[:TOP_SENT]\n",
        "print('queue',queue)\n",
        "queue=list(queue)\n",
        "\n",
        "\n",
        "\n",
        "while len(queue)>0:\n",
        "  next_doc = queue.pop(0)\n",
        "  print('NEXT_DOC',next_doc)\n",
        "\n",
        "  # new_gen_list_list_sen,new_conf_list_sen = sentence_level_interpretability_steps(result,d_to_check=next_doc,\n",
        "  #                                                                                 doc_interp_style=\"DOC_BANZ_50\",sen_masking_type=\"SEN_SLICE\")\n",
        "  new_gen_list_list_sen,new_conf_list_sen = sentence_level_interpretability_steps(result,S=SEN_SAMPLES,d_to_check=next_doc,\n",
        "                                                                                  doc_masking_type=DOC_MASK,doc_interp_style=DOC_INTERP,\n",
        "                                                                                  sen_masking_type=SEN_MASK,sen_interp_style=SEN_INTERP)\n",
        "\n",
        "\n",
        "  root = (next_doc,)\n",
        "  sunflower_obj_sen = convert_conjugate_to_sunflower(new_gen_list_list_sen, new_conf_list_sen, root=root)\n",
        "  sunflower_collection[root] = sunflower_obj_sen    #(or extend existing list)\n",
        "  save_json_now(sunflower_collection)\n",
        "\n",
        "\n",
        "  scores_arr_sen = scores_from_sunflower(sunflower_obj_sen)\n",
        "  sen_queue = np.argsort(-scores_arr_sen[:,1])[:TOP_WORD]\n",
        "  print('sen_queue',sen_queue)\n",
        "  sen_queue=list(sen_queue)\n",
        "\n",
        "\n",
        "  while len(sen_queue)>0:\n",
        "    next_sen = sen_queue.pop(0)\n",
        "    print('NEXT_SEN',next_sen)\n",
        "\n",
        "\n",
        "    # new_gen_list_list_wor,new_conf_list_wor = word_level_interpretability_steps(result,d_to_check=next_doc,sen_to_check=next_sen,\n",
        "    #                                                                                 doc_interp_style=\"DOC_BANZ_50\",sen_masking_type=\"SEN_SLICE\")\n",
        "    new_gen_list_list_wor,new_conf_list_wor = word_level_interpretability_steps(result,S=WOR_SAMPLES,d_to_check=next_doc,sen_to_check=next_sen,\n",
        "                                                                                    doc_masking_type=DOC_MASK,doc_interp_style=DOC_INTERP,\n",
        "                                                                                    sen_masking_type=SEN_MASK,sen_interp_style=SEN_INTERP)\n",
        "    #print('new_gen_list_list_wor',len(new_gen_list_list_wor))\n",
        "    #print('new_gen_list_list_wor[0]',len(new_gen_list_list_wor[0]))\n",
        "    #print('funny?',[len(thing) for thing in new_gen_list_list_wor])\n",
        "    #print('new_conf_list_wor',len(new_conf_list_wor))\n",
        "    pass\n",
        "    root = (next_doc,next_sen)\n",
        "    sunflower_obj_wor = convert_conjugate_to_sunflower(new_gen_list_list_wor, new_conf_list_wor, root=root)\n",
        "    sunflower_collection[root] = sunflower_obj_wor\n",
        "    save_json_now(sunflower_collection)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0J3UY0d2CZfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for root in sunflower_collection:\n",
        "  print('root',root)\n",
        "  print(sunflower_collection[root])\n",
        "  print()"
      ],
      "metadata": {
        "id": "JBc9ydXyXG4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2lanT-5jvfi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JCRpWkOJ3bjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cql-nQGFzv8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "smXroSxuN8YY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rTs8p_E7N8f_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
