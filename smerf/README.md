# SMERF: Real-Time View Synthesis for Large Scenes

> __SMERF: Streamable Memory Efficient Radiance Fields for Real-Time Large-Scene Exploration__
> Daniel Duckworth\*, Peter Hedman\*, Christian Reiser, Peter Zhizhin, Jean-François Thibert, Mario Lučić, Richard Szeliski, Jonathan T. Barron
> _ACM Transactions on Graphics (SIGGRAPH), July 2024_
> __[Project page](https://smerf-3d.github.io/) / [Paper](https://arxiv.org/abs/2312.07541) / [Video](https://www.youtube.com/watch?v=zhO8iUBpnCc&feature=youtu.be) / [Demos](https://smerf-3d.github.io/#demos) / [Datasets & Teacher Checkpoints](https://smerf-3d.github.io/#data)__


This repository contains code for SMERF, a real-time approach for radiance
fields of large indoor & outdoor spaces. See the [project
website](https://smerf-3d.github.io/) for more details.

## Installation

1. Install NVIDIA drivers, CUDA, and cuDNN. The instructions for this are
   platform-specific. The codebase is verified working on driver version
   525.147.05, CUDA 12.3, and cuDNN 8.9. Support for other versions is not
   guaranteed.

2. Install [miniconda](https://docs.anaconda.com/free/miniconda/miniconda-install/).

3. Create a conda environment with Python 3.11.

```
conda create --name smerf-env python=3.11
conda activate smerf-env
```

4. Install JAX with GPU support.

```
python3 -m pip install --upgrade "jax[cuda12_pip]==0.4.23" \
  -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
```

5. Install `smerf` locally,

```
git clone https://github.com/google-research/google-research.git
cd google-research/smerf
python3 -m pip install -e .
```

## Training

1. Download the mip-NeRF 360 and Zip-NeRF datasets. Unzip their contents like
   so,

    ```
    datasets/
      bicycle/
        images/     # Photos
        images_2/
        images_4/
        images_8/
        sparse/0/   # COLMAP camera parameters
      ...
    ```

2. Download teacher checkpoints (Coming soon!). Unzip their contents like so,

    ```
    teachers/
      bicycle/
        checkpoint_50000/   # Model checkpoint
        config.gin          # Gin config
      ...
    ```

3. Run the SMERF pipeline, which includes distillation, baking, and evaluation.

    ```
    ./scripts/demo.sh         # Train a small model on a single, local GPU.
    ./scripts/mipnerf360.sh   # Train models on all mip-NeRF 360 scenes.
    ./scripts/zipnerf.sh      # Train models on all Zip-NeRF scenes.
    ```

## Web Viewer

The webviewer is designed to work with the baked assets generated by the
training SMERF training pipeline. At the end of baking, you will find baked
assets for an experiment stored under `${CHECKPOINT_DIR}/baked` folder. Each
submodel is assigned its own subdirectory, e.g.
`${CHECKPOINT_DIR}/baked/sm_000`. In general, mip-NeRF 360 scenes have exactly
one submodel and Zip-NeRF scenes have 10-20.

To use the webviewer, use the following instructions,

1. Copy baked assets to a directory under `webviewer/`. For example, if
   `${CHECKPOINT_DIR}` contains the root directory of a trained model of the
   `bicycle` scene, then its baked assets can be symbolically linked like so,

    ```
    mkdir -p webviewer/bicycle
    ln -s ${CHECKPOINT_DIR}/baked webviewer/bicycle
    ```

2. Launch the webviewer using the bash script in the same directory. Additional
   options can be specified via environment variables.

    ```
    BAKED_SCENE_DIR="bicycle" \
      ./webviewer/launch_webserver.sh
    ```

