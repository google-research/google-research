{
  "cells": [
    {
      "metadata": {
        "id": "SY7vcrsHXgPv"
      },
      "cell_type": "code",
      "source": [
        "# Copyright 2022 Google LLC\n",
        "\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmt-QvUjmLQ2"
      },
      "outputs": [],
      "source": [
        "import dataclasses\n",
        "import os\n",
        "import tensorflow.compat.v1 as tf\n",
        "import itertools\n",
        "import json\n",
        "from typing import Collection, Mapping\n",
        "\n",
        "from IPython import display\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wL4RQkUzlY-u"
      },
      "outputs": [],
      "source": [
        "PROJECT_ROOT = '/your/project/folder/here'\n",
        "SUBDIR = 'your_subdir'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8WR9dqctjKW"
      },
      "outputs": [],
      "source": [
        "MODELS = [\n",
        "    'dyrep',\n",
        "    'dyrep_structmap',\n",
        "    'edgebank',\n",
        "    'jodie',\n",
        "    'jodie_structmap',\n",
        "    'tgn',\n",
        "    'tgn_structmap',\n",
        "]\n",
        "DATA = [\n",
        "    'tgbl_wiki;cc-subgraph;cc-subgraph;cc-subgraph',\n",
        "    'tgbl_review;cc-subgraph;cc-subgraph;cc-subgraph',\n",
        "    'tgbl_flight;AS;AS;AF',\n",
        "    'tgbl_comment;cc-subgraph;cc-subgraph;cc-subgraph',\n",
        "    'tgbl_coin;cc-subgraph;cc-subgraph;cc-subgraph',\n",
        "]\n",
        "EXPERIMENTS = [\n",
        "    'transductive',\n",
        "    'transfer_no_warmstart',\n",
        "    'transfer_warmstart',\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOcfnLo51XCU"
      },
      "outputs": [],
      "source": [
        "# Transform inputs.\n",
        "@dataclasses.dataclass(frozen=False)\n",
        "class DatasetSpec:\n",
        "  dataset: str = dataclasses.field(default_factory=str)\n",
        "  train_split: str = dataclasses.field(default_factory=str)\n",
        "  val_split: str = dataclasses.field(default_factory=str)\n",
        "  test_split: str = dataclasses.field(default_factory=str)\n",
        "\n",
        "@dataclasses.dataclass(frozen=False)\n",
        "class ExperimentResults:\n",
        "  experiment: str = dataclasses.field(default_factory=str)\n",
        "  train_results: dict[str, float] = dataclasses.field(default_factory=dict)\n",
        "  test_results: dict[str, float] = dataclasses.field(default_factory=dict)\n",
        "  val_warmstart_loss_metrics: pd.DataFrame = dataclasses.field(default_factory=pd.DataFrame)\n",
        "  val_loss_metrics: pd.DataFrame = dataclasses.field(default_factory=pd.DataFrame)\n",
        "  test_warmstart_loss_metrics: pd.DataFrame = dataclasses.field(default_factory=pd.DataFrame)\n",
        "  test_loss_metrics: pd.DataFrame = dataclasses.field(default_factory=pd.DataFrame)\n",
        "\n",
        "@dataclasses.dataclass(frozen=False)\n",
        "class ModelResults:\n",
        "  model: str = dataclasses.field(default_factory=str)\n",
        "  experiment_results: dict[str, ExperimentResults] = dataclasses.field(default_factory=dict)\n",
        "\n",
        "@dataclasses.dataclass(frozen=False)\n",
        "class DatasetResults:\n",
        "  dataset: str = dataclasses.field(default_factory=str)\n",
        "  model_results: dict[str, ModelResults] = dataclasses.field(default_factory=dict)\n",
        "\n",
        "DATASET_SPECS = []\n",
        "DATASETS = []\n",
        "for dataset_string in DATA:\n",
        "  dataset, train_split, val_split, test_split = dataset_string.split(';')\n",
        "  DATASETS.append(dataset)\n",
        "  DATASET_SPECS.append(DatasetSpec(dataset, train_split, val_split, test_split))\n",
        "\n",
        "RESULTS_SUBDIR = os.path.join(PROJECT_ROOT, 'experiments', SUBDIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jnhONMomItq"
      },
      "outputs": [],
      "source": [
        "ALL_RESULTS = {}\n",
        "train_results_df = pd.DataFrame(\n",
        "    index=pd.MultiIndex.from_product(\n",
        "        [MODELS, EXPERIMENTS],\n",
        "        names=['Model', 'Experiment']\n",
        "    ),\n",
        "    columns=pd.MultiIndex.from_product(\n",
        "        [DATASETS, ['auc', 'mrr']],\n",
        "        names=['Dataset', 'Metric']\n",
        "    )\n",
        ")\n",
        "test_results_df = pd.DataFrame(\n",
        "    index=pd.MultiIndex.from_product(\n",
        "        [MODELS, EXPERIMENTS],\n",
        "        names=['Model', 'Experiment']\n",
        "    ),\n",
        "    columns=pd.MultiIndex.from_product(\n",
        "        [DATASETS, ['auc', 'mrr']],\n",
        "        names=['Dataset', 'Metric']\n",
        "    )\n",
        ")\n",
        "for dataset_spec in DATASET_SPECS:\n",
        "  dataset_results = DatasetResults(dataset=dataset_spec.dataset)\n",
        "  for model in MODELS:\n",
        "    model_results = ModelResults(model=model)\n",
        "    model_dataset_folder = os.path.join(\n",
        "        RESULTS_SUBDIR,\n",
        "        dataset_spec.dataset, 'results',\n",
        "        f'{model}_{dataset_spec.dataset}_{dataset_spec.train_split}_{dataset_spec.val_split}'\n",
        "    )\n",
        "    for experiment in EXPERIMENTS:\n",
        "\n",
        "      # Extract results for train.\n",
        "      experiment_results = ExperimentResults(experiment=experiment)\n",
        "      with tf.io.gfile.GFile(os.path.join(model_dataset_folder, f'{experiment}_results_train.json'), 'r') as f:\n",
        "        experiment_results.train_results = json.load(f)\n",
        "      with tf.io.gfile.GFile(os.path.join(model_dataset_folder, f'{experiment}_val_loss.csv'), 'r') as f:\n",
        "        experiment_results.val_loss_metrics = pd.read_csv(f)\n",
        "      if not 'no_warmstart' in experiment:\n",
        "        with tf.io.gfile.GFile(os.path.join(model_dataset_folder, f'{experiment}_val_warmstart_loss.csv'), 'r') as f:\n",
        "          experiment_results.val_warmstart_loss_metrics = pd.read_csv(f)\n",
        "      train_results_df.loc[model, experiment].at[dataset_spec.dataset, 'auc'] = experiment_results.train_results['auc']\n",
        "      train_results_df.loc[model, experiment].at[dataset_spec.dataset, 'mrr'] = experiment_results.train_results['val mrr']\n",
        "\n",
        "      # Extract results for test.\n",
        "      # TODO: save test_split-specific results in their own subfolder of the train/val split folder.\n",
        "      with tf.io.gfile.GFile(os.path.join(model_dataset_folder, f'{experiment}_results_test_{dataset_spec.test_split}.json'), 'r') as f:\n",
        "        experiment_results.test_results = json.load(f)\n",
        "      with tf.io.gfile.GFile(os.path.join(model_dataset_folder, f'{experiment}_test_loss.csv'), 'r') as f:\n",
        "        experiment_results.test_loss_metrics = pd.read_csv(f)\n",
        "      if not 'no_warmstart' in experiment:\n",
        "        with tf.io.gfile.GFile(os.path.join(model_dataset_folder, f'{experiment}_test_warmstart_loss.csv'), 'r') as f:\n",
        "          experiment_results.test_warmstart_loss_metrics = pd.read_csv(f)\n",
        "      # TODO: align metric variable names across train and test.\n",
        "      test_results_df.loc[model, experiment].at[dataset_spec.dataset, 'auc'] = experiment_results.test_results['test auc']\n",
        "      test_results_df.loc[model, experiment].at[dataset_spec.dataset, 'mrr'] = experiment_results.test_results['test mrr']\n",
        "\n",
        "      model_results.experiment_results[experiment] = experiment_results\n",
        "\n",
        "    dataset_results.model_results[model] = model_results\n",
        "  ALL_RESULTS[dataset_spec.dataset] = dataset_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kbu84WAOOLZ0"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def plot_eval_metric_curves(\n",
        "    dataset='tgbl_wiki',\n",
        "    experiments=EXPERIMENTS,\n",
        "    model='tgn',\n",
        "    val=True,\n",
        "    metric_name='perf',\n",
        "    master_results_dict=ALL_RESULTS,\n",
        "    ax=None,\n",
        "    show_legend=True,\n",
        "):\n",
        "\n",
        "  eval_df_string = 'val' if val else 'test'\n",
        "  plot_dataframes = []\n",
        "\n",
        "  # Make sure that if there is warmstart experiment that it comes first.\n",
        "  experiment_list = copy.deepcopy(experiments)\n",
        "  first_experiment = ''\n",
        "  for experiment in experiment_list:\n",
        "    if 'no_warmstart' not in experiment:\n",
        "      first_experiment = copy.deepcopy(experiment)\n",
        "  experiment_list.remove(first_experiment)\n",
        "  experiment_list = [first_experiment] + experiment_list\n",
        "\n",
        "  warmstart_end_index = 0\n",
        "  for idx, experiment in enumerate(experiment_list):\n",
        "    if 'no_warmstart' not in experiment:\n",
        "      warmstart_df = getattr(\n",
        "          master_results_dict[dataset].model_results[model].experiment_results[experiment],\n",
        "          f'{eval_df_string}_warmstart_loss_metrics'\n",
        "      )\n",
        "      if idx == 0:\n",
        "        warmstart_end_index = len(warmstart_df)\n",
        "      warmstart_df['batch_index'] = list(range(warmstart_end_index))\n",
        "      warmstart_df = warmstart_df.melt(\n",
        "          id_vars=['batch_index'],\n",
        "          value_vars=['loss', 'perf', 'auc'],\n",
        "          value_name='metric_value',\n",
        "          var_name='metric_name'\n",
        "      )\n",
        "      warmstart_df['experiment'] = experiment\n",
        "      warmstart_df = warmstart_df[\n",
        "          warmstart_df.metric_name == metric_name\n",
        "      ].copy()\n",
        "      warmstart_df['period'] = 'warmstart'\n",
        "      plot_dataframes.append(warmstart_df)\n",
        "\n",
        "\n",
        "    eval_df = getattr(\n",
        "        master_results_dict[dataset].model_results[model].experiment_results[experiment],\n",
        "        f'{eval_df_string}_loss_metrics'\n",
        "    )\n",
        "    eval_df['batch_index'] = list(range(warmstart_end_index, len(eval_df) + warmstart_end_index))\n",
        "    eval_df = eval_df.melt(\n",
        "        id_vars=['batch_index'],\n",
        "        value_vars=['loss', 'perf', 'auc'],\n",
        "        value_name='metric_value',\n",
        "        var_name='metric_name'\n",
        "    )\n",
        "    eval_df['experiment'] = experiment\n",
        "    eval_df = eval_df[eval_df.metric_name == metric_name].copy()\n",
        "    eval_df['period'] = 'eval'\n",
        "    plot_dataframes.append(eval_df)\n",
        "\n",
        "  # Make plot.\n",
        "  master_plot_dataframe = pd.concat(plot_dataframes, axis=0)\n",
        "\n",
        "  if ax is None:\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    plt.title(f'{dataset} {eval_df_string} {model} {metric_name}')\n",
        "    sns.lineplot(\n",
        "        data=master_plot_dataframe,\n",
        "        x='batch_index',\n",
        "        y='metric_value',\n",
        "        hue='experiment',\n",
        "        style='period',\n",
        "        legend=show_legend,\n",
        "    )\n",
        "    plt.show()\n",
        "    return master_plot_dataframe\n",
        "  else:\n",
        "    ax.set_title('validation' if eval_df_string == 'val' else 'test')\n",
        "    sns.lineplot(\n",
        "        data=master_plot_dataframe,\n",
        "        x='batch_index',\n",
        "        y='metric_value',\n",
        "        hue='experiment',\n",
        "        style='period',\n",
        "        ax=ax,\n",
        "        legend=show_legend,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9uzcymifl8v"
      },
      "outputs": [],
      "source": [
        "for model in MODELS:\n",
        "  display.display(display.Markdown(f'# {model}'))\n",
        "\n",
        "  for dataset in DATASETS:\n",
        "    display.display(display.Markdown(f'## {dataset}'))\n",
        "\n",
        "    for val in [True, False]:\n",
        "      display.display(display.Markdown(f'### {\"validation\" if val else \"test\"}'))\n",
        "      display.display(\n",
        "          plot_eval_metric_curves(\n",
        "              model=model,\n",
        "              dataset=dataset,\n",
        "              val=val)\n",
        "          )"
      ]
    },
    {
      "metadata": {
        "id": "gYFAMEhaAYgX"
      },
      "cell_type": "code",
      "source": [
        "def plot_side_by_side_eval_metric_curves(\n",
        "    *,\n",
        "    dataset: str,\n",
        "    experiments: Collection[str] | None = None,\n",
        "    models: Collection[str] | None = None,\n",
        "    metric: str ='perf',\n",
        "    results: Mapping[str, DatasetResults] | None = None,\n",
        "):\n",
        "  if not experiments:\n",
        "    experiments = EXPERIMENTS\n",
        "  if not models:\n",
        "    models = MODELS\n",
        "  if not results:\n",
        "    results = ALL_RESULTS\n",
        "\n",
        "  fig = plt.figure(\n",
        "      figsize=(8, len(models) * 3),\n",
        "      constrained_layout=True,\n",
        "  )\n",
        "\n",
        "  subfigs = fig.subfigures(nrows=len(models), ncols=1)\n",
        "\n",
        "  for idx, subfig in enumerate(subfigs):\n",
        "    model = models[idx]\n",
        "    subfig.suptitle(f'{model=}', fontsize=15, y=1.04)\n",
        "\n",
        "    ax = subfig.subplots(nrows=1, ncols=2, sharey='row')\n",
        "    for jdx, val in enumerate([True, False]):\n",
        "      plot_eval_metric_curves(\n",
        "          dataset=dataset,\n",
        "          experiments=experiments,\n",
        "          model=model,\n",
        "          val=val,\n",
        "          metric_name=metric,\n",
        "          master_results_dict=results,\n",
        "          ax=ax[jdx],\n",
        "          show_legend=(idx == jdx == 0)\n",
        "      )\n",
        "      ax[jdx].set_ylabel(f'metric={\"mrr\" if metric == \"perf\" else metric}')\n",
        "\n",
        "  fig.suptitle(f'{dataset=}', fontsize=20, y=1.04)\n",
        "  plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "X9rAIyZxAYgX"
      },
      "cell_type": "code",
      "source": [
        "for dataset in DATASETS:\n",
        "  plot_side_by_side_eval_metric_curves(dataset=dataset)"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/grp/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1LSHHrBHWDs-o_y68I_Lxv5fISoe15WFS",
          "timestamp": 1724442130455
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
