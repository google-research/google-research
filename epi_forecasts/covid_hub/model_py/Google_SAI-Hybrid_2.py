# coding=utf-8
# Copyright 2026 The Google Research Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# pylint: disable=g-bad-import-order,missing-module-docstring,unused-import,g-import-not-at-top,g-line-too-long,unused-variable,used-before-assignment,redefined-outer-name,pointless-statement,unnecessary-pass,invalid-name

import datetime

from absl import app
import pandas as pd

from constant_defs import HORIZONS
from constant_defs import QUANTILES
from constant_defs import REQUIRED_CDC_LOCATIONS
from constant_defs import TARGET_STR
from epi_utils import compute_rolling_evaluation
from epi_utils import format_for_cdc
from epi_utils import get_most_recent_saturday_date_str
from epi_utils import get_next_saturday_date_str
from epi_utils import get_saturdays_between_dates
from plotting_utils import plot_season_forecasts

timedelta = datetime.timedelta


INPUT_DIR = ''
MODEL_NAME = 'Google_SAI-Hybrid_2'
TARGET_STR = 'Total COVID-19 Admissions'

ilinet_hhs = pd.read_csv(f'{INPUT_DIR}/ilinet_hhs_before_20221015.csv')
ilinet = pd.read_csv(f'{INPUT_DIR}/ilinet_before_20221015.csv')
ilinet_state = pd.read_csv(f'{INPUT_DIR}/ilinet_state_before_20221015.csv')
locations = pd.read_csv(f'{INPUT_DIR}/locations.csv')

import warnings
import numpy as np
import pandas as pd
from scipy.stats import norm
from sklearn.linear_model import LinearRegression
from collections import deque
import logging
from typing import Any, Dict, Tuple, Set

# Define the quantiles that the model should predict as per competition requirements
QUANTILE_COLUMNS = [f'quantile_{q}' for q in QUANTILES]


def get_week_of_year_wrapped(date_series):
  """Calculates ISO week of year (1-53) for a Pandas Series of dates.

  Using .isocalendar().week ensures ISO week definition (weeks can span year
  boundaries). This aligns with Code 1's explicit seasonal focus.
  """
  return date_series.dt.isocalendar().week.astype(int)


def fit_and_predict_fn(
    train_x,
    train_y,
    test_x,
):
  """Truly Wonderful Hybrid Model for COVID-19 hospital admissions.

  Core Principles of this Hybrid Method:
  1.  Lagged Dynamics with Seasonal and Location-Specific Baselines: The model
  uses an Autoregressive (AR)
      structure on fourth-root transformed admissions to capture short-term
      persistence. It integrates
      **exogenous seasonal context features** (climatological medians of
      transformed rates, from Code 1) and
      **location-specific dummy variables** (for Code 2's shared parameter
      approach) to provide dynamic seasonal
      baselines and unique location intercepts, allowing for robust shared AR
      coefficients while adapting to local characteristics.
  2.  Dual-Source Probabilistic Quantile Generation: Forecast quantiles are
  generated by combining the AR model's
      mean prediction (in transformed space) with **seasonally-aware empirical
      residuals** (derived from Code 2's residual
      analysis, but with Code 1's seasonal lookup logic). These AR-derived
      transformed quantiles are then inverse-transformed.
      For longer forecast horizons (H >= 1), these are **blended with direct
      historical climatological quantiles** (Code 1, on raw data)
      to robustly capture long-term seasonal uncertainty.
  3.  Robustness through Transformation, Fallbacks, and Interval Scaling: A
  fourth-root transformation (Code 2)
      stabilizes variance and normalizes errors. The model includes
      comprehensive fallback mechanisms for sparse
      data (Code 1's minimum samples, Code 2's global means/variances, padding
      lags), handles nowcasts, and
      scales prediction intervals based on forecast horizon to reflect
      increasing uncertainty.
  """

  # --- Configuration Parameters (Adjusted for stronger climatological influence and robustness) ---
  AR_ORDER = 3
  epsilon_base = 0.169688  # A small value added before transformation to handle zeros and stabilize
  epsilon_std_fallback = 1e-3  # Minimum std for normal distribution fallback

  clima_blend_weight_for_mean = 0.015  # Weight for blending climatological mean into AR mean prediction (kept small as feature is primary)

  min_samples_for_empirical_residuals = (
      15  # Min samples for using empirical residuals for quantiles
  )
  seasonal_window_weeks_res = (
      7  # Window for collecting residuals for empirical quantiles
  )

  # Feature flags for AR model
  use_population_feature = True
  use_week_index_feature = True
  use_polynomial_week_index = False  # REMOVED: Simplified feature set
  use_horizon_feature = True

  # Interaction terms (REMOVED: Simplified feature set)
  use_interaction_term = False
  use_lag2_clima_interaction = False
  use_week_index_clima_interaction = False
  use_log_population_clima_interaction = False
  use_location_dummies = True  # Flag to enable location dummy variables

  epsilon_for_known_actual = (
      0.01  # Small scale for known actuals to produce narrow intervals
  )

  # Parameters for climatological context feature for AR mean (now uses median)
  min_samples_climatological_geo_specific_mean = 5
  min_samples_climatological_geo_aggregated_mean = 20
  seasonal_window_weeks_clima_mean = 10
  clima_geo_specific_weight_mean = (
      0.8  # Weight for geo-specific data in climatological median
  )

  # Parameters for horizon-dependent uncertainty
  use_horizon_dependent_uncertainty = True
  uncertainty_growth_per_horizon = 0.02
  uncertainty_growth_start_factor = 1.0

  # Parameters for blending AR-derived quantiles with direct climatological quantiles
  clima_quantile_blend_weight = 0.05
  use_horizon_dependent_quantile_blending = True
  clima_quantile_blend_weight_growth_per_horizon = 0.01
  clima_quantile_blend_weight_max = 0.9

  # Parameters for direct climatological quantiles (raw data)
  min_samples_climatological_geo_specific_for_quantiles = 5
  min_samples_climatological_geo_aggregated_for_quantiles = 20
  seasonal_window_weeks_clima_for_quantiles = 8
  clima_geo_specific_quantile_weight = (
      0.7  # Weight for geo-specific data in direct quantiles
  )

  # New constant for AR model fitting robustness
  MIN_SAMPLES_FOR_AR_FIT = 50

  # --- Helper Functions (nested to respect rewrite cell constraint) ---

  def _get_seasonally_windowed_data(
      train_df_full,
      loc_id,
      current_weekofyear,
      seasonal_window_weeks,
      target_column_geo_specific,
      target_column_geo_aggregated,
  ):
    """Retrieves seasonally-windowed historical data for a given location and week.

    Returns geo-specific and geo-aggregated data series for specified target
    columns.
    """
    candidate_weeks = set()
    for offset in range(-seasonal_window_weeks, seasonal_window_weeks + 1):
      candidate_week_val = (current_weekofyear + offset - 1 + 53) % 53 + 1
      candidate_weeks.add(candidate_week_val)

    # Filter by week of year for both geo-specific and geo-aggregated
    filtered_by_week = train_df_full[
        train_df_full['weekofyear'].isin(candidate_weeks)
    ]

    geo_specific_data = filtered_by_week[
        filtered_by_week['location'] == loc_id
    ][target_column_geo_specific].dropna()

    geo_aggregated_data = filtered_by_week[
        target_column_geo_aggregated
    ].dropna()

    return geo_specific_data, geo_aggregated_data

  def calculate_transformed_rate_clima_feature(
      loc_id,
      current_weekofyear,
      train_df_full,
      min_samples_climatological_geo_specific,
      min_samples_climatological_geo_aggregated,
      seasonal_window_weeks,
      global_transformed_rate_median_fallback,  # Changed to median fallback
      clima_geo_specific_weight,
  ):
    """Calculates a single climatological MEDIAN in the transformed 'rate' space for a given location and week.

    This function embodies Code 1's "Cross-Location Information Sharing
    (Borrowing Strength)" and "Temporal Dependencies & Seasonality: Explicit
    Seasonal Focus" principles, using MEDIAN for robustness. It leverages both
    geo-specific and geo-aggregated transformed historical 'rate' data,
    providing a seasonal context feature for the AR model which predicts
    transformed counts.
    """
    geo_specific_data, geo_aggregated_transformed_data = (
        _get_seasonally_windowed_data(
            train_df_full=train_df_full,
            loc_id=loc_id,
            current_weekofyear=current_weekofyear,
            seasonal_window_weeks=seasonal_window_weeks,
            target_column_geo_specific='transformed_admissions_per_100k',
            target_column_geo_aggregated='transformed_admissions_per_100k',
        )
    )

    geo_specific_median = np.nan
    if len(geo_specific_data) >= min_samples_climatological_geo_specific:
      geo_specific_median = np.nanmedian(geo_specific_data)  # Changed to median

    geo_aggregated_median_transformed = np.nan
    if (
        len(geo_aggregated_transformed_data)
        >= min_samples_climatological_geo_aggregated
    ):
      geo_aggregated_median_transformed = np.nanmedian(
          geo_aggregated_transformed_data
      )  # Changed to median

    clima_median_transformed = np.nan
    if np.isfinite(geo_specific_median) and np.isfinite(
        geo_aggregated_median_transformed
    ):
      clima_median_transformed = (
          clima_geo_specific_weight * geo_specific_median
          + (1 - clima_geo_specific_weight) * geo_aggregated_median_transformed
      )
    elif np.isfinite(geo_specific_median):
      clima_median_transformed = geo_specific_median
    elif np.isfinite(geo_aggregated_median_transformed):
      clima_median_transformed = geo_aggregated_median_transformed
    else:
      clima_median_transformed = global_transformed_rate_median_fallback

    if not np.isfinite(clima_median_transformed):
      return float(global_transformed_rate_median_fallback)
    return float(clima_median_transformed)

  def get_ar_residual_quantiles(
      loc_id,
      current_weekofyear_for_res,
      all_residuals_ar_df,
      min_samples_for_empirical_residuals,
      seasonal_window_weeks_res,
      global_std_transformed_ar,
      epsilon_std_fallback,
      fallback_normal_dist_loc = 0,
  ):
    """Retrieves seasonally-aware empirical quantiles of AR model residuals.

    Prioritizes location-specific, then global-week-specific, then global-all,
    then normal fallback. Residuals are NOT explicitly centered here, assuming
    the AR model's mean prediction aligns.
    """
    candidate_weeks_res = set()
    for offset in range(
        -seasonal_window_weeks_res, seasonal_window_weeks_res + 1
    ):
      candidate_week_val = (
          current_weekofyear_for_res + offset - 1 + 53
      ) % 53 + 1
      candidate_weeks_res.add(candidate_week_val)

    if not all_residuals_ar_df.empty:
      # Attempt 1: Location-specific, Windowed Week-specific residuals
      loc_week_residuals = all_residuals_ar_df[
          (all_residuals_ar_df['location'] == loc_id)
          & (all_residuals_ar_df['weekofyear'].isin(candidate_weeks_res))
      ]['residuals_ar'].dropna()

      if len(loc_week_residuals) >= min_samples_for_empirical_residuals:
        res_q = np.percentile(loc_week_residuals, [q * 100 for q in QUANTILES])
        # Removed: res_q -= np.percentile(loc_week_residuals, 50) # No explicit centering
        return res_q

      # Attempt 2: Global, Windowed Week-specific residuals (across all locations for this week window)
      global_week_residuals = all_residuals_ar_df[
          (all_residuals_ar_df['weekofyear'].isin(candidate_weeks_res))
      ]['residuals_ar'].dropna()

      if len(global_week_residuals) >= min_samples_for_empirical_residuals:
        res_q = np.percentile(
            global_week_residuals, [q * 100 for q in QUANTILES]
        )
        # Removed: res_q -= np.percentile(global_week_residuals, 50) # No explicit centering
        return res_q

    # Attempt 3: Global, All-week residuals
    global_all_residuals_ar = all_residuals_ar_df['residuals_ar'].dropna()
    if len(global_all_residuals_ar) >= min_samples_for_empirical_residuals:
      res_q = np.percentile(
          global_all_residuals_ar, [q * 100 for q in QUANTILES]
      )
      # Removed: res_q -= np.percentile(global_all_residuals_ar, 50) # No explicit centering
      return res_q

    # Final Fallback: Normal distribution if no empirical residuals are sufficient
    scale = max(global_std_transformed_ar, epsilon_std_fallback)
    return np.array([
        norm.ppf(q, loc=fallback_normal_dist_loc, scale=scale)
        for q in QUANTILES
    ])

  def get_climatological_quantiles_raw(
      loc_id,
      current_weekofyear,
      train_df_raw_admissions,
      min_samples_geo_specific,
      min_samples_geo_aggregated,
      seasonal_window_weeks,
      clima_geo_specific_quantile_weight,
      global_median_raw_fallback,
  ):
    """Generates direct climatological quantiles for raw admissions, combining geo-specific and geo-aggregated data.

    This directly applies Code 1's quantile generation principle to the raw
    (untransformed) target variable.
    """
    geo_specific_data_raw, geo_aggregated_data_raw = (
        _get_seasonally_windowed_data(
            train_df_full=train_df_raw_admissions,
            loc_id=loc_id,
            current_weekofyear=current_weekofyear,
            seasonal_window_weeks=seasonal_window_weeks,
            target_column_geo_specific='Total COVID-19 Admissions',
            target_column_geo_aggregated='Total COVID-19 Admissions',
        )
    )

    geo_specific_quantiles = np.full(len(QUANTILES), np.nan)
    if len(geo_specific_data_raw) >= min_samples_geo_specific:
      geo_specific_quantiles = np.percentile(
          geo_specific_data_raw, [q * 100 for q in QUANTILES]
      )

    geo_aggregated_quantiles = np.full(len(QUANTILES), np.nan)
    if len(geo_aggregated_data_raw) >= min_samples_geo_aggregated:
      geo_aggregated_quantiles = np.percentile(
          geo_aggregated_data_raw, [q * 100 for q in QUANTILES]
      )

    final_clima_quantiles = np.full(
        len(QUANTILES), float(global_median_raw_fallback)
    )

    for i in range(len(QUANTILES)):
      qs_val = geo_specific_quantiles[i]
      qa_val = geo_aggregated_quantiles[i]

      if np.isfinite(qs_val) and np.isfinite(qa_val):
        final_clima_quantiles[i] = (
            clima_geo_specific_quantile_weight * qs_val
            + (1 - clima_geo_specific_quantile_weight) * qa_val
        )
      elif np.isfinite(qs_val):
        final_clima_quantiles[i] = qs_val
      elif np.isfinite(qa_val):
        final_clima_quantiles[i] = qa_val
      else:
        final_clima_quantiles[i] = global_median_raw_fallback

    return final_clima_quantiles

  # --- Prepare Training Data ---
  train_df = train_x.copy()
  train_df['Total COVID-19 Admissions'] = train_y

  train_df['target_end_date'] = pd.to_datetime(train_df['target_end_date'])
  train_df['horizon'] = (
      0  # All training data corresponds to horizon 0 for AR feature
  )

  min_train_date_for_week_index = train_df['target_end_date'].min()
  train_df['weekofyear'] = get_week_of_year_wrapped(train_df['target_end_date'])

  train_df = train_df.sort_values(
      by=['location', 'target_end_date']
  ).reset_index(drop=True)

  # --- Data Transformation: Fourth-Root for Target (Counts) (Code 2 principle) ---
  train_df['transformed_admissions'] = (
      train_df['Total COVID-19 Admissions'] + epsilon_base
  ) ** (1 / 4)

  # --- Calculate global fallbacks for transformed admissions (BEFORE feature creation) ---
  global_transformed_mean_fallback = (0 + epsilon_base) ** (1 / 4)
  temp_mean = train_df['transformed_admissions'].mean()
  if pd.notna(temp_mean) and np.isfinite(temp_mean):
    global_transformed_mean_fallback = temp_mean
  global_transformed_mean_fallback = max(
      global_transformed_mean_fallback, epsilon_base ** (1 / 4)
  )

  # --- Intermediate Transformation for Climatological Context (Rates) (Hybrid: Code 1 + Code 2) ---
  population_safe = train_df['population'].replace(0, 1).fillna(1)
  train_df['admissions_per_100k'] = (
      train_df['Total COVID-19 Admissions'] / population_safe * 100000
  ).fillna(0)
  train_df['transformed_admissions_per_100k'] = (
      train_df['admissions_per_100k'] + epsilon_base
  ) ** (1 / 4)

  # --- Calculate global median of transformed *rates* for climatological context fallback ---
  global_transformed_rate_median_fallback = (0 + epsilon_base) ** (
      1 / 4
  )  # Changed to median fallback
  temp_rate_median = train_df[
      'transformed_admissions_per_100k'
  ].median()  # Changed to median
  if pd.notna(temp_rate_median) and np.isfinite(temp_rate_median):
    global_transformed_rate_median_fallback = temp_rate_median
  global_transformed_rate_median_fallback = max(
      global_transformed_rate_median_fallback, epsilon_base ** (1 / 4)
  )

  # --- Calculate global median of raw admissions for direct climatological quantile fallback ---
  global_median_raw_fallback = np.nanmedian(train_y)
  if (
      not np.isfinite(global_median_raw_fallback)
      or global_median_raw_fallback < 0
  ):
    global_median_raw_fallback = 0.0

  # --- Determine default AR lag value based on training data mean or absolute fallback ---
  default_ar_lag_value_transformed = global_transformed_mean_fallback

  # --- HYBRID STEP: Precompute Climatological Context Feature in Transformed Rate Space ---
  # Now stores the median of transformed rates, for robustness
  climatological_context_lookup = {}
  unique_loc_weeks = (
      train_df[['location', 'weekofyear']]
      .drop_duplicates()
      .to_records(index=False)
  )

  for loc_id, current_weekofyear in unique_loc_weeks:
    clima_context_val = calculate_transformed_rate_clima_feature(
        loc_id=loc_id,
        current_weekofyear=current_weekofyear,
        train_df_full=train_df,
        min_samples_climatological_geo_specific=min_samples_climatological_geo_specific_mean,
        min_samples_climatological_geo_aggregated=min_samples_climatological_geo_aggregated_mean,
        seasonal_window_weeks=seasonal_window_weeks_clima_mean,
        global_transformed_rate_median_fallback=global_transformed_rate_median_fallback,  # Changed
        clima_geo_specific_weight=clima_geo_specific_weight_mean,
    )
    climatological_context_lookup[(loc_id, current_weekofyear)] = (
        clima_context_val
    )

  train_df['climatological_context_rate_transformed'] = train_df.apply(
      lambda row: climatological_context_lookup.get(
          (row['location'], row['weekofyear']),
          global_transformed_rate_median_fallback,
      ),
      axis=1,
  )

  # --- Feature Engineering for AR Model ---
  # Define a dictionary for all potential AR features and their default fallbacks
  base_feature_default_values: Dict[str, Any] = {
      'climatological_context_rate_transformed': (
          global_transformed_rate_median_fallback
      ),  # Changed
      'log_population': np.log1p(1e-6),
      'week_index': 0.0,
      # Removed polynomial week_index features
      'horizon': 0.0,
  }

  # Generate lagged features
  for i in range(1, AR_ORDER + 1):
    lag_col = f'transformed_admissions_lag_{i}'
    train_df[lag_col] = train_df.groupby('location')[
        'transformed_admissions'
    ].shift(i)
    base_feature_default_values[lag_col] = default_ar_lag_value_transformed

  # Build up the list of actual AR features based on configuration (SIMPLIFIED)
  all_potential_ar_features = []
  for i in range(1, AR_ORDER + 1):
    all_potential_ar_features.append(f'transformed_admissions_lag_{i}')

  if use_population_feature:
    train_df['log_population'] = np.log1p(
        train_df['population'].replace(0, 1e-6).fillna(1e-6)
    )
    all_potential_ar_features.append('log_population')

  all_potential_ar_features.append('climatological_context_rate_transformed')

  train_df['week_index'] = (
      train_df['target_end_date'] - min_train_date_for_week_index
  ).dt.days / 7
  train_df['week_index'] = np.maximum(0, train_df['week_index'])
  if use_week_index_feature:
    all_potential_ar_features.append('week_index')
    # Removed polynomial week index features

  if use_horizon_feature:
    all_potential_ar_features.append('horizon')

  # Removed: Interaction terms (simplified feature set)

  # --- Add location_name as a categorical feature for dummy variables ---
  trained_dummy_cols = []
  if use_location_dummies:
    train_df_dummies = pd.get_dummies(
        train_df['location_name'], prefix='loc', drop_first=True
    )
    if not train_df_dummies.empty:
      train_df = pd.concat([train_df, train_df_dummies], axis=1)
      trained_dummy_cols = train_df_dummies.columns.tolist()
      all_potential_ar_features.extend(trained_dummy_cols)
      # Dummy features should always default to 0
      for col in trained_dummy_cols:
        base_feature_default_values[col] = 0.0
    else:
      logging.warning(
          'No location dummies generated for training data,'
          " 'use_location_dummies' is True but train_df_dummies is empty."
      )
      use_location_dummies = False

  # Filter to actual features available in the DataFrame for fitting after checking for all-NaN columns
  actual_ar_features = [
      f
      for f in all_potential_ar_features
      if f in train_df.columns and not train_df[f].isnull().all()
  ]
  if not actual_ar_features:
    logging.warning('No valid AR features found. AR model will not be fitted.')

  train_df_cleaned_ar = train_df.dropna(
      subset=['transformed_admissions']
  ).copy()
  if actual_ar_features:
    train_df_cleaned_ar = train_df_cleaned_ar.dropna(subset=actual_ar_features)

  # Robustly calculate ar_feature_means, using actual means or defined fallbacks
  ar_feature_means = {}
  for f in actual_ar_features:
    if (
        f in trained_dummy_cols
    ):  # Ensure dummy features default to 0 for missing
      ar_feature_means[f] = 0.0
    else:
      mean_val = (
          train_df_cleaned_ar[f].replace([np.inf, -np.inf], np.nan).mean()
      )
      if not np.isfinite(mean_val) or pd.isna(mean_val):
        mean_val = base_feature_default_values.get(
            f, 0.0
        )  # Fallback to specified default or 0
        if not np.isfinite(mean_val):
          mean_val = 0.0  # Final safety fallback
      ar_feature_means[f] = mean_val

  ar_model = None
  all_residuals_ar_df = pd.DataFrame(
      columns=['location', 'weekofyear', 'residuals_ar']
  )

  # Fit AR model if sufficient data and features exist (NEW: uses MIN_SAMPLES_FOR_AR_FIT)
  if actual_ar_features and train_df_cleaned_ar.shape[0] >= max(
      len(actual_ar_features) + 1, MIN_SAMPLES_FOR_AR_FIT
  ):
    X_train_ar = train_df_cleaned_ar[actual_ar_features]
    y_train_transformed_ar = train_df_cleaned_ar['transformed_admissions']
    ar_model = LinearRegression(fit_intercept=True)
    try:
      ar_model.fit(X_train_ar, y_train_transformed_ar)
      train_df_cleaned_ar['predicted_transformed_ar'] = ar_model.predict(
          X_train_ar
      )
      train_df_cleaned_ar['residuals_ar'] = (
          train_df_cleaned_ar['transformed_admissions']
          - train_df_cleaned_ar['predicted_transformed_ar']
      )
      all_residuals_ar_df = train_df_cleaned_ar[
          ['location', 'weekofyear', 'residuals_ar']
      ].dropna()
    except Exception as e:
      ar_model = None  # Ensure AR model is set to None if fitting fails
      logging.warning(
          f'AR model fitting failed: {e}. Falling back to default predictions.'
      )
  else:
    logging.warning(
        f'Insufficient data ({train_df_cleaned_ar.shape[0]} samples, min'
        f' {max(len(actual_ar_features) + 1, MIN_SAMPLES_FOR_AR_FIT)}) or'
        ' features to fit AR model. AR model will not be fitted.'
    )

  global_std_transformed_ar = epsilon_std_fallback
  if not all_residuals_ar_df['residuals_ar'].empty:
    global_std_transformed_ar = max(
        all_residuals_ar_df['residuals_ar'].std(), epsilon_std_fallback
    )
  elif not train_df_cleaned_ar['transformed_admissions'].empty:
    global_std_transformed_ar = max(
        train_df_cleaned_ar['transformed_admissions'].std(),
        epsilon_std_fallback,
    )

  # --- Predict for Test Data ---
  test_x_copy = test_x.copy()
  test_x_copy['target_end_date'] = pd.to_datetime(
      test_x_copy['target_end_date']
  )
  test_x_copy['weekofyear'] = get_week_of_year_wrapped(
      test_x_copy['target_end_date']
  )
  test_x_copy['horizon'] = test_x_copy['horizon'].astype(int)

  if use_population_feature:
    test_x_copy['log_population'] = np.log1p(
        test_x_copy['population'].replace(0, 1e-6).fillna(1e-6)
    )

  test_x_copy['week_index'] = (
      test_x_copy['target_end_date'] - min_train_date_for_week_index
  ).dt.days / 7
  test_x_copy['week_index'] = np.maximum(0, test_x_copy['week_index'])
  # Removed polynomial week index features

  # --- Create dummy variables for test data and align columns ---
  if use_location_dummies and trained_dummy_cols:
    test_df_dummies = pd.get_dummies(
        test_x_copy['location_name'], prefix='loc', drop_first=True
    )

    missing_cols_in_test = set(trained_dummy_cols) - set(
        test_df_dummies.columns
    )
    for col in missing_cols_in_test:
      test_df_dummies[col] = 0

    extra_cols_in_test = set(test_df_dummies.columns) - set(trained_dummy_cols)
    if extra_cols_in_test:
      test_df_dummies = test_df_dummies.drop(columns=list(extra_cols_in_test))

    test_df_dummies = test_df_dummies[
        trained_dummy_cols
    ]  # Ensure column order matches training
    test_x_copy = pd.concat([test_x_copy, test_df_dummies], axis=1)
  elif use_location_dummies and not trained_dummy_cols:
    logging.warning(
        'Location dummies were enabled but no dummy columns were trained.'
        ' Disabling for test data.'
    )
    use_location_dummies = False

  test_y_hat_quantiles = pd.DataFrame(
      index=test_x.index, columns=QUANTILE_COLUMNS
  )

  train_actuals_lookup_transformed = train_df.set_index(
      ['location', 'target_end_date']
  )['transformed_admissions']
  train_actuals_lookup_raw = train_df.set_index(
      ['location', 'target_end_date']
  )['Total COVID-19 Admissions']

  all_locations = pd.concat(
      [train_df['location'], test_x_copy['location']]
  ).unique()
  location_lag_buffers = {}

  for loc in all_locations:
    loc_history = train_df[train_df['location'] == loc].sort_values(
        'target_end_date', ascending=True
    )
    recent_transformed_admissions = (
        loc_history['transformed_admissions'].iloc[-AR_ORDER:].to_list()
    )
    padded_lags = [default_ar_lag_value_transformed] * max(
        0, AR_ORDER - len(recent_transformed_admissions)
    ) + recent_transformed_admissions
    location_lag_buffers[loc] = deque(padded_lags)

  test_x_copy_sorted = test_x_copy.sort_values(
      by=['location', 'target_end_date']
  )

  for loc, loc_test_df in test_x_copy_sorted.groupby('location'):
    current_loc_buffer = location_lag_buffers.get(
        loc, deque([default_ar_lag_value_transformed] * AR_ORDER)
    )
    if len(current_loc_buffer) != AR_ORDER:
      current_loc_buffer = deque([default_ar_lag_value_transformed] * AR_ORDER)

    for idx, row in loc_test_df.iterrows():
      current_target_date = row['target_end_date']
      current_weekofyear_for_forecast = row['weekofyear']
      current_horizon = row['horizon']

      is_known_actual = (
          loc,
          current_target_date,
      ) in train_actuals_lookup_raw.index

      predicted_quantiles_final = np.full(len(QUANTILES), 0.0)

      if is_known_actual:
        actual_raw = train_actuals_lookup_raw.loc[(loc, current_target_date)]
        actual_transformed = train_actuals_lookup_transformed.loc[
            (loc, current_target_date)
        ]
        predicted_quantiles_final = norm.ppf(
            QUANTILES, loc=max(0, actual_raw), scale=epsilon_for_known_actual
        )
        predicted_quantiles_final = np.maximum(0, predicted_quantiles_final)
        value_for_ar_buffer = actual_transformed
      else:
        # --- Get Climatological Context Feature for Current Forecast (for AR mean, now median-based) ---
        clima_context_for_forecast_mean = climatological_context_lookup.get(
            (loc, current_weekofyear_for_forecast),
            global_transformed_rate_median_fallback,  # Changed to median fallback
        )

        # --- AR Component Calculation (Forecasts) ---
        predict_features_vals = {}
        for i in range(1, AR_ORDER + 1):
          predict_features_vals[f'transformed_admissions_lag_{i}'] = (
              current_loc_buffer[AR_ORDER - i]
          )

        predict_features_vals['climatological_context_rate_transformed'] = (
            clima_context_for_forecast_mean
        )
        if use_population_feature:
          predict_features_vals['log_population'] = row['log_population']
        if use_week_index_feature:
          predict_features_vals['week_index'] = row['week_index']
          # Removed polynomial week index features
        if use_horizon_feature:
          predict_features_vals['horizon'] = row['horizon']

        # --- Add location dummy variables to prediction features ---
        if use_location_dummies and trained_dummy_cols:
          for col in trained_dummy_cols:
            predict_features_vals[col] = row.get(col, 0.0)

        # Removed: Interaction terms (simplified feature set)

        # Prepare prediction row, filling any missing features with their calculated means/fallbacks
        X_predict_row_ar_dict = {
            f: predict_features_vals.get(f, ar_feature_means.get(f, 0.0))
            for f in actual_ar_features
        }
        ar_mean_transformed = default_ar_lag_value_transformed

        if ar_model and actual_ar_features:
          X_predict_row_ar = pd.DataFrame([X_predict_row_ar_dict])
          X_predict_row_ar = X_predict_row_ar[actual_ar_features].fillna(
              ar_feature_means
          )
          if not X_predict_row_ar.empty and len(
              X_predict_row_ar.columns
          ) == len(actual_ar_features):
            try:
              ar_mean_transformed = ar_model.predict(X_predict_row_ar)[0]
              if not np.isfinite(ar_mean_transformed):
                ar_mean_transformed = default_ar_lag_value_transformed
            except Exception as e:
              ar_mean_transformed = default_ar_lag_value_transformed
              logging.warning(
                  f'AR model prediction failed for loc {loc}, date'
                  f' {current_target_date}: {e}. Falling back to default'
                  ' transformed value.'
              )
        else:
          ar_mean_transformed = (
              clima_context_for_forecast_mean
              if np.isfinite(clima_context_for_forecast_mean)
              else default_ar_lag_value_transformed
          )

        # Blend AR mean with climatological mean (kept small as climatological context is already a feature)
        ar_mean_transformed = (
            (1 - clima_blend_weight_for_mean) * ar_mean_transformed
            + clima_blend_weight_for_mean * clima_context_for_forecast_mean
        )
        ar_mean_transformed = max(
            ar_mean_transformed, epsilon_base ** (1 / 4)
        )  # Ensure non-negative transformed value

        # --- Quantiles based on AR Residuals ---
        ar_residual_quantiles_unscaled = get_ar_residual_quantiles(
            loc_id=loc,
            current_weekofyear_for_res=current_weekofyear_for_forecast,
            all_residuals_ar_df=all_residuals_ar_df,
            min_samples_for_empirical_residuals=min_samples_for_empirical_residuals,
            seasonal_window_weeks_res=seasonal_window_weeks_res,
            global_std_transformed_ar=global_std_transformed_ar,
            epsilon_std_fallback=epsilon_std_fallback,
        )

        if use_horizon_dependent_uncertainty:
          current_horizon_scale_factor = (
              uncertainty_growth_start_factor
              + (current_horizon) * uncertainty_growth_per_horizon
          )
          current_horizon_scale_factor = max(
              epsilon_std_fallback, current_horizon_scale_factor
          )
          ar_residual_quantiles_transformed = (
              ar_residual_quantiles_unscaled * current_horizon_scale_factor
          )
        else:
          ar_residual_quantiles_transformed = ar_residual_quantiles_unscaled

        predicted_quantiles_transformed_ar = (
            ar_mean_transformed + ar_residual_quantiles_transformed
        )
        predicted_quantiles_final_ar_raw = (
            np.maximum(0, predicted_quantiles_transformed_ar) ** 4
        ) - epsilon_base
        predicted_quantiles_final_ar_raw = np.maximum(
            0, predicted_quantiles_final_ar_raw
        )

        # --- Direct Climatological Quantiles (from raw admissions, Code 1) ---
        clima_quantiles_raw = get_climatological_quantiles_raw(
            loc_id=loc,
            current_weekofyear=current_weekofyear_for_forecast,
            train_df_raw_admissions=train_df,
            min_samples_geo_specific=min_samples_climatological_geo_specific_for_quantiles,
            min_samples_geo_aggregated=min_samples_climatological_geo_aggregated_for_quantiles,
            seasonal_window_weeks=seasonal_window_weeks_clima_for_quantiles,
            clima_geo_specific_quantile_weight=clima_geo_specific_quantile_weight,
            global_median_raw_fallback=global_median_raw_fallback,
        )
        clima_quantiles_raw = np.maximum(0, clima_quantiles_raw)

        # --- Final Hybrid Blending of Quantiles ---
        current_clima_quantile_blend_weight_actual = 0.0

        if current_horizon >= 1:
          if use_horizon_dependent_quantile_blending:
            current_clima_quantile_blend_weight_actual = (
                clima_quantile_blend_weight
                + (current_horizon - 1)
                * clima_quantile_blend_weight_growth_per_horizon
            )
          else:
            current_clima_quantile_blend_weight_actual = (
                clima_quantile_blend_weight
            )

        current_clima_quantile_blend_weight_actual = min(
            current_clima_quantile_blend_weight_actual,
            clima_quantile_blend_weight_max,
        )
        current_clima_quantile_blend_weight_actual = max(
            current_clima_quantile_blend_weight_actual, 0.0
        )

        predicted_quantiles_final = (
            (1 - current_clima_quantile_blend_weight_actual)
            * predicted_quantiles_final_ar_raw
            + current_clima_quantile_blend_weight_actual * clima_quantiles_raw
        )

        value_for_ar_buffer = ar_mean_transformed

      # Update the lag buffer for the next recursive step for this location
      current_loc_buffer.popleft()
      current_loc_buffer.append(value_for_ar_buffer)
      location_lag_buffers[loc] = current_loc_buffer

      # --- Post-processing ---
      predicted_quantiles_final = np.nan_to_num(
          predicted_quantiles_final, nan=0.0, posinf=1e10, neginf=0.0
      )
      predicted_quantiles_final = np.round(
          np.maximum(0, predicted_quantiles_final)
      )
      predicted_quantiles_final = np.maximum.accumulate(
          predicted_quantiles_final
      ).astype(int)

      test_y_hat_quantiles.loc[idx, QUANTILE_COLUMNS] = (
          predicted_quantiles_final
      )

  return test_y_hat_quantiles


def main(argv):
  del argv  # Unused.
  locations = locations[locations['location'].isin(REQUIRED_CDC_LOCATIONS)]
  locations['location'] = locations['location'].astype(int)
  location_codes = locations['location'].unique()

  print('Locations sample:')
  print(locations.head())

  dataset = pd.read_csv(f'{INPUT_DIR}/dataset.csv')
  dataset['target_end_date'] = pd.to_datetime(
      dataset['target_end_date']
  ).dt.date

  print('Dataset sample (check for existence of most recent data):')
  print(dataset.sort_values(by=['target_end_date'], ascending=False).head())

  dataset['Total Influenza Admissions'] = (
      pd.to_numeric(dataset['Total Influenza Admissions'], errors='coerce')
      .replace({np.nan: np.nan})
      .astype('Int64')
  )

  # --- Execute Validation Run ---
  print('--- Starting Validation Run ---')
  # Define validation and test periods

  validation_date_end = get_most_recent_saturday_date_str()
  validation_date_start = pd.to_datetime(validation_date_end) - pd.Timedelta(
      weeks=3
  )

  validation_reference_dates = get_saturdays_between_dates(
      validation_date_start, validation_date_end
  )
  print('validation_reference_dates:', validation_reference_dates)
  validation_forecasts, validation_score = compute_rolling_evaluation(
      observed_values=dataset.copy(),
      reference_dates=validation_reference_dates,
      fit_and_predict_fn=fit_and_predict_fn,
      horizons=HORIZONS,
      location_codes=location_codes,
      locations_df=locations,
  )

  print(f'\nValidation Score: {validation_score}')
  if not validation_forecasts.empty:
    validation_forecasts.to_csv('/tmp/validation_forecasts.csv', index=False)
    print("Validation forecasts saved to '/tmp/validation_forecasts.csv'")

  # Plot forecast and predictions on validation dates against observed data

  validation_forecasts['target_end_date'] = pd.to_datetime(
      validation_forecasts['target_end_date']
  )
  validation_forecasts['reference_date'] = pd.to_datetime(
      validation_forecasts['reference_date']
  )

  # Prepare the observed data
  national_observed_all = (
      dataset.groupby('target_end_date')['Total Influenza Admissions']
      .sum()
      .reset_index()
  )
  national_observed_all['target_end_date'] = pd.to_datetime(
      national_observed_all['target_end_date']
  )

  dates_to_plot_validation = [
      {
          'start': pd.to_datetime(validation_date_start) - timedelta(weeks=2),
          'end': pd.to_datetime(validation_date_end) + timedelta(weeks=5),
          'name': 'validation',
      },
  ]

  for season in dates_to_plot_validation:
    print(f"--- Generating plot for {season['name']} dates ---")
    plot_season_forecasts(
        season_start=season['start'],
        season_end=season['end'],
        season_name=season['name'],
        all_forecasts_df=validation_forecasts,
        national_observed_df=national_observed_all,
        step_size=1,
    )

  submission_date_str = get_next_saturday_date_str()
  submission_date = pd.to_datetime(submission_date_str).date()

  test_forecasts, _ = compute_rolling_evaluation(
      observed_values=dataset.copy(),
      reference_dates=[submission_date],
      fit_and_predict_fn=fit_and_predict_fn,
      horizons=HORIZONS,
      location_codes=location_codes,
      locations_df=locations,
  )

  print('\n--- Creating the submission file ---')

  if not test_forecasts.empty:
    cdc_submission = format_for_cdc(test_forecasts, 'wk inc flu hosp')
    cdc_submission.to_csv(
        f'/tmp/{submission_date_str}_{MODEL_NAME}.csv', index=False
    )
    print(
        'Submission forecasts saved to'
        f" '/tmp/{submission_date_str}_{MODEL_NAME}.csv'"
    )

    print('Verify final submission file:')
    print(cdc_submission)

    # Convert dates in test_forecasts to Timestamp
    test_forecasts['target_end_date'] = pd.to_datetime(
        test_forecasts['target_end_date']
    )
    test_forecasts['reference_date'] = pd.to_datetime(
        test_forecasts['reference_date']
    )

    # Plot forecasts for submission (all horizons)
    cdc_submission['target_end_date'] = pd.to_datetime(
        cdc_submission['target_end_date']
    )
    cdc_submission['reference_date'] = pd.to_datetime(
        cdc_submission['reference_date']
    )

    dates_to_plot_submission = [
        {
            'start': pd.to_datetime(submission_date) - timedelta(weeks=1),
            'end': pd.to_datetime(submission_date) + timedelta(weeks=3),
            'name': f'{submission_date} forecast',
        },
    ]

    for season in dates_to_plot_submission:
      print(f"--- Generating plot for {season['name']} dates ---")
      plot_season_forecasts(
          season_start=season['start'],
          season_end=season['end'],
          season_name=season['name'],
          all_forecasts_df=test_forecasts,
          national_observed_df=None,
          step_size=1,
      )


if __name__ == '__main__':
  app.run(main)
