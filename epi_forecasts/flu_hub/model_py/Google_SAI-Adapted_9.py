# coding=utf-8
# Copyright 2025 The Google Research Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# pylint: disable=g-bad-import-order,missing-module-docstring,unused-import,g-import-not-at-top,g-line-too-long,unused-variable,used-before-assignment,redefined-outer-name,pointless-statement,unnecessary-pass,invalid-name

import datetime

from absl import app
import pandas as pd

from constant_defs import HORIZONS
from constant_defs import QUANTILES
from constant_defs import REQUIRED_CDC_LOCATIONS
from constant_defs import TARGET_STR
from epi_utils import compute_rolling_evaluation
from epi_utils import format_for_cdc
from epi_utils import get_most_recent_saturday_date_str
from epi_utils import get_next_saturday_date_str
from epi_utils import get_saturdays_between_dates
from plotting_utils import plot_season_forecasts

timedelta = datetime.timedelta


INPUT_DIR = ''
MODEL_NAME = 'Google_SAI-Adapted_9'
TARGET_STR = 'Total Influenza Admissions'

ilinet_hhs = pd.read_csv(f'{INPUT_DIR}/ilinet_hhs_before_20221015.csv')
ilinet = pd.read_csv(f'{INPUT_DIR}/ilinet_before_20221015.csv')
ilinet_state = pd.read_csv(f'{INPUT_DIR}/ilinet_state_before_20221015.csv')
locations = pd.read_csv(f'{INPUT_DIR}/locations.csv')

# Core principles of the method, as described by the expert:
# 1. Epidemiological Model (SIR-like Dynamics with unknown case ascertainment, R0, population immunity, and a splined effective reproduction number): The expert proposes an SIR model to capture seasonal influenza dynamics within a season.
#    -> Statistical Proxy: The implemented LightGBM model, with features like lagged normalized admissions, lagged normalized ILI (as a proxy for case ascertainment/infections), and cyclical seasonal components (sin/cos week), aims to statistically approximate these complex epidemiological dynamics. Population normalization helps to make these dynamics more comparable across different-sized states.
# 2. Bayesian Hierarchical Model (Across-season trends in SIR model parameters as hyperparameters, used as priors for the current season): The expert outlines a Bayesian hierarchical model to derive across-season trends and use them as priors.
#    -> Statistical Proxy: By training multiple LightGBM models (one for each quantile) across all locations and historical seasons, and including `location_name` as a categorical feature, the model learns shared underlying relationships (global priors) while allowing for location-specific adjustments (group-specific deviations). The improved, seasonally-varying transformation of historical ILI data, which now operates on population-normalized rates, provides a more robust and longer "synthetic history" which acts as an augmented prior for training.
# 3. Disease Model Integrated in C++ (for performance): The expert states the disease model is integrated in C++ and bound to Python with pybind11 for computational efficiency.
#    -> Implementation Note: Direct C++ integration is not possible within this Python cell. The focus is on using an efficient, optimized Python library (LightGBM) which is implemented in C++ internally, providing a computationally performant statistical proxy for the disease model.
# 4. Bayesian Posterior Probability Sampled with `emcee` (for robustness and uncertainty characterization): The expert specifies using the ensemble sampler of Goodman and Weare available in `emcee` for robust sampling of the Bayesian posterior.
#    -> Statistical Proxy: Probabilistic forecasts are generated by training separate LightGBM quantile regression models for each required quantile. This directly produces ordered quantile predictions, which is a more principled and robust statistical method for uncertainty quantification than additive bootstrapping of residuals, analogous to directly sampling quantiles from a posterior distribution. This approach inherently ensures quantile monotonicity and can capture heteroscedasticity.

import warnings
import numpy as np
import pandas as pd
import lightgbm as lgb
from sklearn.linear_model import LinearRegression


# Epsilon for log transformation to handle zero values
EPSILON = 0.1


def fit_and_predict_fn(
    train_x,
    train_y,
    test_x,
):
  # 1. Prepare data - ensure dates are datetime objects and get global dataframes
  train_x_copy = train_x.copy()
  train_y_copy = train_y.copy()
  test_x_copy = test_x.copy()

  train_x_copy['target_end_date'] = pd.to_datetime(
      train_x_copy['target_end_date']
  )
  test_x_copy['target_end_date'] = pd.to_datetime(
      test_x_copy['target_end_date']
  )

  # Access global dataframes (assuming they are pre-loaded in the notebook environment)
  global ilinet_state, locations

  ilinet_state_clean = ilinet_state[
      ilinet_state['region_type'] == 'States'
  ].copy()
  ilinet_state_clean['week_start'] = pd.to_datetime(
      ilinet_state_clean['week_start']
  )
  # Correct date alignment: Convert week_start (Sunday) to target_end_date (Saturday)
  ilinet_state_clean['target_end_date'] = ilinet_state_clean[
      'week_start'
  ] + pd.Timedelta(days=6)
  ilinet_state_clean = ilinet_state_clean.rename(
      columns={'region': 'location_name'}
  )
  ilinet_state_clean = ilinet_state_clean[
      ['target_end_date', 'location_name', 'unweighted_ili']
  ].copy()
  # Fill NaN unweighted_ili with 0 BEFORE any log transformations to represent no ILI activity
  ilinet_state_clean['unweighted_ili'] = ilinet_state_clean[
      'unweighted_ili'
  ].fillna(0)

  # Combine train_x and train_y
  train_df = train_x_copy
  train_df[TARGET_STR] = train_y_copy

  # Merge actual training data with available ILI data
  # This step ensures that the actual training data also has 'unweighted_ili' for overlap learning and feature generation
  train_df = pd.merge(
      train_df,
      ilinet_state_clean[
          ['target_end_date', 'location_name', 'unweighted_ili']
      ],
      on=['target_end_date', 'location_name'],
      how='left',
  )
  # Fill any NaNs in unweighted_ili introduced by the merge (if ILI was missing for some actual dates)
  train_df['unweighted_ili'] = train_df['unweighted_ili'].fillna(0)

  # Get unique location names from combined train_x and test_x to ensure all locations are processed
  all_locations_names = pd.concat(
      [train_df['location_name'], test_x_copy['location_name']]
  ).unique()
  # Create a mapping for location_name to its FIPS code and population for efficiency
  location_meta = locations[
      ['location_name', 'location', 'population']
  ].set_index('location_name')

  augmented_train_data = []

  # 2. Robust Augmentation with Historical ILINet Data (Strategy 2: Learn a Transformation)

  # Pre-calculate seasonal features for ILI data for the transformation model
  ilinet_state_clean['weekofyear'] = (
      ilinet_state_clean['target_end_date'].dt.isocalendar().week.astype(int)
  )
  ilinet_state_clean['sin_week'] = np.sin(
      2 * np.pi * ilinet_state_clean['weekofyear'] / 52
  )
  ilinet_state_clean['cos_week'] = np.cos(
      2 * np.pi * ilinet_state_clean['weekofyear'] / 52
  )

  # --- Learn a GLOBAL transformation model first as a fallback ---
  global_transformation_model = None
  # Add seasonal features to actual train_df for transformation model
  train_df['weekofyear'] = (
      train_df['target_end_date'].dt.isocalendar().week.astype(int)
  )
  train_df['sin_week'] = np.sin(2 * np.pi * train_df['weekofyear'] / 52)
  train_df['cos_week'] = np.cos(2 * np.pi * train_df['weekofyear'] / 52)

  # Convert to rates per 100,000 population for normalization
  train_df['admissions_rate'] = train_df[TARGET_STR] / (
      train_df['population'] / 100_000
  )
  train_df['ili_rate'] = train_df['unweighted_ili'] / (
      train_df['population'] / 100_000
  )
  ilinet_state_clean_with_pop = pd.merge(
      ilinet_state_clean,
      locations[['location_name', 'population']],
      on='location_name',
      how='left',
  )
  ilinet_state_clean_with_pop['ili_rate'] = ilinet_state_clean_with_pop[
      'unweighted_ili'
  ] / (ilinet_state_clean_with_pop['population'] / 100_000)
  ilinet_state_clean_with_pop['ili_rate'] = ilinet_state_clean_with_pop[
      'ili_rate'
  ].fillna(
      0
  )  # Fill NaN after division if population was missing

  # Use all available overlap data from train_df (actual admissions and ILI) to learn a general relationship
  # Filter for positive rates before log transform
  global_overlap_df = train_df[
      (train_df['admissions_rate'] > 0) & (train_df['ili_rate'] > 0)
  ].copy()

  if not global_overlap_df.empty:
    # Features for the transformation model now include seasonal components
    X_global_overlap = global_overlap_df[
        ['ili_rate', 'sin_week', 'cos_week']
    ].copy()
    X_global_overlap['ili_rate'] = np.log(
        X_global_overlap['ili_rate'] + EPSILON
    )  # Log transform ILI rates
    y_global_overlap = np.log(
        global_overlap_df['admissions_rate'] + EPSILON
    ).values.ravel()  # Log transform admissions rates

    # Ensure there's variance in features and target for meaningful regression
    if (
        X_global_overlap['ili_rate'].std() > 1e-6
        and y_global_overlap.std() > 1e-6
    ):
      global_transformation_model = LinearRegression()
      global_transformation_model.fit(X_global_overlap, y_global_overlap)
    else:
      warnings.warn(
          'Global transformation model cannot be trained due to constant ILI or'
          ' Admissions rates in overlap.',
          UserWarning,
      )
  else:
    warnings.warn(
        'No sufficient global overlap data (positive ILI/Admissions rates) to'
        ' train a fallback transformation model.',
        UserWarning,
    )

  for loc_name in all_locations_names:
    loc_train_df = train_df[train_df['location_name'] == loc_name].copy()
    loc_ilinet_df = ilinet_state_clean_with_pop[
        ilinet_state_clean_with_pop['location_name'] == loc_name
    ].copy()

    # Get location metadata (FIPS code, population)
    loc_fips = None
    loc_population = None
    if loc_name in location_meta.index:
      loc_meta = location_meta.loc[loc_name]
      loc_fips = loc_meta['location']
      loc_population = loc_meta['population']
    else:
      warnings.warn(
          f"Location '{loc_name}' not found in 'locations' metadata. Attempting"
          ' to use population/location from train_df if available.',
          UserWarning,
      )
      if (
          not loc_train_df.empty
          and 'population' in loc_train_df.columns
          and 'location' in loc_train_df.columns
      ):
        loc_population = loc_train_df['population'].iloc[0]
        loc_fips = loc_train_df['location'].iloc[0]
      else:
        warnings.warn(
            f"No population/location found for '{loc_name}'. Skipping synthetic"
            ' data generation.',
            UserWarning,
        )
        # Still append actual train data if available with potentially missing meta
        if not loc_train_df.empty:
          loc_train_df['source'] = 'actual'
          loc_train_df['horizon'] = 0
          augmented_train_data.append(
              loc_train_df.drop(columns=['admissions_rate', 'ili_rate'])
          )  # Drop rates before appending
        continue  # Skip synthetic data generation for this location

    transformation_model = None
    # Find overlap period for this specific location to learn local transformation
    # Filter for positive rates before log transform
    overlap_df_for_transform = loc_train_df[
        (loc_train_df['admissions_rate'] > 0) & (loc_train_df['ili_rate'] > 0)
    ].copy()

    # Attempt to learn a local transformation: Linear Regression (log(ILI_rate + EPS) + seasonal -> log(Admissions_rate + EPS))
    if not overlap_df_for_transform.empty:
      X_overlap = overlap_df_for_transform[
          ['ili_rate', 'sin_week', 'cos_week']
      ].copy()
      X_overlap['ili_rate'] = np.log(
          X_overlap['ili_rate'] + EPSILON
      )  # Log transform ILI rates
      y_overlap = np.log(
          overlap_df_for_transform['admissions_rate'] + EPSILON
      ).values.ravel()

      if X_overlap['ili_rate'].std() > 1e-6 and y_overlap.std() > 1e-6:
        transformation_model = LinearRegression()
        transformation_model.fit(X_overlap, y_overlap)
      else:
        warnings.warn(
            f'Skipping local transformation for {loc_name} due to constant'
            ' positive ILI or Admissions rates during overlap.',
            UserWarning,
        )
    else:
      warnings.warn(
          f'Skipping local transformation for {loc_name} due to insufficient'
          ' overlap or no positive ILI/Admissions rates.',
          UserWarning,
      )

    # Use local model if available, else global fallback
    effective_transformation_model = (
        transformation_model
        if transformation_model
        else global_transformation_model
    )

    if effective_transformation_model:
      # Create synthetic history for this location for dates before actual target data
      earliest_actual_flu_date = (
          loc_train_df['target_end_date'].min()
          if not loc_train_df.empty
          else pd.Timestamp.max
      )
      synthetic_dates_mask = (
          loc_ilinet_df['target_end_date'] < earliest_actual_flu_date
      )

      if not loc_ilinet_df[synthetic_dates_mask].empty:
        # Apply transformation to the entire historical ILI data (pre-actuals)
        synthetic_ili_data_for_transform = loc_ilinet_df[synthetic_dates_mask][
            ['ili_rate', 'sin_week', 'cos_week']
        ].copy()
        # Use log transform for prediction, handling zeros with EPSILON
        synthetic_ili_log_input = synthetic_ili_data_for_transform.copy()
        synthetic_ili_log_input['ili_rate'] = np.log(
            synthetic_ili_log_input['ili_rate'] + EPSILON
        )

        synthetic_admissions_log_rate = effective_transformation_model.predict(
            synthetic_ili_log_input
        )
        synthetic_admissions_rate = (
            np.exp(synthetic_admissions_log_rate) - EPSILON
        )

        # Convert back to counts
        synthetic_admissions = (
            (synthetic_admissions_rate * (loc_population / 100_000))
            .round()
            .astype(int)
        )
        synthetic_admissions = np.maximum(0, synthetic_admissions).flatten()

        synthetic_df = loc_ilinet_df[synthetic_dates_mask].copy()
        synthetic_df[TARGET_STR] = synthetic_admissions
        synthetic_df['source'] = 'synthetic'
        synthetic_df['population'] = (
            loc_population  # Add population to synthetic data
        )
        synthetic_df['location'] = loc_fips  # Add FIPS code
        synthetic_df['horizon'] = 0  # All historical data has horizon 0
        synthetic_df['unweighted_ili'] = loc_ilinet_df[synthetic_dates_mask][
            'unweighted_ili'
        ]  # Keep original ILI
        augmented_train_data.append(
            synthetic_df[[
                'target_end_date',
                'location_name',
                'location',
                'population',
                TARGET_STR,
                'unweighted_ili',
                'source',
                'horizon',
            ]]
        )
    else:
      warnings.warn(
          f'No transformation model available (local or global) for {loc_name}.'
          ' No synthetic data generated.',
          UserWarning,
      )

    # Always append actual training data (already merged with ILI if available)
    if not loc_train_df.empty:
      loc_train_df['source'] = 'actual'
      # Ensure population and FIPS are consistent
      loc_train_df['population'] = loc_population
      loc_train_df['location'] = loc_fips
      loc_train_df['horizon'] = 0
      # Drop the intermediate 'rate' and seasonal columns before appending
      augmented_train_data.append(
          loc_train_df.drop(
              columns=[
                  'admissions_rate',
                  'ili_rate',
                  'weekofyear',
                  'sin_week',
                  'cos_week',
              ]
          )
      )

  if not augmented_train_data:
    warnings.warn(
        'No data available after augmentation. Returning empty predictions.',
        UserWarning,
    )
    return pd.DataFrame(
        index=test_x_copy.index, columns=[f'quantile_{q}' for q in QUANTILES]
    )

  full_augmented_train_df = pd.concat(augmented_train_data, ignore_index=True)
  full_augmented_train_df = full_augmented_train_df.sort_values(
      by=['location_name', 'target_end_date']
  ).reset_index(drop=True)

  # --- Feature Engineering for the combined training data ---
  full_augmented_train_df['year'] = (
      full_augmented_train_df['target_end_date']
      .dt.isocalendar()
      .year.astype(int)
  )
  full_augmented_train_df['weekofyear'] = (
      full_augmented_train_df['target_end_date']
      .dt.isocalendar()
      .week.astype(int)
  )
  full_augmented_train_df['dayofyear'] = full_augmented_train_df[
      'target_end_date'
  ].dt.dayofyear.astype(int)
  full_augmented_train_df['sin_week'] = np.sin(
      2 * np.pi * full_augmented_train_df['weekofyear'] / 52
  )
  full_augmented_train_df['cos_week'] = np.cos(
      2 * np.pi * full_augmented_train_df['weekofyear'] / 52
  )

  # Normalize target and ILI by population for the main model
  full_augmented_train_df['admissions_rate'] = full_augmented_train_df[
      TARGET_STR
  ] / (full_augmented_train_df['population'] / 100_000)
  full_augmented_train_df['ili_rate'] = full_augmented_train_df[
      'unweighted_ili'
  ] / (full_augmented_train_df['population'] / 100_000)

  # Generate lagged features for admissions rates and ILI rates per location
  for lag in range(1, 9):  # Lags from 1 to 8
    full_augmented_train_df[f'lag_{lag}_admissions_rate'] = (
        full_augmented_train_df.groupby('location_name')['admissions_rate']
        .shift(lag)
        .fillna(0)
    )
    full_augmented_train_df[f'lag_{lag}_ili_rate'] = (
        full_augmented_train_df.groupby('location_name')['ili_rate']
        .shift(lag)
        .fillna(0)
    )

  # Apply log transform to target admissions_rate for modeling
  full_augmented_train_df['log_admissions_rate'] = np.log(
      full_augmented_train_df['admissions_rate'] + EPSILON
  )

  # Select features for the model (same features for training and prediction)
  features = [
      'population',
      'year',
      'weekofyear',
      'dayofyear',
      'sin_week',
      'cos_week',
      'horizon',
  ]
  for lag in range(1, 9):  # Updated lag range
    features.extend(
        [f'lag_{lag}_admissions_rate', f'lag_{lag}_ili_rate']
    )  # Using rates for lags

  # Convert population to float (LightGBM typically works better with numeric types)
  full_augmented_train_df['population'] = full_augmented_train_df[
      'population'
  ].astype(float)

  # Make location_name a categorical feature for LightGBM (proxy for hierarchical modeling)
  full_augmented_train_df['location_name_cat'] = full_augmented_train_df[
      'location_name'
  ].astype('category')
  features.append('location_name_cat')
  categorical_features = ['location_name_cat']

  # 3. Model Training (Multiple LightGBM Quantile Regressors)
  quantile_models = {}
  lgbm_params_base = {
      'objective': 'quantile',  # Changed objective to quantile regression
      'metric': 'quantile',
      'n_estimators': 1200,  # Increased estimators for better learning capacity
      'learning_rate': (
          0.02
      ),  # Slightly reduced learning rate to prevent overfitting
      'feature_fraction': 0.8,
      'bagging_fraction': 0.8,
      'bagging_freq': 1,
      'lambda_l1': 0.1,
      'lambda_l2': 0.1,
      'num_leaves': 64,  # Increased complexity
      'max_depth': 8,  # Added max_depth to control overfitting
      'verbose': -1,  # Suppress verbose output
      'n_jobs': -1,
      'seed': 42,
      'boosting_type': 'gbdt',
  }

  # Drop rows where target or features are NaN (e.g., at the very beginning of the series for lags)
  train_data_final = full_augmented_train_df.dropna(
      subset=['log_admissions_rate'] + features
  )
  X_train = train_data_final[features]
  y_train_log_rate = train_data_final['log_admissions_rate']

  if X_train.empty or y_train_log_rate.empty:
    warnings.warn(
        'Training data for LightGBM is empty after preparation. Returning empty'
        ' predictions.',
        UserWarning,
    )
    return pd.DataFrame(
        index=test_x_copy.index, columns=[f'quantile_{q}' for q in QUANTILES]
    )

  for q in QUANTILES:
    lgbm_params = lgbm_params_base.copy()
    lgbm_params['alpha'] = q  # Set alpha for quantile regression
    model = lgb.LGBMRegressor(**lgbm_params)
    model.fit(
        X_train, y_train_log_rate, categorical_feature=categorical_features
    )
    quantile_models[q] = model

  # 4. Generate Quantile Predictions for test_x using iterative forecasting
  test_predictions_list = []

  # Loop through each location and predict horizons iteratively
  for loc_name in test_x_copy['location_name'].unique():
    loc_test_x = (
        test_x_copy[test_x_copy['location_name'] == loc_name]
        .sort_values('target_end_date')
        .copy()
    )

    # Initialize lags for this location from the last known training data point
    # Need enough history for all 8 lags
    last_train_data_for_loc = (
        full_augmented_train_df[
            (full_augmented_train_df['location_name'] == loc_name)
            & (
                full_augmented_train_df['target_end_date']
                < loc_test_x['target_end_date'].min()
            )
        ]
        .sort_values('target_end_date')
        .tail(8)
    )  # Get enough history for all 8 lags

    # Extract initial lagged values of rates, defaulting to 0 if not enough history
    current_lag_admissions_rate = {}
    for lag in range(1, 9):  # Updated lag range
      current_lag_admissions_rate[lag] = (
          last_train_data_for_loc['admissions_rate'].iloc[-lag]
          if len(last_train_data_for_loc) >= lag
          else 0
      )

    current_lag_ili_rate = {}
    for lag in range(1, 9):  # Updated lag range
      current_lag_ili_rate[lag] = (
          last_train_data_for_loc['ili_rate'].iloc[-lag]
          if len(last_train_data_for_loc) >= lag
          else 0
      )

    # Get location population for back-transformation
    loc_population = loc_test_x['population'].iloc[
        0
    ]  # Population should be constant per location

    loc_horizon_predictions_quantiles = pd.DataFrame(
        index=loc_test_x.index, columns=[f'quantile_{q}' for q in QUANTILES]
    )

    # Prepare static features for test_x (seasonal, population, location_cat)
    loc_test_x_prepared = loc_test_x.copy()
    loc_test_x_prepared['year'] = (
        loc_test_x_prepared['target_end_date'].dt.isocalendar().year.astype(int)
    )
    loc_test_x_prepared['weekofyear'] = (
        loc_test_x_prepared['target_end_date'].dt.isocalendar().week.astype(int)
    )
    loc_test_x_prepared['dayofyear'] = loc_test_x_prepared[
        'target_end_date'
    ].dt.dayofyear.astype(int)
    loc_test_x_prepared['sin_week'] = np.sin(
        2 * np.pi * loc_test_x_prepared['weekofyear'] / 52
    )
    loc_test_x_prepared['cos_week'] = np.cos(
        2 * np.pi * loc_test_x_prepared['weekofyear'] / 52
    )
    loc_test_x_prepared['population'] = loc_test_x_prepared[
        'population'
    ].astype(float)
    loc_test_x_prepared['location_name_cat'] = loc_test_x_prepared[
        'location_name'
    ].astype('category')

    for i, (original_idx, row) in enumerate(loc_test_x_prepared.iterrows()):
      # Construct feature vector for current horizon, dynamically updating admissions lags
      current_features_dict = row[[
          'population',
          'year',
          'weekofyear',
          'dayofyear',
          'sin_week',
          'cos_week',
          'location_name_cat',
          'horizon',
      ]].to_dict()

      for lag_val in range(1, 9):  # Updated lag range
        current_features_dict[f'lag_{lag_val}_admissions_rate'] = (
            current_lag_admissions_rate[lag_val]
        )
        current_features_dict[f'lag_{lag_val}_ili_rate'] = current_lag_ili_rate[
            lag_val
        ]

      # Create a DataFrame for prediction
      current_X_test_row = pd.DataFrame([current_features_dict])
      current_X_test_row['location_name_cat'] = current_X_test_row[
          'location_name_cat'
      ].astype('category')

      # Ensure columns are in the same order as training
      current_X_test_row = current_X_test_row[features]

      # Predict all quantiles for the current horizon
      predicted_log_rates_for_horizon = {}
      for q in QUANTILES:
        predicted_log_rates_for_horizon[q] = quantile_models[q].predict(
            current_X_test_row
        )[0]

      # Back-transform and ensure non-negative integers
      predicted_rates_for_horizon = {
          q: np.exp(val) - EPSILON
          for q, val in predicted_log_rates_for_horizon.items()
      }
      predicted_admissions_for_horizon = {
          q: (
              np.maximum(0, (val * (loc_population / 100_000)))
              .round()
              .astype(int)
          )
          for q, val in predicted_rates_for_horizon.items()
      }

      for q in QUANTILES:
        loc_horizon_predictions_quantiles.loc[original_idx, f'quantile_{q}'] = (
            predicted_admissions_for_horizon[q]
        )

      # Update admissions rates lags for the next horizon using the median prediction
      median_prediction_rate_for_lag_update = predicted_rates_for_horizon[
          0.5
      ]  # Use the median rate for lag update
      for lag_idx in sorted(current_lag_admissions_rate.keys(), reverse=True):
        if lag_idx > 1:
          current_lag_admissions_rate[lag_idx] = current_lag_admissions_rate[
              lag_idx - 1
          ]
      current_lag_admissions_rate[1] = median_prediction_rate_for_lag_update

      # ILI lags remain static as future ILI values are unknown; they were set once at the beginning.

    # Ensure monotonicity of quantiles per row (a safeguard, quantile regression should mostly handle this)
    for result_idx, row_val in loc_horizon_predictions_quantiles.iterrows():
      loc_horizon_predictions_quantiles.loc[result_idx] = np.sort(
          row_val.values
      )

    test_predictions_list.append(loc_horizon_predictions_quantiles)

  if not test_predictions_list:
    warnings.warn(
        'No predictions generated for test set. Returning empty DataFrame.',
        UserWarning,
    )
    return pd.DataFrame(
        index=test_x_copy.index, columns=[f'quantile_{q}' for q in QUANTILES]
    )

  test_y_hat_quantiles = pd.concat(test_predictions_list)

  # Reindex to match the input test_x, important for evaluation framework
  test_y_hat_quantiles = test_y_hat_quantiles.reindex(test_x_copy.index)

  # Ensure final predictions are non-negative and integer
  test_y_hat_quantiles = np.maximum(0, test_y_hat_quantiles).astype(int)

  return test_y_hat_quantiles


def main(argv):
  del argv  # Unused.
  locations = locations[locations['location'].isin(REQUIRED_CDC_LOCATIONS)]
  locations['location'] = locations['location'].astype(int)
  location_codes = locations['location'].unique()

  print('Locations sample:')
  print(locations.head())

  dataset = pd.read_csv(f'{INPUT_DIR}/dataset.csv')
  dataset['target_end_date'] = pd.to_datetime(
      dataset['target_end_date']
  ).dt.date

  print('Dataset sample (check for existence of most recent data):')
  print(dataset.sort_values(by=['target_end_date'], ascending=False).head())

  dataset['Total Influenza Admissions'] = (
      pd.to_numeric(dataset['Total Influenza Admissions'], errors='coerce')
      .replace({np.nan: np.nan})
      .astype('Int64')
  )

  # --- Execute Validation Run ---
  print('--- Starting Validation Run ---')
  # Define validation and test periods

  validation_date_end = get_most_recent_saturday_date_str()
  validation_date_start = pd.to_datetime(validation_date_end) - pd.Timedelta(
      weeks=3
  )

  validation_reference_dates = get_saturdays_between_dates(
      validation_date_start, validation_date_end
  )
  print('validation_reference_dates:', validation_reference_dates)
  validation_forecasts, validation_score = compute_rolling_evaluation(
      observed_values=dataset.copy(),
      reference_dates=validation_reference_dates,
      fit_and_predict_fn=fit_and_predict_fn,
      horizons=HORIZONS,
      location_codes=location_codes,
      locations_df=locations,
  )

  print(f'\nValidation Score: {validation_score}')
  if not validation_forecasts.empty:
    validation_forecasts.to_csv('/tmp/validation_forecasts.csv', index=False)
    print("Validation forecasts saved to '/tmp/validation_forecasts.csv'")

  # Plot forecast and predictions on validation dates against observed data

  validation_forecasts['target_end_date'] = pd.to_datetime(
      validation_forecasts['target_end_date']
  )
  validation_forecasts['reference_date'] = pd.to_datetime(
      validation_forecasts['reference_date']
  )

  # Prepare the observed data
  national_observed_all = (
      dataset.groupby('target_end_date')['Total Influenza Admissions']
      .sum()
      .reset_index()
  )
  national_observed_all['target_end_date'] = pd.to_datetime(
      national_observed_all['target_end_date']
  )

  dates_to_plot_validation = [
      {
          'start': pd.to_datetime(validation_date_start) - timedelta(weeks=2),
          'end': pd.to_datetime(validation_date_end) + timedelta(weeks=5),
          'name': 'validation',
      },
  ]

  for season in dates_to_plot_validation:
    print(f"--- Generating plot for {season['name']} dates ---")
    plot_season_forecasts(
        season_start=season['start'],
        season_end=season['end'],
        season_name=season['name'],
        all_forecasts_df=validation_forecasts,
        national_observed_df=national_observed_all,
        step_size=1,
    )

  submission_date_str = get_next_saturday_date_str()
  submission_date = pd.to_datetime(submission_date_str).date()

  test_forecasts, _ = compute_rolling_evaluation(
      observed_values=dataset.copy(),
      reference_dates=[submission_date],
      fit_and_predict_fn=fit_and_predict_fn,
      horizons=HORIZONS,
      location_codes=location_codes,
      locations_df=locations,
  )

  print('\n--- Creating the submission file ---')

  if not test_forecasts.empty:
    cdc_submission = format_for_cdc(test_forecasts, 'wk inc flu hosp')
    cdc_submission.to_csv(
        f'/tmp/{submission_date_str}_{MODEL_NAME}.csv', index=False
    )
    print(
        'Submission forecasts saved to'
        f" '/tmp/{submission_date_str}_{MODEL_NAME}.csv'"
    )

    print('Verify final submission file:')
    print(cdc_submission)

    # Convert dates in test_forecasts to Timestamp
    test_forecasts['target_end_date'] = pd.to_datetime(
        test_forecasts['target_end_date']
    )
    test_forecasts['reference_date'] = pd.to_datetime(
        test_forecasts['reference_date']
    )

    # Plot forecasts for submission (all horizons)
    cdc_submission['target_end_date'] = pd.to_datetime(
        cdc_submission['target_end_date']
    )
    cdc_submission['reference_date'] = pd.to_datetime(
        cdc_submission['reference_date']
    )

    dates_to_plot_submission = [
        {
            'start': pd.to_datetime(submission_date) - timedelta(weeks=1),
            'end': pd.to_datetime(submission_date) + timedelta(weeks=3),
            'name': f'{submission_date} forecast',
        },
    ]

    for season in dates_to_plot_submission:
      print(f"--- Generating plot for {season['name']} dates ---")
      plot_season_forecasts(
          season_start=season['start'],
          season_end=season['end'],
          season_name=season['name'],
          all_forecasts_df=test_forecasts,
          national_observed_df=None,
          step_size=1,
      )


if __name__ == '__main__':
  app.run(main)
