include './configs/parser.gin'
include './configs/model.gin'

import losses.base_losses
import losses.maskrcnn_losses
import losses.multitask_losses
import losses.optimizers
import metrics.coco_metrics
import metrics.metrics
import metrics.multitask_metrics
import utils.trainer_utils
import utils.checkpoint_utils
import utils.dataset_utils
import utils.task_utils

TRAIN_STEPS = 184400 # Default TRAIN_STEPS=46100 for TRAIN_BS=256 (100 epochs).
OUTPUT_SIZE = 1024
ATTENTION_SHIFT_SIZE = 8 # Non-shift setting uses 0.
ANCHOR_SIZE = 8
MIN_LEVEL = 2
MAX_LEVEL = 5
TEMP = 0.01
MAX_NUM_INSTANCES = 300
TRAIN_FILE_PATTERN = './datasets/coco/train-*'
EVAL_FILE_PATTERN = './datasets/coco/val-*'
TRAIN_EMBED_PATH = './dito/embeddings/coco_embed.npy' # lvis_base_embed.npy
EVAL_EMBED_PATH = './dito/embeddings/coco_embed.npy' # lvis_embed.npy
CATG_PAD_SIZE = 92 # Max # of categories among training datasets (LVIS: 1204).
EVAL_STEPS = 5000 # EVAL_STEPS * EVAL_BS = # eval examples.
CLIP_NAME = 'vit_l16'

TRAIN_BS = 1  # Default TRAIN_BS=256 for 4x4 TPUv3. Use 1 to fit a 1080 GPU.
EVAL_BS = 1
AUG_SCALE_MIN = 0.1
AUG_SCALE_MAX = 2.0
WEIGHT_DECAY = 1e-4
DTYPE = %jnp.float32 # Set to bfloat16 for time/memory savings.
BN_GROUP_SIZE = %TRAIN_BS # Sync Batchnorm across the whole batch.
INCLUDE_MASK = False
METRICS_DIR_NAME = 'coco_metrics'  # Temporarily save the output metrics.

VIT_NAME = 'vit_l16'
TEXT_DIM = 1024
BACKBONE_LR_SCALE = 0.6
CKPT_DIR = './dito/checkpoints/pretrained_dito_large/'
PRETRAINED_DETECTOR_CKPT_PATH = ''

train/get_input.return_dataset = True
get_input_mixture:
  # Users can add more datasets as needed as comma separated list.
  loader_fns = [@train/get_input,]
  batch_size = %TRAIN_BS
  sampling_weights = None
  feature_names = ['images', 'texts', 'labels']
  label_names = ['labels']

# Vocab embeddings
train/load_dataset_vocab_embed:
  embedding_path = %TRAIN_EMBED_PATH
eval/load_dataset_vocab_embed:
  embedding_path = %EVAL_EMBED_PATH

set_remap:
  MultitaskModel___call__ = {'images': 'image', 'texts': 'text'}

heads.FastrcnnHead:
  use_batch_norm = False
  use_norm_classifier = True

heads.RpnHead:
  use_batch_norm = False
  anchors_per_location = 1

heads.MaskrcnnHead:
  use_batch_norm = False
  use_norm_activation = True

# Override the default mask parser.
maskrcnn_map_fn.include_mask = %INCLUDE_MASK
ClipFasterRCNNHead.include_mask = %INCLUDE_MASK
MaskRCNNLoss.include_masks = %INCLUDE_MASK
MaskRCNNLoss.class_box_regression = %CLS_BOX_REG
MaskRCNNLoss.frcnn_background_weight = 0.9

# Det improvement
ClipFasterRCNNHead.roi_scale_factor = 0.7
ClipFasterRCNNHead.objectness_weight = 3.0
ClipFasterRCNNHead.output_decoded_boxes = True
maskrcnn_map_fn.use_centerness = True
maskrcnn_map_fn.use_lrtb_box_targets = True
maskrcnn_map_fn.aspect_ratios = [1.0,]
MaskRCNNLoss.use_centerness_rpn = True
MaskRCNNLoss.use_box_iou_loss = True
# ViTDet setting
ClipFasterRCNNHead.roi_output_size = 14
SimpleFpnV2.min_level = %MIN_LEVEL
SimpleFpnV2.max_level = %MAX_LEVEL
ClipFasterRCNNHead.feature_pyramid = @SimpleFpnV2
ClipFasterRCNNHead.roi_head_fn = @AveragePool

# Eval setting
process_and_generate_detections.class_is_logit = False
process_and_generate_detections.post_nms_num_detections = %MAX_NUM_INSTANCES
process_and_generate_detections.score_threshold = 0
ClipFasterRCNNHead.base_vlm_weight = 0.35
ClipFasterRCNNHead.novel_vlm_weight = 0.65
ClipFasterRCNNHead.clip_sim_temp = 0.01

# Define tasks
DetectionTask._loss = @MaskRCNNLoss()
DetectionTask.head = @ClipFasterRCNNHead
DetectionTask.metric = %coco_metric
TASKS = (@DetectionTask(),)

MultitaskModel.tasks = %TASKS
MultitaskModel.dtype = %DTYPE

get_clip_vision_model.model_name = %VIT_NAME
get_clip_vision_model.window_attention_shift_size = %ATTENTION_SHIFT_SIZE
maskrcnn_map_fn.normalize_image_values = @clip_image_normalization_values()
maskrcnn_map_fn.denormalize_image = True
MultitaskModel.vision_model_fn = @get_clip_vision_model()
MultitaskModel.train_vision_model = True

# Learning rates.
step_learning_rate_with_linear_warmup.init_learning_rate = 0.02  # Default 0.18.
step_learning_rate_with_linear_warmup.warmup_learning_rate = 0.0032
step_learning_rate_with_linear_warmup.warmup_steps = 1000
step_learning_rate_with_linear_warmup.total_steps = %TRAIN_STEPS
step_learning_rate_with_linear_warmup.learning_rate_step_ratios = [0.8, 0.9, 0.95]
step_learning_rate_with_linear_warmup.decay_factor = 0.1

# Define losses.
get_multitask_loss_fn.tasks = %TASKS
weight_decay_loss_wrapper.loss_fn = @get_multitask_loss_fn()
weight_decay_loss_wrapper.factor = %WEIGHT_DECAY
weight_decay_loss_wrapper.exclude = [
    'bn', 'temperature', 'attnpool', 'positional_embedding',
    'ln', 'layer_norm', 'LayerNorm', 'bias', 'vision_model',
]
optimizers.submodule_gradient_scaling:
  filter_regex_scales = {'vision_model': %BACKBONE_LR_SCALE,}

# Training the model.
train.process_gradients_fn = @submodule_gradient_scaling
train.debug_grad_norm = False
train.input_fn = @get_input_mixture
train.model_fn = @MultitaskModel
train.optimizer_def = @Momentum()
train.loss_fn = @weight_decay_loss_wrapper()
train.total_train_steps = %TRAIN_STEPS
train.learning_rate = @step_learning_rate_with_linear_warmup
train.pretrained_clip = %CKPT_DIR
train.pretrained_detector = %PRETRAINED_DETECTOR_CKPT_PATH
train.clip_target_filters = [
    ('task_heads_0', 'feature_pyramid_fn'),
    ('task_heads_0', 'fastrcnn_head_fn'),
    ('vision_model', '.+'),
]
train.clip_pretrained_filters = [
    ('', 'feature_pyramid_fn'),
    ('', 'fastrcnn_head_fn'),
    ('', '.+'),
]
train.log_flops = False  # Disable to save launch time.

# Evaluating the model.
get_host_evaluator.evaluator_name = %coco_metrics.EvaluatorName.COCO
get_host_evaluator.include_mask = %INCLUDE_MASK
get_host_evaluator.annotation_file = './datasets/coco/instances_val2017.json'

evaluate.input_fn = @eval/get_input
evaluate.model_fn = @MultitaskModel
evaluate.optimizer_def = @Momentum()
evaluate.eval_steps = %EVAL_STEPS
evaluate.host_evaluator = @get_host_evaluator()
evaluate.total_train_steps = %TRAIN_STEPS
evaluate.metrics_dir_name = %METRICS_DIR_NAME
evaluate.pretrained_detector = %PRETRAINED_DETECTOR_CKPT_PATH

# Frozen backbone
get_clip_frozen_vision_model.model_name = %VIT_NAME
MultitaskModel.frozen_vision_model_fn = @get_clip_frozen_vision_model()
ClipFasterRCNNHead.use_frozen_vlm = True
train.frozen_clip_target_filters = [
    ('task_heads_0', 'feature_pyramid_fn'),
    ('task_heads_0', 'fastrcnn_head_fn'),
    ('frozen_vision_model', ('.+')),
]
evaluate.frozen_clip_target_filters = [
    ('task_heads_0', 'feature_pyramid_fn'),
    ('task_heads_0', 'fastrcnn_head_fn'),
    ('frozen_vision_model', ('.+')),
]
