# Region-Centric Image-Language Pretraining for Open-Vocabulary Detection

This is a JAX/Flax implementation of DITO [Region-Centric Image-Language Pretraining for Open-Vocabulary Detection](https://arxiv.org/abs/2310.00161). The model is also supported on the Cloud Vertex API [here](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/150), where you can predict with this model on Google Cloud Vertex AI Prediction service using the provided notebook at the top of the model card.

## Installation
We use the Python built-in virtual env to set up the environment. Run the following commands:

```
svn export https://github.com/google-research/google-research/trunk/fvlm

PATH_TO_VENV=/path/to/your/venv
python3 -m venv ${PATH_TO_VENV}
source ${PATH_TO_VENV}/bin/activate
```

Install the requirements from the root fvlm directory.

```
pip install -r requirements.txt
pip install -e .
```

For GPU training, please refer to [this github page](https://github.com/google/jax/issues/13637) for installation instructions.


## Download the DITO checkpoint and precomputed text embeddings.
Run the following commands from the root directory.

```
cd ./dito/checkpoints
./download.sh

cd ../embeddings
./download.sh
```

## Run the demo.

Run the following command from the root directory. This will run the DITO demo.

```
python ./dito/demo.py
```

You can set demo image and visualization options by the command line flags. Please refer to demo.py for more documentation on the flags.
We note that the demo model was pretrained on DataComp-1B and finetuned on the base categories of LVIS.

## Train and evaluate DITO.

Here we describe the steps to use the [COCO](https://cocodataset.org/#home) dataset for training and evaluation as an example. To use any custom dataset, users would need to follow the similar setup.

* Follow the steps [here](https://cloud.google.com/tpu/docs/tutorials/mask-rcnn-2.x#prepare-coco) to set up the COCO dataset and move it to datasets/coco. The coco directory should contain train*.tfrecord, val*.tfrecord, and instances_val2017.json (the standard COCO evaluation [file](https://cocodataset.org/#download)).

* Run the following command from the root directory:

```
OUTPUT_DIR="/your/output/dir"

./dito/train_and_eval.sh "${OUTPUT_DIR}"
```

## Set up custom datasets.

Here we describe the specific changes needed in ./dito/configs/dito_train_and_eval.gin to set up training/evaluation with custom datasets.

* Update TRAIN_FILE_PATTERN and EVAL_FILE_PATTERN to point to your dataset.
* Update TRAIN_EMBED_PATH and EVAL_EMBED_PATH to point to your cached embedding.npy.
* Update CATG_PAD_SIZE to the number of your training categories.
* Update EVAL_STEPS to the number of your validation set size.

## Citation
```
@article{kim2024dito,
  title={Region-Centric Image-Language Pretraining for Open-Vocabulary Detection},
  author={Dahun Kim and Anelia Angelova and Weicheng Kuo},
  booktitle={European conference on computer vision},
  year={2024},
  organization={Springer}
}
```

## Disclaimer
This is not an officially supported Google product.