{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWnXXZSnQ8gt"
      },
      "source": [
        "Copyright 2024 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use\n",
        "this file except in compliance with the License. You may obtain a copy of the\n",
        "License at\n",
        "\n",
        "```\n",
        " http://www.apache.org/licenses/LICENSE-2.0\n",
        "```\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed\n",
        "under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR\n",
        "CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
        "specific language governing permissions and limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hL_MDBGFhXs"
      },
      "source": [
        "# Code to reproduce primary analyses of Pfohl and Cole-Lewis et al. \"A toolbox for surfacing health equity biases and harms in large language models\".\n",
        "\n",
        "This Python code produces the point estimates and confidence intervals that\n",
        "underpin Figures 2, 3, and 4. This includes the estimates of the reported rates\n",
        "for the three rubrics (independent, pairwise, and counterfactual) for each rater\n",
        "group. This notebook does not reproduce the analyses reflected in Extended Data\n",
        "or Supplementary figures and tables.\n",
        "\n",
        "Before executing this notebook, please extract the supplementary data file(s)\n",
        "from the links provided in the article. This notebook can be executed using\n",
        "either (1) the xlsx workbook (`equitymedqa_ratings.xlsx`) or (2) the csv files\n",
        "(`ratings_independent.csv`, `ratings_pairwise.csv`, and\n",
        "`ratings_counterfactual.csv`).\n",
        "\n",
        "Disclaimer: The bootstrap confidence intervals generated in this code are\n",
        "sensitive to the random seed (or reshuffling of the row order), but deviations\n",
        "are small (generally $\u003c0.01$). Increasing the number of bootstrap iterates\n",
        "reduces variability. Minor deviations from the confidence interval bounds\n",
        "reported in the paper are expected due to differences in the row order in the\n",
        "extract released as supplementary data. This issue does not affect point\n",
        "estimates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndO0c_TkE6o1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Any, Callable\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgG9cMCBpAJ8"
      },
      "source": [
        "### User specified parameters\n",
        "\n",
        "1.  `CSV_MODE`: Set to False if using the xlsx workbook and to True if using the\n",
        "    csv files.\n",
        "2.  `DATA_PATH`: Path to a directory where the data files are stored\n",
        "3.  `N_RESAMPLES`: The number of bootstrap iterations to use for confidence\n",
        "    interval computations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFqPlmOqpQ4O"
      },
      "outputs": [],
      "source": [
        "CSV_MODE = False  # @param\n",
        "DATA_PATH = './'  # @param {type: \"string\"}\n",
        "N_RESAMPLES = 1000  # @param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydG3Snc-uIng"
      },
      "outputs": [],
      "source": [
        "data_dict = {}\n",
        "if CSV_MODE:\n",
        "  data_dict['independent'] = pd.read_csv(\n",
        "      os.path.join(DATA_PATH, 'ratings_independent.csv')\n",
        "  )\n",
        "  data_dict['pairwise'] = pd.read_csv(\n",
        "      os.path.join(DATA_PATH, 'ratings_pairwise.csv')\n",
        "  )\n",
        "  data_dict['counterfactual'] = pd.read_csv(\n",
        "      os.path.join(DATA_PATH, 'ratings_counterfactual.csv')\n",
        "  )\n",
        "else:\n",
        "  data_dict['independent'] = pd.read_excel(\n",
        "      os.path.join(DATA_PATH, 'equitymedqa_ratings.xlsx'),\n",
        "      sheet_name='Independent ratings',\n",
        "  )\n",
        "  data_dict['pairwise'] = pd.read_excel(\n",
        "      os.path.join(DATA_PATH, 'equitymedqa_ratings.xlsx'),\n",
        "      sheet_name='Paired ratings',\n",
        "  )\n",
        "  data_dict['counterfactual'] = pd.read_excel(\n",
        "      os.path.join(DATA_PATH, 'equitymedqa_ratings.xlsx'),\n",
        "      sheet_name='Counterfactual Paired ratings',\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGdBftAibZjZ"
      },
      "outputs": [],
      "source": [
        "# @title Stat functions\n",
        "def bootstrap_metric(\n",
        "    x: np.ndarray,\n",
        "    metric_fn: Callable[[Any], float] = np.mean,\n",
        "    n_resamples: int = 1000,\n",
        "    return_string: str = False,\n",
        "    vectorized: bool = True,\n",
        "    boot_method: str = 'bca',\n",
        "    random_state=101,\n",
        ") -\u003e str | tuple[float, float]:\n",
        "  \"\"\"Bootstraps a metric with the option to return a formatted string.\n",
        "\n",
        "  Args:\n",
        "    x: A sequence of array-like; matching input spec for scipy.stats.bootstrap\n",
        "    metric_fn: A statistical function to bootstrap. Matches the input spec for\n",
        "      the `statistic` argument to scipy.stats.bootstrap.\n",
        "    n_resamples: The number of bootstrap iterations.\n",
        "    return_string: If true, the confidence interval is returned as a string; if\n",
        "      false, it is returned as a tuple.\n",
        "    vectorized: Argument passed to scipy.stats.bootstrap.\n",
        "    boot_method: The bootstrap method to use. Passes directly to the `method`\n",
        "      argument of scipy.stats.bootstrap. Valid values are 'bca, 'percentile',\n",
        "      and 'basic'.\n",
        "    random_state: The random seed. Matches the input spec for the random_state\n",
        "      argument to scipy.stats.bootstrap.\n",
        "\n",
        "  Returns:\n",
        "    The confidence interval bounds as a string or tuple.\n",
        "  \"\"\"\n",
        "  result = scipy.stats.bootstrap(\n",
        "      (x,),\n",
        "      statistic=metric_fn,\n",
        "      n_resamples=n_resamples,\n",
        "      method=boot_method,\n",
        "      vectorized=vectorized,\n",
        "      random_state=random_state,\n",
        "  )\n",
        "  if return_string:\n",
        "    if not np.isnan(result.confidence_interval[0]):\n",
        "      return (\n",
        "          f'({result.confidence_interval[0]:.3f},'\n",
        "          f' {result.confidence_interval[1]:.3f})'\n",
        "      )\n",
        "    else:\n",
        "      return ''\n",
        "  else:\n",
        "    return (result.confidence_interval[0], result.confidence_interval[1])\n",
        "\n",
        "\n",
        "def combine_point_estimates_and_cis(estimate_df, ci_df):\n",
        "  \"\"\"Combines a dataframe of point estimates with a dataframe of CIs produced by\n",
        "\n",
        "    a call to bootstrap_metric.\n",
        "\n",
        "  Args:\n",
        "    estimate_df: A pd.DataFrame containing point estimates.\n",
        "    ci_df: A pd.DataFrame of the same size and semantics of estimate_df\n",
        "      containing confidence intervals. Canonically, this is an output of\n",
        "      bootstrap_metric with `return_string`=True.\n",
        "\n",
        "  Returns:\n",
        "    A combined dataframe.\n",
        "  \"\"\"\n",
        "  return estimate_df.map(lambda x: f'{x:.3f}').combine(\n",
        "      ci_df, lambda x, y: x + ' ' + y\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fffSlXAHvUq1"
      },
      "outputs": [],
      "source": [
        "# In this block, we define lists of strings for later use\n",
        "\n",
        "# Define the set of datasets included in EquityMedQA\n",
        "equitymedqa_names = [\n",
        "    'Open-ended Medical Adversarial Queries (OMAQ)',\n",
        "    'Equity in Health AI (EHAI)',\n",
        "    'Failure-Based Red Teaming - Manual (FBRT-Manual)',\n",
        "    'Failure-Based Red Teaming - LLM (FBRT-LLM)',\n",
        "    'TRopical and INfectious DiseaseS (TRINDS)',\n",
        "    'Counterfactual Context - Manual (CC-Manual)',\n",
        "    'Counterfactual Context - LLM (CC-LLM)',\n",
        "]\n",
        "\n",
        "# Define the order in which to display datasets in tables\n",
        "dataset_order = (\n",
        "    ['HealthSearchQA', 'EquityMedQA']\n",
        "    + equitymedqa_names\n",
        "    + ['Mixed MMQA-OMAQ', 'Filtered Mixed MMQA-OMAQ', 'Omiye et al.']\n",
        ")\n",
        "\n",
        "# Define the order in which to display rater groups\n",
        "rater_type_order = ['Physician', 'Health equity expert']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S5nGQBBZ7mA"
      },
      "source": [
        "## Independent Rubric: reproduce Figure 2 results\n",
        "\n",
        "Here, we compute the rate that each rater group reported bias for each dataset\n",
        "under the independent assessment rubric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sav-OO12F4fW"
      },
      "outputs": [],
      "source": [
        "independent_df = data_dict['independent'].query('rater_type != \"Consumer\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yhqn-x1H383"
      },
      "outputs": [],
      "source": [
        "# Create separate EquityMedQA set as aggregation over individual datasets.\n",
        "equitymedqa_df = independent_df.query('dataset in @equitymedqa_names')\n",
        "equitymedqa_df = equitymedqa_df.assign(dataset='EquityMedQA')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmJbSe3mg_8N"
      },
      "outputs": [],
      "source": [
        "# Create a filtered copy of the Mixed MMQA-OMAQ dataset that removes cases without three ratings.\n",
        "mmqa_omaq_df = independent_df.query('dataset == \"Mixed MMQA-OMAQ\"')\n",
        "mmqa_omaq_count_df = (\n",
        "    mmqa_omaq_df.groupby(['question_id', 'dataset', 'rater_type'])['rater_id']\n",
        "    .count()\n",
        "    .rename('rater_count')\n",
        "    .to_frame()\n",
        "    .query('rater_count == 3')\n",
        "    .reset_index()\n",
        ")\n",
        "filtered_mmqa_omaq_df = mmqa_omaq_df.merge(mmqa_omaq_count_df).assign(\n",
        "    dataset='Filtered Mixed MMQA-OMAQ'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xwq1hRfyhBoM"
      },
      "outputs": [],
      "source": [
        "# Concatenate dataframes\n",
        "independent_df_concat = pd.concat(\n",
        "    [independent_df, equitymedqa_df, filtered_mmqa_omaq_df]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7l3NkbIXovfn"
      },
      "outputs": [],
      "source": [
        "# Set fields to categorical to control display order\n",
        "independent_df_concat['dataset'] = pd.Categorical(\n",
        "    independent_df_concat['dataset'], dataset_order\n",
        ")\n",
        "independent_df_concat['rater_type'] = pd.Categorical(\n",
        "    independent_df_concat['rater_type'], rater_type_order\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuNh3wGbGewQ"
      },
      "outputs": [],
      "source": [
        "# Create bias_presence_binary column that combined minor and severe bias into one category.\n",
        "independent_df_concat['bias_presence_binary'] = (\n",
        "    independent_df_concat['bias_presence'] != 'No bias'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ol1KaZry0ICI"
      },
      "outputs": [],
      "source": [
        "# Define a list of columns corresponding to rubric dimensions.\n",
        "bias_columns = [\n",
        "    'bias_presence_binary',\n",
        "    'inaccuracy_for_some_axes',\n",
        "    'not_inclusive_for_some_axes',\n",
        "    'stereotypical_language_characterization',\n",
        "    'omits_systemic_explanation',\n",
        "    'failure_to_challenge',\n",
        "    'potential_for_disproportionate',\n",
        "    'other_bias',\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPEZzTQtGEG6"
      },
      "outputs": [],
      "source": [
        "# Compute the estimates of the rates.\n",
        "independent_results_mean = independent_df_concat.groupby(\n",
        "    ['dataset', 'rater_type']\n",
        ")[bias_columns].agg('mean')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3l8ZRve35zM"
      },
      "outputs": [],
      "source": [
        "# Compute the confidence intervals.\n",
        "independent_results_boot_string = independent_df_concat.groupby(\n",
        "    ['dataset', 'rater_type']\n",
        ")[bias_columns].agg(\n",
        "    lambda *args: bootstrap_metric(\n",
        "        *args, return_string=True, n_resamples=N_RESAMPLES\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blEJ7NS6Lbgu"
      },
      "outputs": [],
      "source": [
        "# Combine the rates with the confidence intervals and display the results.\n",
        "independent_results_combined = combine_point_estimates_and_cis(\n",
        "    independent_results_mean, independent_results_boot_string\n",
        ")\n",
        "independent_results_combined"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4xUn3GoZ2uQ"
      },
      "source": [
        "## Pairwise Rubric: Reproduce Figure 3 results\n",
        "\n",
        "Here, we compute the rate that raters preferred the outputs of either model with\n",
        "respect to the presence of bias and its dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOBg9wryz86v"
      },
      "outputs": [],
      "source": [
        "pairwise_df = data_dict['pairwise']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ql6M7Jpvn6l"
      },
      "outputs": [],
      "source": [
        "pairwise_df['dataset'] = pd.Categorical(pairwise_df['dataset'], dataset_order)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PC9NscNjJD63"
      },
      "outputs": [],
      "source": [
        "# Create separate EquityMedQA set as aggregation over the individual datasets.\n",
        "pairwise_equitymedqa_df = pairwise_df.query('dataset in @equitymedqa_names')\n",
        "pairwise_equitymedqa_df = pairwise_equitymedqa_df.assign(dataset='EquityMedQA')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5wsbJ-O2rxo"
      },
      "outputs": [],
      "source": [
        "# Create columns corresponding to the preferred source.\n",
        "pairwise_df_concat = pd.concat([pairwise_df, pairwise_equitymedqa_df])\n",
        "pairwise_df_concat['preferred_source'] = 'No preference'\n",
        "pairwise_df_concat.loc[\n",
        "    pairwise_df_concat['Med-PaLM_answer_more_bias'], 'preferred_source'\n",
        "] = 'Med-PaLM 2'\n",
        "pairwise_df_concat.loc[\n",
        "    pairwise_df_concat['Med-PaLM-2_answer_more_bias'], 'preferred_source'\n",
        "] = 'Med-PaLM'\n",
        "pairwise_df_concat['bias_presence'] = (\n",
        "    pairwise_df_concat['preferred_source'] != 'No preference'\n",
        ")  # Dummy dimension of bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWMUrSOZEMcU"
      },
      "outputs": [],
      "source": [
        "# Define a list containing the names of relevant columns.\n",
        "bias_columns_pairwise = [\n",
        "    'bias_presence',\n",
        "    'inaccuracy_for_some_axes',\n",
        "    'not_inclusive_for_some_axes',\n",
        "    'stereotypical_language_characterization',\n",
        "    'omits_systemic_explanation',\n",
        "    'failure_to_challenge',\n",
        "    'potential_for_disproportionate',\n",
        "    'other_bias',\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_P7rhp08CAPq"
      },
      "outputs": [],
      "source": [
        "# Reshape the data into a long format,\n",
        "# such that each bias dimension column is mapped to a new row,\n",
        "# creating a string `dimension_of_bias` column indicating the dimension of bias\n",
        "# and a boolean `dimension_present` column indicating whether the dimension of bias was relevant to the pairwise comparison.\n",
        "\n",
        "dimension_df = pairwise_df_concat.melt(\n",
        "    id_vars=['question_id', 'rater_type', 'dataset', 'preferred_source'],\n",
        "    value_vars=bias_columns_pairwise,\n",
        "    var_name='dimension_of_bias',\n",
        "    value_name='dimension_present',\n",
        ")\n",
        "\n",
        "# Create a new column `preferred_source_dimension` that takes on the value of `preferred_source` when the dimension is present, and 'No preference' otherwise.\n",
        "dimension_df = dimension_df.assign(\n",
        "    preferred_source_dimension=lambda x: x['preferred_source'].where(\n",
        "        x['dimension_present'] == True, other='No preference'\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JwUGUjJwDlY"
      },
      "outputs": [],
      "source": [
        "# Reshape the dataframe so that values of `preferred_source_dimension` are mapped to columns\n",
        "dimension_df_wide = pd.get_dummies(\n",
        "    dimension_df,\n",
        "    columns=['preferred_source_dimension'],\n",
        "    prefix='',\n",
        "    prefix_sep='',\n",
        ")\n",
        "\n",
        "dimension_df_wide['dataset'] = pd.Categorical(\n",
        "    dimension_df_wide['dataset'], dataset_order\n",
        ")\n",
        "dimension_df_wide['rater_type'] = pd.Categorical(\n",
        "    dimension_df_wide['rater_type'], rater_type_order\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKJJxms3v4AY"
      },
      "outputs": [],
      "source": [
        "# Compute the estimates of the rates.\n",
        "\n",
        "pairwise_result_mean = dimension_df_wide.groupby(\n",
        "    ['dataset', 'rater_type', 'dimension_of_bias'], observed=True\n",
        ")[['Med-PaLM 2', 'No preference', 'Med-PaLM']].agg('mean')\n",
        "pairwise_result_mean = pairwise_result_mean.stack().unstack(level=-2)[\n",
        "    bias_columns_pairwise\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRPLSmiMIAGp"
      },
      "outputs": [],
      "source": [
        "# Compute the confidence intervals.\n",
        "pairwise_result_boot = dimension_df_wide.groupby(\n",
        "    ['dataset', 'rater_type', 'dimension_of_bias'], observed=True\n",
        ")[['Med-PaLM 2', 'No preference', 'Med-PaLM']].agg(\n",
        "    lambda *args: bootstrap_metric(\n",
        "        *args, return_string=True, n_resamples=N_RESAMPLES\n",
        "    )\n",
        ")\n",
        "pairwise_result_boot = pairwise_result_boot.stack().unstack(level=-2)[\n",
        "    bias_columns_pairwise\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxzQZYMQK7fa"
      },
      "outputs": [],
      "source": [
        "# Combined and display the results.\n",
        "pairwise_results_combined = combine_point_estimates_and_cis(\n",
        "    pairwise_result_mean, pairwise_result_boot\n",
        ")\n",
        "\n",
        "\n",
        "pairwise_results_combined"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OazQgX8Zuw3"
      },
      "source": [
        "## Counterfactual Rubric: reproduce Figure 4 results\n",
        "\n",
        "Here, we compute the rates that counterfactual pairs were reported to contain\n",
        "bias under the counterfactual rubric (corresponding to the \"Counterfactual\n",
        "rubric row\" of Figure 4). Then, we use the ratings under the independent rubric\n",
        "to compute the rates that one, one or more, or both answers were reported as\n",
        "containing bias (corresponding to the subsequent rows)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OESWg8dYiK-"
      },
      "outputs": [],
      "source": [
        "counterfactual_df = data_dict['counterfactual']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zx6Arqpha6_t"
      },
      "outputs": [],
      "source": [
        "# Modify column names to match those used for the independent rubric.\n",
        "counterfactual_df = counterfactual_df.rename(\n",
        "    columns={\n",
        "        'bias_presence': 'bias_presence_binary',\n",
        "        'omits_systemic_explanations': 'omits_systemic_explanation',\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAodBjtuaKAl"
      },
      "outputs": [],
      "source": [
        "# Define relevant column names.\n",
        "bias_columns = [\n",
        "    'bias_presence_binary',\n",
        "    'inaccuracy_for_some_axes',\n",
        "    'not_inclusive_for_some_axes',\n",
        "    'stereotypical_language_characterization',\n",
        "    'omits_systemic_explanation',\n",
        "    'failure_to_challenge',\n",
        "    'potential_for_disproportionate',\n",
        "    'other_bias',\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfRDlzp8ZTw5"
      },
      "outputs": [],
      "source": [
        "# Compute the rates under the counterfactual rubric.\n",
        "counterfactual_results_mean = counterfactual_df.groupby(\n",
        "    ['dataset', 'rater_type']\n",
        ")[bias_columns].agg('mean')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEwqx2ClZybE"
      },
      "outputs": [],
      "source": [
        "# Compute confidence intervals.\n",
        "counterfactual_results_boot_string = counterfactual_df.groupby(\n",
        "    ['dataset', 'rater_type']\n",
        ")[bias_columns].agg(\n",
        "    lambda *args: bootstrap_metric(\n",
        "        *args, return_string=True, n_resamples=N_RESAMPLES\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hE4kAs4McJ7b"
      },
      "outputs": [],
      "source": [
        "# Combine the dataframes.\n",
        "counterfactual_results_combined = combine_point_estimates_and_cis(\n",
        "    counterfactual_results_mean, counterfactual_results_boot_string\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-OGx-N4cPy9"
      },
      "outputs": [],
      "source": [
        "# Map pairs of ratings from the independent rubric using the pairs defined in the counterfactual ratings.\n",
        "\n",
        "counterfactual_datasets = [\n",
        "    'Counterfactual Context - Manual (CC-Manual)',\n",
        "    'Counterfactual Context - LLM (CC-LLM)',\n",
        "]\n",
        "\n",
        "# Get the ratings for question_1_id,\n",
        "counterfactual_merged_1 = pd.merge(\n",
        "    counterfactual_df[\n",
        "        ['question_1_id', 'question_2_id', 'rater_type']\n",
        "    ].drop_duplicates(),\n",
        "    independent_df_concat.query('dataset in @counterfactual_datasets'),\n",
        "    how='inner',\n",
        "    left_on=['question_1_id', 'rater_type'],\n",
        "    right_on=['question_id', 'rater_type'],\n",
        "    suffixes=(None, '_1'),\n",
        ")\n",
        "# Rename columns for clarity,\n",
        "counterfactual_merged_1 = counterfactual_merged_1.rename(\n",
        "    columns={col: f'{col}_1' for col in bias_columns}\n",
        ")\n",
        "\n",
        "# Get the ratings for question_2_id,\n",
        "counterfactual_merged_df = pd.merge(\n",
        "    counterfactual_merged_1,\n",
        "    independent_df_concat.query('dataset in @counterfactual_datasets'),\n",
        "    how='left',\n",
        "    left_on=['question_2_id', 'rater_type'],\n",
        "    right_on=['question_id', 'rater_type'],\n",
        "    suffixes=(None, '_2'),\n",
        ")\n",
        "# Rename columns for clarity,\n",
        "merged_df = counterfactual_merged_df.rename(\n",
        "    columns={col: f'{col}_2' for col in bias_columns}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWo1VGJMsyG0"
      },
      "outputs": [],
      "source": [
        "# Compute rates and CIs for the \"one answer biased\" statistics (second row of Figure 4).\n",
        "counterfactual_df_one = merged_df.copy(deep=True)\n",
        "for col in bias_columns:\n",
        "  counterfactual_df_one[col] = (\n",
        "      counterfactual_df_one[f'{col}_1'] + counterfactual_df_one[f'{col}_2']\n",
        "  ) == True\n",
        "\n",
        "counterfactual_results_one = counterfactual_df_one.groupby(\n",
        "    ['dataset', 'rater_type'], observed=True\n",
        ")[bias_columns].agg('mean')\n",
        "\n",
        "counterfactual_results_one_boot_string = counterfactual_df_one.groupby(\n",
        "    ['dataset', 'rater_type'], observed=True\n",
        ")[bias_columns].agg(\n",
        "    lambda *args: bootstrap_metric(\n",
        "        *args, return_string=True, n_resamples=N_RESAMPLES\n",
        "    )\n",
        ")\n",
        "counterfactual_results_one_combined = combine_point_estimates_and_cis(\n",
        "    counterfactual_results_one, counterfactual_results_one_boot_string\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1_w01iir9wx"
      },
      "outputs": [],
      "source": [
        "# Compute rates and CIs for the \"one or more answers biased\" statistics (third row of Figure 4).\n",
        "counterfactual_df_one_or_more = merged_df.copy(deep=True)\n",
        "for col in bias_columns:\n",
        "  counterfactual_df_one_or_more[col] = (\n",
        "      counterfactual_df_one_or_more[f'{col}_1']\n",
        "      | counterfactual_df_one_or_more[f'{col}_2']\n",
        "  )\n",
        "\n",
        "counterfactual_results_one_or_more = counterfactual_df_one_or_more.groupby(\n",
        "    ['dataset', 'rater_type'], observed=True\n",
        ")[bias_columns].agg('mean')\n",
        "counterfactual_results_one_or_more_boot_string = (\n",
        "    counterfactual_df_one_or_more.groupby(\n",
        "        ['dataset', 'rater_type'], observed=True\n",
        "    )[bias_columns].agg(\n",
        "        lambda *args: bootstrap_metric(\n",
        "            *args, return_string=True, n_resamples=N_RESAMPLES\n",
        "        )\n",
        "    )\n",
        ")\n",
        "counterfactual_results_one_or_more_combined = combine_point_estimates_and_cis(\n",
        "    counterfactual_results_one_or_more,\n",
        "    counterfactual_results_one_or_more_boot_string,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7hmhTRYi_d7"
      },
      "outputs": [],
      "source": [
        "# Compute rates and CIs for the \"both answers biased\" statistics (fourth row of Figure 4).\n",
        "counterfactual_df_both = merged_df.copy(deep=True)\n",
        "for col in bias_columns:\n",
        "  counterfactual_df_both[col] = (\n",
        "      counterfactual_df_both[f'{col}_1'] + counterfactual_df_both[f'{col}_2']\n",
        "  ) == True\n",
        "\n",
        "counterfactual_results_both = counterfactual_df_both.groupby(\n",
        "    ['dataset', 'rater_type'], observed=True\n",
        ")[bias_columns].agg('mean')\n",
        "counterfactual_results_both_boot_string = counterfactual_df_both.groupby(\n",
        "    ['dataset', 'rater_type'], observed=True\n",
        ")[bias_columns].agg(\n",
        "    lambda *args: bootstrap_metric(\n",
        "        *args, return_string=True, n_resamples=N_RESAMPLES\n",
        "    )\n",
        ")\n",
        "counterfactual_results_both_combined = combine_point_estimates_and_cis(\n",
        "    counterfactual_results_both, counterfactual_results_both_boot_string\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSIvnCae62um"
      },
      "outputs": [],
      "source": [
        "# Get the independent results from the earlier part of this notebook to create the final row of Figure 4\n",
        "counterfactual_results_independent_df = (\n",
        "    independent_results_combined.reset_index().query(\n",
        "        'dataset in @counterfactual_datasets'\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5NL0gkV436k"
      },
      "outputs": [],
      "source": [
        "# Concatenate each of the counterfactual dataframes together for ease of viewing\n",
        "counterfactual_concat_df = pd.concat([\n",
        "    counterfactual_results_combined.reset_index().assign(\n",
        "        condition='Counterfactual rubric'\n",
        "    ),\n",
        "    counterfactual_results_one_combined.reset_index().assign(\n",
        "        condition='One answer biased'\n",
        "    ),\n",
        "    counterfactual_results_one_or_more_combined.reset_index().assign(\n",
        "        condition='One or more answer biased'\n",
        "    ),\n",
        "    counterfactual_results_both_combined.reset_index().assign(\n",
        "        condition='Both answers biased'\n",
        "    ),\n",
        "    counterfactual_results_independent_df.reset_index().assign(\n",
        "        condition='Independent evaluation'\n",
        "    ),\n",
        "])\n",
        "\n",
        "# Set index viewing order\n",
        "counterfactual_concat_df['rater_type'] = pd.Categorical(\n",
        "    counterfactual_concat_df['rater_type'], rater_type_order\n",
        ")\n",
        "counterfactual_concat_df['dataset'] = pd.Categorical(\n",
        "    counterfactual_concat_df['dataset'], counterfactual_datasets\n",
        ")\n",
        "counterfactual_concat_df['condition'] = pd.Categorical(\n",
        "    counterfactual_concat_df['condition'],\n",
        "    [\n",
        "        'Counterfactual rubric',\n",
        "        'One answer biased',\n",
        "        'One or more answer biased',\n",
        "        'Both answers biased',\n",
        "        'Independent evaluation',\n",
        "    ],\n",
        ")\n",
        "counterfactual_concat_df = counterfactual_concat_df.set_index(\n",
        "    ['condition', 'dataset', 'rater_type']\n",
        ").sort_index()\n",
        "counterfactual_concat_df"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/grp/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "name": "primary_analyses.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1CQ7cDvahbHwfC7mY69nON-0Ck1cGPnQN",
          "timestamp": 1726253349559
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
